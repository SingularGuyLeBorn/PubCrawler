

==================== Start of: app.py ====================

# FILE: app.py (Gradio Web UI for PubCrawler - v1.2.3 - Final Fixes)

import gradio as gr
import sys
import textwrap
from typing import List, Dict, Any, Tuple

# --- 从 search_service 导入所有功能和配置 ---
from src.search.search_service import (
    initialize_components,
    keyword_search,
    semantic_search,
    save_results_to_markdown,
    generate_ai_response,
    _sqlite_conn,
    _initialized,
    ZHIPUAI_API_KEY,
    SEARCH_RESULTS_DIR
)

# Gradio启动时调用初始化函数 (确保在任何函数被Gradio调用前完成)
initialize_components()

# --- Gradio UI 核心逻辑 ---

# 全局变量用于存储当前搜索结果，以便AI和保存功能访问
current_search_results: List[Dict[str, Any]] = []
current_query_string: str = ""


def perform_search_and_reset_chat(query_input: str) -> Tuple[
    gr.Dataframe, str, str, gr.Column, gr.Accordion, gr.Button]:
    """
    在Gradio UI中执行搜索并更新UI组件。
    """
    global current_search_results, current_query_string
    current_query_string = query_input
    current_search_results = []

    results: List[Dict[str, Any]] = []
    stats: Dict[str, Any] = {"total_found": 0, "distribution": {}, "message": "搜索未执行。"}

    if not _initialized:
        error_msg = "后端服务未成功初始化。"
        return gr.Dataframe(value=[]), error_msg, "初始化失败，无法搜索。", gr.Column(visible=False), gr.Accordion(
            open=False), gr.Button(interactive=False)

    if query_input.lower().startswith('sem:'):
        semantic_query = query_input[4:].strip()
        if semantic_query:
            results, stats = semantic_search(semantic_query)
        else:
            stats['message'] = "语义搜索查询内容不能为空。"
    else:
        results, stats = keyword_search(query_input)

    current_search_results = results

    # 格式化统计信息
    stats_markdown = f"**总计找到 {stats['total_found']} 篇相关论文。**\n\n"
    if stats['distribution']:
        stats_markdown += "**分布情况:**\n"
        for conf_year, count in stats['distribution'].items():
            stats_markdown += f"- {conf_year}: {count} 篇\n"
    else:
        stats_markdown += "无结果分布信息。\n"

    # 格式化结果为Gradio表格
    table_data = []
    for paper in results:
        authors = textwrap.shorten(paper.get('authors', 'N/A'), width=50, placeholder="...")
        title = textwrap.shorten(paper.get('title', 'N/A'), width=80, placeholder="...")
        similarity = f"{paper['similarity']:.2f}" if 'similarity' in paper and paper[
            'similarity'] is not None else "N/A"
        table_data.append([title, authors, paper.get('conference', 'N/A'), paper.get('year', 'N/A'), similarity])

    # 根据是否有结果和API Key来决定AI按钮是否可用
    ai_button_interactive = bool(results and ZHIPUAI_API_KEY)

    return (gr.Dataframe(value=table_data, headers=["标题", "作者", "会议", "年份", "相似度"]),
            stats.get('message', "搜索完成。"),
            stats_markdown,
            gr.Column(visible=False),
            gr.Accordion(open=False),
            gr.Button(interactive=ai_button_interactive))


def save_current_results_gradio() -> str:
    """
    Gradio UI中保存当前搜索结果的回调函数。
    """
    global current_search_results, current_query_string
    if not current_search_results:
        return "没有搜索结果可保存。"
    return save_results_to_markdown(current_search_results, current_query_string)


# --- 【重要修改】: AI 对话函数适配 `type="messages"` ---
def handle_chat_interaction(user_message: str, chat_history: List[Dict[str, str]]):
    """
    处理用户的聊天输入，调用AI服务，并返回响应。
    现在的 chat_history 是一个字典列表，例如: [{"role": "user", "content": "你好"}]
    """
    global current_search_results
    if not current_search_results:
        chat_history.append({"role": "user", "content": user_message})
        chat_history.append({"role": "assistant", "content": "错误：没有可供对话的搜索结果。"})
        return chat_history

    chat_history.append({"role": "user", "content": user_message})

    # generate_ai_response 函数本身就需要这种格式，所以现在无需转换
    ai_response = generate_ai_response(
        chat_history=chat_history,
        search_results_context=current_search_results
    )

    chat_history.append({"role": "assistant", "content": ai_response})
    return chat_history


def clear_chat():
    """清空聊天记录"""
    return [], ""


# --- Gradio UI 布局 ---
with gr.Blocks(title="PubCrawler AI Assistant") as demo:
    gr.Markdown(
        """
        # 📚 PubCrawler AI 学术助手
        欢迎使用 PubCrawler！在这里，您可以搜索学术论文，查看统计信息，并将结果保存或与AI进行对话。

        ---

        ### 搜索语法:
        - `关键词` 或 `短语` (例如: `transformer`, `"large language model"`)
        - `字段搜索`: `author:vaswani`, `title:"vision transformer"`, `abstract:diffusion`
        - `逻辑组合`: `transformer AND author:vaswani`, `"large language model" OR efficient`
        - `语义搜索`: 在查询前加上 `sem:` (例如: `sem: efficiency of few-shot learning`)
        """
    )

    with gr.Row():
        query_input = gr.Textbox(
            label="请输入您的查询",
            placeholder="例如: transformer author:vaswani 或 sem: efficiency of few-shot learning",
            scale=4
        )
        search_button = gr.Button("搜索", variant="primary", scale=1)

    status_output = gr.Textbox(label="状态/消息", interactive=False)

    with gr.Row():
        stats_markdown_output = gr.Markdown(
            value="--- 查询结果统计 --- \n总计找到 0 篇相关论文。\n无结果分布信息。",
            label="结果统计"
        )

    results_dataframe = gr.Dataframe(
        headers=["标题", "作者", "会议", "年份", "相似度"],
        col_count=(5, "fixed"),
        interactive=False,
        label="搜索结果"
    )

    with gr.Row():
        save_button = gr.Button("保存当前结果到 Markdown")
        start_chat_button = gr.Button("与AI对话 (需先搜索)", interactive=False)

    with gr.Accordion("🤖 AI 对话窗口", open=False) as chat_accordion:
        with gr.Column(visible=True) as chat_interface_column:
            # 【重要修改】: 修复 UserWarning
            chatbot = gr.Chatbot(label="与AI的对话", type="messages")
            chat_input = gr.Textbox(label="你的问题", placeholder="例如：请总结一下这些论文的核心贡献。")
            with gr.Row():
                chat_submit_btn = gr.Button("发送", variant="primary")
                chat_clear_btn = gr.Button("清除对话")

    # --- 绑定事件 ---
    search_button.click(
        fn=perform_search_and_reset_chat,
        inputs=query_input,
        outputs=[results_dataframe, status_output, stats_markdown_output, chat_interface_column, chat_accordion,
                 start_chat_button]
    )

    query_input.submit(
        fn=perform_search_and_reset_chat,
        inputs=query_input,
        outputs=[results_dataframe, status_output, stats_markdown_output, chat_interface_column, chat_accordion,
                 start_chat_button]
    )

    save_button.click(
        fn=save_current_results_gradio,
        inputs=[],
        outputs=status_output
    )

    start_chat_button.click(
        fn=lambda: (gr.Accordion(open=True), gr.Column(visible=True)),
        inputs=None,
        outputs=[chat_accordion, chat_interface_column]
    )

    chat_submit_btn.click(
        fn=handle_chat_interaction,
        inputs=[chat_input, chatbot],
        outputs=[chatbot]
    ).then(lambda: "", inputs=None, outputs=chat_input)

    chat_input.submit(
        fn=handle_chat_interaction,
        inputs=[chat_input, chatbot],
        outputs=[chatbot]
    ).then(lambda: "", inputs=None, outputs=chat_input)

    chat_clear_btn.click(
        fn=clear_chat,
        inputs=None,
        outputs=[chatbot, chat_input]
    )

# 运行Gradio应用
if __name__ == "__main__":
    if not _initialized:
        print(f"无法启动Gradio应用，后端初始化失败。请检查错误信息。")
        sys.exit(1)
    else:
        SEARCH_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        # 【重要修改】: 添加 inbrowser=True
        demo.launch(share=True, inbrowser=True)


==================== End of: app.py ====================



==================== Start of: getallcode.py ====================

import os

# --- 配置 ---

# 1. 指定要包含的文件后缀名
TARGET_EXTENSIONS = ['.py', '.html', '.css', '.yaml']

# 2. 指定输出的聚合文件名
OUTPUT_FILENAME = 'combined_files.txt'

# 3. 指定要排除的目录名
EXCLUDED_DIRS = ['.git', '__pycache__', 'node_modules', '.vscode', '.venv']


# --- 脚本 ---

def combine_files():
    """
    遍历当前脚本所在目录及子目录,将指定后缀的文件内容合并到一个txt文件中。
    """

    # 获取此脚本所在的目录
    # __file__ 是 Python 的一个内置变量，表示当前执行的脚本文件的路径
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        # 如果在 REPL 或 notebook 中运行，__file__ 可能未定义
        script_dir = os.getcwd()
        print(f"警告: 无法获取脚本路径, 使用当前工作目录: {script_dir}")

    print(f"开始在 {script_dir} 中搜索文件...")
    print(f"将排除以下目录: {', '.join(EXCLUDED_DIRS)}")

    found_files_count = 0

    # 'w' 模式会覆盖已存在的文件。确保每次运行都是一个全新的聚合文件。
    # 使用 utf-8 编码处理各种文件内容
    try:
        with open(os.path.join(script_dir, OUTPUT_FILENAME), 'w', encoding='utf-8') as outfile:

            # os.walk 会递归遍历目录
            # root: 当前目录路径
            # dirs: 当前目录下的子目录列表
            # files: 当前目录下的文件列表
            for root, dirs, files in os.walk(script_dir):

                # *** 修改点在这里 ***
                # 通过修改 dirs 列表 (dirs[:]) 来阻止 os.walk 进一步遍历这些目录
                dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]

                for filename in files:
                    # 检查文件后缀是否在我们的目标列表中
                    if any(filename.endswith(ext) for ext in TARGET_EXTENSIONS):

                        file_path = os.path.join(root, filename)

                        # 获取相对路径，以便在输出文件中更清晰地显示
                        relative_path = os.path.relpath(file_path, script_dir)

                        # 排除输出文件本身，防止它把自己也包含进去
                        if relative_path == OUTPUT_FILENAME:
                            continue

                        print(f"  正在添加: {relative_path}")
                        found_files_count += 1

                        # 写入文件分隔符和路径
                        outfile.write(f"\n\n{'=' * 20} Start of: {relative_path} {'=' * 20}\n\n")

                        try:
                            # 以只读 ('r') 模式打开源文件
                            # 使用 errors='ignore' 来跳过无法解码的字符
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                                content = infile.read()
                                outfile.write(content)

                        except Exception as e:
                            # 如果读取失败（例如权限问题），则记录错误
                            outfile.write(f"--- 无法读取文件: {e} ---\n")
                            print(f"  [错误] 无法读取 {relative_path}: {e}")

                        # 写入文件结束符
                        outfile.write(f"\n\n{'=' * 20} End of: {relative_path} {'=' * 20}\n\n")

        print(f"\n完成！成功聚合 {found_files_count} 个文件。")
        print(f"输出文件已保存为: {os.path.join(script_dir, OUTPUT_FILENAME)}")

    except IOError as e:
        print(f"创建输出文件时发生错误: {e}")
    except Exception as e:
        print(f"发生未知错误: {e}")


# --- 执行 ---
if __name__ == "__main__":
    combine_files()

==================== End of: getallcode.py ====================



==================== Start of: streamlit_app.py ====================

# FILE: streamlit_app.py
# 放置于项目根目录，与 'src' 和 'app.py' 同级。
# 运行: streamlit run streamlit_app.py

import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import logging
import re
import math
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import traceback # 用于更详细的错误捕获

# -----------------------------------------------------------------
# 1. 导入项目模块 (保持不变)
# -----------------------------------------------------------------
try:
    from src.search.search_service import (
        initialize_components, keyword_search, semantic_search,
        generate_ai_response, get_stats_summary, _initialized,
        ZHIPUAI_API_KEY, SEARCH_RESULTS_DIR
    )
    from src.crawlers.config import METADATA_OUTPUT_DIR, TRENDS_OUTPUT_DIR
except ImportError as e:
    st.error(f"导入项目模块失败！错误: {e}\n\n请确保满足所有依赖和文件结构要求。")
    st.stop()

# -----------------------------------------------------------------
# 2. 应用级常量 (保持不变)
# -----------------------------------------------------------------
STREAMLIT_AI_CONTEXT_PAPERS = 20
RESULTS_PER_PAGE = 25
ANALYSIS_TOP_N = 50 # 趋势分析图表默认显示 Top N 主题

# -----------------------------------------------------------------
# 3. Streamlit 页面配置与后端初始化 (保持不变)
# -----------------------------------------------------------------
st.set_page_config(page_title="PubCrawler Pro 🚀", layout="wide", initial_sidebar_state="expanded")

@st.cache_resource
def load_backend_components():
    print("--- [Streamlit] 正在初始化 PubCrawler 后端服务... ---")
    if not _initialized:
        try: initialize_components()
        except Exception as e:
            logging.error(f"后端初始化失败: {e}"); return False
    if not _initialized: print("--- [Streamlit] 后端初始化失败! ---"); return False
    print("--- [Streamlit] 后端服务准备就绪。 ---")
    return True

backend_ready = load_backend_components()
if not backend_ready:
    st.error("后端服务初始化失败！请检查终端日志。"); st.stop()

# -----------------------------------------------------------------
# 4. Streamlit 页面状态管理 (保持不变)
# -----------------------------------------------------------------
if "chat_history" not in st.session_state: st.session_state.chat_history: List[Dict[str, str]] = []
if "current_search_results" not in st.session_state: st.session_state.current_search_results: List[Dict[str, Any]] = []
if "current_filtered_results" not in st.session_state: st.session_state.current_filtered_results: List[Dict[str, Any]] = []
if "current_query" not in st.session_state: st.session_state.current_query: str = ""
if "current_page" not in st.session_state: st.session_state.current_page: int = 1

# -----------------------------------------------------------------
# 5. 辅助函数
# -----------------------------------------------------------------
def find_analysis_files():
    """
    扫描 output/ 目录查找分析 CSV 文件，无缓存。
    v1.6: 区分 CSV 类型 (raw_data, summary_table, trends)。
    返回:
        analysis_data (dict): 按 会议/年份 组织的 {"csvs": [{"path": Path, "type": str}]} 字典。
        all_conferences (list): 找到的所有会议名称列表。
        all_years (list): 找到的所有年份列表。
    """
    print("--- [Streamlit] 正在扫描分析文件 (v1.6 - 无缓存)... ---")
    scan_dirs = {"metadata": METADATA_OUTPUT_DIR, "trends": TRENDS_OUTPUT_DIR}
    analysis_data = {}
    all_conferences = set(); all_years = set()
    for dir_type, base_path in scan_dirs.items():
        if not base_path.exists(): continue
        csv_files = []
        if dir_type == "metadata":
            # 查找 analysis/ 目录下的 CSV (通常是 summary table)
            csv_files.extend(list(base_path.rglob("analysis/*.csv")))
            # 查找 metadata/conf/year/ 目录下的 CSV (通常是 raw data)
            csv_files.extend(list(base_path.rglob("*_data_*.csv")))
        elif dir_type == "trends":
            csv_files.extend(list(base_path.rglob("*.csv"))) # 通常是 trends 数据

        for f in csv_files:
            try:
                conf, year = None, None
                csv_type = "unknown" # 默认类型

                # 识别文件类型和位置
                if dir_type == "metadata":
                    if f.parent.name == "analysis":
                        year = f.parent.parent.name
                        conf = f.parent.parent.parent.name
                        # 启发式判断是否为 summary_table
                        if "summary_table" in f.name or "4_summary_table" in f.name:
                             csv_type = "summary_table"
                        else:
                             csv_type = "analysis_other" # 其他分析文件
                    elif "_data_" in f.name:
                        year = f.parent.name
                        conf = f.parent.parent.name
                        csv_type = "raw_data"
                elif dir_type == "trends":
                    conf = f.parent.name
                    year = "Cross-Year"
                    csv_type = "trends"

                # 存储文件信息
                if conf and year:
                    all_conferences.add(conf); all_years.add(year)
                    if conf not in analysis_data: analysis_data[conf] = {}
                    if year not in analysis_data[conf]: analysis_data[conf][year] = {"csvs": []}
                    # 检查是否重复添加 (路径相同)
                    if not any(item["path"] == f for item in analysis_data[conf][year]["csvs"]):
                        analysis_data[conf][year]["csvs"].append({"path": f, "type": csv_type})
            except Exception as scan_e: print(f"扫描文件 {f} 时出错: {scan_e}")

    print(f"--- [Streamlit] 扫描完成，找到会议: {list(all_conferences)}, 年份: {list(all_years)} ---")
    # 对年份排序，"Cross-Year" 可能需要特殊处理，这里简单按字符串排序
    return analysis_data, sorted(list(all_conferences)), sorted(list(all_years), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)


def save_results_to_markdown_fixed(results: List[Dict[str, Any]], query: str) -> str:
    """ (v1.3) 保存 Markdown，包含摘要和 PDF 链接。"""
    # ... (内部逻辑保持不变) ...
    if not results: return "没有搜索结果可保存。"
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# 搜索查询: \"{query}\"\n\n**共找到 {len(results)} 条相关结果**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title = paper.get('title', 'N/A'); authors = paper.get('authors', 'N/A')
            abstract = paper.get('abstract', 'N/A'); conf = paper.get('conference', 'N/A')
            year = paper.get('year', 'N/A'); pdf_url = paper.get('pdf_url', '#')
            f.write(f"### {idx}. {title}\n\n"); f.write(f"- **作者**: {authors}\n")
            f.write(f"- **会议/年份**: {conf} {year}\n")
            if 'similarity' in paper: f.write(f"- **语义相似度**: {paper['similarity']:.3f}\n")
            if pdf_url and pdf_url != '#':
                pdf_display_name = pdf_url.split('/')[-1] if '/' in pdf_url else "链接"
                f.write(f"- **PDF 链接**: [{pdf_display_name}]({pdf_url})\n")
            f.write(f"\n**摘要:**\n> {abstract}\n\n---\n\n")
    return str(filename.resolve())


def get_numeric_columns(df): return df.select_dtypes(include=['number']).columns.tolist()
def get_categorical_columns(df): return df.select_dtypes(include=['object', 'category']).columns.tolist()

# -----------------------------------------------------------------
# 6. 页面渲染函数
# -----------------------------------------------------------------

def render_search_and_chat_page():
    """ 渲染 "AI 助手 & 搜索" 页面 (v1.5 逻辑，摘要部分高亮) """
    # ... (大部分逻辑不变，仅修改摘要显示部分) ...
    st.header("🔍 PubCrawler Pro: AI 助手 & 搜索", divider="rainbow")
    _, conf_list, year_list = find_analysis_files()
    search_query = st.text_input("搜索本地学术知识库...", key="search_input", placeholder="输入关键词或 'sem:' 前缀进行语义搜索", help="关键词搜索: `transformer author:vaswani` | 语义搜索: `sem: few-shot learning efficiency`")
    col_f1, col_f2 = st.columns(2)
    with col_f1: selected_conferences = st.multiselect("筛选会议", options=conf_list, key="filter_conf")
    with col_f2: selected_years = st.multiselect("筛选年份", options=year_list, key="filter_year")
    is_new_search = (st.session_state.search_input != st.session_state.current_query)
    if is_new_search:
        with st.spinner(f"正在搜索: {st.session_state.search_input}..."):
            results, stats = [], {}; query = st.session_state.search_input.strip()
            if query.lower().startswith('sem:'):
                query_text = query[4:].strip();
                if query_text: results, stats = semantic_search(query_text)
            elif query: results, stats = keyword_search(query)
            st.session_state.current_search_results = results
            st.session_state.current_query = st.session_state.search_input
            st.session_state.chat_history = []; st.session_state.current_page = 1
            st.toast(stats.get('message', '搜索完成!'))
    if st.session_state.current_search_results:
        temp_filtered_results = st.session_state.current_search_results
        if selected_conferences: temp_filtered_results = [p for p in temp_filtered_results if p.get('conference') in selected_conferences]
        if selected_years: temp_filtered_results = [p for p in temp_filtered_results if str(p.get('year')) in selected_years]
        st.session_state.current_filtered_results = temp_filtered_results
    else: st.session_state.current_filtered_results = []
    if is_new_search: st.session_state.current_page = 1
    col_results, col_chat = st.columns([0.6, 0.4])
    with col_results:
        results_to_display = st.session_state.current_filtered_results
        st.subheader(f"搜索结果 (筛选后: {len(results_to_display)} 篇 / 原始: {len(st.session_state.current_search_results)} 篇)")
        if results_to_display:
            with st.container(border=True, height=300):
                stats = get_stats_summary(results_to_display); c1, c2 = st.columns(2)
                c1.metric("筛选后找到", f"{stats['total_found']} 篇")
                if c2.button("📥 保存当前 *筛选后* 的结果到 Markdown", use_container_width=True):
                    with st.spinner("正在保存..."):
                        save_path = save_results_to_markdown_fixed(results_to_display, st.session_state.current_query)
                        st.success(f"结果已保存到: {save_path}")
                st.write("**会议/年份分布 (筛选后):**"); st.dataframe(pd.DataFrame(stats['distribution'].items(), columns=['来源', '论文数']), use_container_width=True, hide_index=True)
            st.divider()
            total_items = len(results_to_display); total_pages = math.ceil(total_items / RESULTS_PER_PAGE)
            if st.session_state.current_page > total_pages: st.session_state.current_page = max(1, total_pages)
            page_display_text = f"第 {st.session_state.current_page} / {total_pages} 页 ({total_items} 条)" if total_pages > 0 else "无结果"
            col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
            with col_page1:
                 if st.button("上一页", disabled=st.session_state.current_page <= 1, use_container_width=True): st.session_state.current_page -= 1; st.rerun()
            with col_page2: st.markdown(f"<div style='text-align: center; margin-top: 8px;'>{page_display_text}</div>", unsafe_allow_html=True)
            with col_page3:
                 if st.button("下一页", disabled=st.session_state.current_page >= total_pages, use_container_width=True): st.session_state.current_page += 1; st.rerun()
            start_idx = (st.session_state.current_page - 1) * RESULTS_PER_PAGE; end_idx = start_idx + RESULTS_PER_PAGE
            paginated_results = results_to_display[start_idx:end_idx]
            for i, paper in enumerate(paginated_results, start=start_idx + 1):
                with st.expander(f"**{i}. {paper.get('title', 'N/A')}**"):
                    if 'similarity' in paper: st.markdown(f"**语义相似度**: `{paper['similarity']:.3f}`")
                    st.markdown(f"**作者**: *{paper.get('authors', 'N/A')}*"); st.markdown(f"**会议/年份**: {paper.get('conference', 'N/A')} {paper.get('year', 'N/A')}")
                    # ---【v1.6 摘要高亮】---
                    abstract_text = paper.get('abstract', None) # Use None to distinguish missing key vs empty string
                    if abstract_text is None or abstract_text == '' or abstract_text == 'N/A' or pd.isna(abstract_text):
                        st.markdown(f"**摘要**: <span style='color:orange; font-style: italic;'>[摘要信息缺失或为空]</span>", unsafe_allow_html=True)
                    else:
                        st.markdown(f"**摘要**: \n> {abstract_text}")

                    pdf_url = paper.get('pdf_url', '#');
                    if pdf_url and pdf_url != '#': st.link_button("🔗 打开 PDF 链接", pdf_url)
        elif st.session_state.current_query: st.info("未找到相关结果（或被筛选条件过滤）。")
        else: st.info("请输入查询以开始搜索。")
    # ... (聊天部分逻辑保持不变) ...
    with col_chat:
        st.subheader(f"🤖 AI 对话助手"); st.info(f"AI 将基于上文搜索到的 **Top {STREAMLIT_AI_CONTEXT_PAPERS}** 篇论文进行回答。")
        chat_container = st.container(height=500, border=True)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]): st.markdown(message["content"])
        if not ZHIPUAI_API_KEY: st.error("未配置 ZHIPUAI_API_KEY!"); chat_disabled = True
        elif not st.session_state.current_filtered_results: st.info("请先搜索并确保有结果再对话。"); chat_disabled = True
        else: chat_disabled = False
        if prompt := st.chat_input("基于搜索结果提问...", disabled=chat_disabled, key="chat_input"):
            st.session_state.chat_history.append({"role": "user", "content": prompt}); st.rerun()
        if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
            with chat_container:
                for message in st.session_state.chat_history: # Re-render history
                    with st.chat_message(message["role"]): st.markdown(message["content"])
                with st.chat_message("assistant"): # Request AI
                    with st.spinner("AI 正在思考..."):
                        response = generate_ai_response(st.session_state.chat_history, st.session_state.current_filtered_results[:STREAMLIT_AI_CONTEXT_PAPERS])
                        st.markdown(response)
            st.session_state.chat_history.append({"role": "assistant", "content": response}); st.rerun() # Persist AI response
        if st.session_state.chat_history and not chat_disabled:
            if st.button("清除对话历史", use_container_width=True): st.session_state.chat_history = []; st.rerun()

def render_analysis_dashboard_page():
    """
    渲染 "趋势分析仪表盘" 页面 (v1.6 - 智能 CSV 处理和核心图表)
    """
    st.header("📊 趋势分析仪表盘 (核心图表)", divider="rainbow")
    st.info(
        "选择会议、年份和具体的 CSV 分析文件，自动生成核心趋势图表或展示原始数据。"
    )

    analysis_data, all_conferences, _ = find_analysis_files()

    if not analysis_data:
        st.warning("未找到任何分析文件。请先运行 `run_crawler.py`。")
        st.stop()

    # --- 用户选择 ---
    selected_conf = st.selectbox("1. 选择会议", options=sorted(analysis_data.keys()))
    if not selected_conf: st.stop()

    conf_data = analysis_data[selected_conf]
    sorted_years = sorted(conf_data.keys(), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)
    selected_year = st.selectbox(f"2. 选择 {selected_conf} 的年份或跨年数据", options=sorted_years)
    if not selected_year: st.stop()

    files_info = conf_data[selected_year]

    if not files_info["csvs"]:
        st.warning(f"未找到 {selected_conf} {selected_year} 的 CSV 数据文件。")
        st.stop()

    # ---【v1.6 新增】让用户选择要分析的 CSV 文件 ---
    # 创建显示标签，包含类型信息
    csv_options = {f"{item['path'].name} ({item['type']})": item for item in files_info["csvs"]}
    selected_csv_label = st.selectbox("3. 选择要分析的 CSV 文件", options=csv_options.keys())
    if not selected_csv_label: st.stop()

    selected_csv_info = csv_options[selected_csv_label]
    csv_path = selected_csv_info["path"]
    csv_type = selected_csv_info["type"]

    st.markdown(f"#### 正在分析: `{csv_path.name}` (类型: `{csv_type}`)")

    # --- 加载并处理 CSV ---
    try:
        df = pd.read_csv(csv_path)

        if df.empty:
            st.warning("CSV 文件为空。"); st.stop()

        # --- 【核心 v1.6】根据 CSV 类型决定展示内容 ---
        st.markdown("---")

        # <<< --- A. 如果是分析汇总文件 (summary_table) --- >>>
        if csv_type == "summary_table":
            st.subheader("📈 核心分析图表")

            # 图表 1: 主题热度 (按论文数)
            required_cols_heat = ['Topic_Name', 'paper_count']
            if all(col in df.columns for col in required_cols_heat):
                st.markdown(f"##### 1. 主题热度排名 (Top {ANALYSIS_TOP_N} 论文数)")
                df_sorted_count = df.sort_values(by='paper_count', ascending=False).head(ANALYSIS_TOP_N)
                fig_bar_count = px.bar(df_sorted_count, x='paper_count', y='Topic_Name', orientation='h',
                                 title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} 热门主题 (论文数量)',
                                 labels={'paper_count': '论文数量', 'Topic_Name': '主题'},
                                 text_auto=True, height=max(600, len(df_sorted_count)*20))
                fig_bar_count.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_bar_count, use_container_width=True)
            else:
                st.caption(f"无法生成主题热度图：CSV 文件 `{csv_path.name}` 缺少必需的列: {required_cols_heat}")

            # 图表 2: 主题质量 (按平均分)
            required_cols_quality = ['Topic_Name', 'avg_rating']
            if all(col in df.columns for col in required_cols_quality) and df['avg_rating'].notna().any():
                st.markdown(f"##### 2. 主题质量排名 (Top {ANALYSIS_TOP_N} 平均审稿分)")
                df_sorted_rating = df.dropna(subset=['avg_rating']).sort_values(by='avg_rating', ascending=False).head(ANALYSIS_TOP_N)
                if not df_sorted_rating.empty:
                    fig_bar_rating = px.bar(df_sorted_rating, x='avg_rating', y='Topic_Name', orientation='h',
                                     title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} 主题 (平均审稿分)',
                                     labels={'avg_rating': '平均审稿分', 'Topic_Name': '主题'},
                                     text_auto='.2f', height=max(600, len(df_sorted_rating)*20))
                    fig_bar_rating.update_layout(yaxis={'categoryorder':'total ascending'})
                    st.plotly_chart(fig_bar_rating, use_container_width=True)
                else: st.caption("未能生成主题质量图：筛选/排序后数据为空。")
            elif not all(col in df.columns for col in required_cols_quality):
                st.caption(f"无法生成主题质量图：CSV 文件 `{csv_path.name}` 缺少必需的列: {required_cols_quality}")
            else: st.caption("无法生成主题质量图：'avg_rating' 列没有有效数据。")

            # 图表 3: 决策构成 (按接收率)
            decision_cols = ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']
            present_decision_cols = [col for col in decision_cols if col in df.columns]
            required_cols_decision = ['Topic_Name', 'acceptance_rate'] + present_decision_cols
            if present_decision_cols and all(col in df.columns for col in ['Topic_Name', 'acceptance_rate']):
                st.markdown(f"##### 3. 主题接收构成 (Top {ANALYSIS_TOP_N} 按接收率排序)")
                df_sorted_accept = df.dropna(subset=['acceptance_rate']).sort_values(by='acceptance_rate', ascending=False).head(ANALYSIS_TOP_N)
                if not df_sorted_accept.empty:
                    df_plot = df_sorted_accept.set_index('Topic_Name')[present_decision_cols]
                    # 检查是否有非零数据行
                    df_plot = df_plot.loc[df_plot.sum(axis=1) > 0] # 移除全零行避免除零错误
                    if not df_plot.empty:
                        df_plot_normalized = df_plot.apply(lambda x: x / x.sum(), axis=1) # 归一化
                        fig_stack = px.bar(df_plot_normalized, y=df_plot_normalized.index, x=df_plot_normalized.columns,
                                           orientation='h', title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} 主题决策构成 (按接收率排序)',
                                           labels={'value': '论文比例', 'variable': '决策类型', 'y': '主题'},
                                           height=max(600, len(df_plot_normalized)*25), text_auto='.1%')
                        fig_stack.update_layout(yaxis={'categoryorder':'total ascending'}, xaxis_tickformat=".0%", legend_title_text='决策类型')
                        fig_stack.update_traces(hovertemplate='<b>%{y}</b><br>%{variable}: %{x:.1%}<extra></extra>') # 改进悬停
                        st.plotly_chart(fig_stack, use_container_width=True)
                    else:
                        st.caption("未能生成决策构成图：筛选掉全零数据后为空。")

                else: st.caption("未能生成决策构成图：按接受率筛选/排序后数据为空。")
            elif not present_decision_cols: st.caption(f"无法生成决策构成图：CSV 文件 `{csv_path.name}` 缺少决策列 (如 Oral, Poster 等)。")
            else: st.caption(f"无法生成决策构成图：CSV 文件 `{csv_path.name}` 缺少 'Topic_Name' 或 'acceptance_rate' 列。")

        # <<< --- B. 如果是跨年趋势文件 (trends) --- >>>
        elif csv_type == "trends":
            st.subheader("📈 跨年份趋势图")
            # 假设 trends CSV 的结构：index=年份, columns=主题名称, values=论文数
            try:
                # 确保年份是索引
                if 'year' in df.columns: df = df.set_index('year')
                elif df.index.name != 'year': # 尝试假设第一列是年份索引
                     potential_year_col = df.columns[0]
                     if pd.api.types.is_numeric_dtype(df[potential_year_col]):
                          df = df.set_index(potential_year_col)
                          df.index.name = 'year'

                if df.index.name == 'year' and len(df.columns) > 0:
                    # 选择 Top N 主题进行展示
                    top_topics = df.sum().nlargest(15).index # 显示 Top 15 趋势
                    df_top = df[top_topics]
                    fig_line = px.line(df_top, x=df_top.index, y=df_top.columns, markers=True,
                                       title=f'{selected_conf} - Top 15 主题历年论文数趋势',
                                       labels={'year': '年份', 'value': '论文数量', 'variable': '主题'})
                    fig_line.update_layout(xaxis_type='category') # 确保年份按类别显示
                    st.plotly_chart(fig_line, use_container_width=True)
                else:
                    st.warning(f"无法为 `{csv_path.name}` 生成趋势折线图。请确保 CSV 格式正确（例如，年份作为索引或名为 'year' 的列，其他列为主题名称）。")

            except Exception as trend_e:
                st.error(f"生成跨年趋势图时出错: {trend_e}")
                st.error(traceback.format_exc())

        # <<< --- C. 如果是原始数据文件 (raw_data) 或其他 --- >>>
        else:
            st.subheader("📄 原始数据预览")
            st.info("检测到这是一个原始数据文件或无法识别的分析文件，仅提供数据表格预览。")
            st.dataframe(df, height=500, use_container_width=True)


        # --- 显示/下载原始数据表格 (适用于所有类型) ---
        st.markdown("---")
        with st.expander("查看/下载当前 CSV 数据表格"):
            st.dataframe(df, height=300, use_container_width=True)
            st.download_button(
                label=f"📥 下载数据 {csv_path.name}",
                data=df.to_csv(index=False).encode('utf-8-sig'),
                file_name=csv_path.name,
                mime='text/csv',
                key=f"download_{csv_path.stem}" # 添加唯一 key
            )

    except pd.errors.EmptyDataError:
        st.warning(f"文件 {csv_path.name} 为空，跳过。")
    except Exception as e:
        st.error(f"处理 CSV 文件 {csv_path.name} 时失败: {e}")
        st.error(traceback.format_exc()) # 显示详细错误信息

# -----------------------------------------------------------------
# 7. 主应用逻辑：侧边栏导航 (保持不变)
# -----------------------------------------------------------------
st.sidebar.title("PubCrawler Pro 🚀"); st.sidebar.caption("v1.6 - Smart CSV Analysis")
page = st.sidebar.radio("选择功能页面", ["🤖 AI 助手 & 搜索", "📊 趋势分析仪表盘"], key="page_selection")
st.sidebar.divider()
analysis_data_sidebar, conf_list_sidebar, _ = find_analysis_files()
if conf_list_sidebar:
    with st.sidebar.expander("目前已索引的会议", expanded=True):
        st.dataframe(conf_list_sidebar, use_container_width=True, hide_index=True, column_config={"value": "会议名称"})
if page == "🤖 AI 助手 & 搜索": render_search_and_chat_page()
elif page == "📊 趋势分析仪表盘": render_analysis_dashboard_page()



==================== End of: streamlit_app.py ====================



==================== Start of: configs\tasks.yaml ====================

# FILE: configs/tasks.yaml

# ==============================================================================
# PubCrawler Task Configuration v11.0 (Refactored)
# ==============================================================================

# ------------------------------------------------------------------------------
# 1. DATA SOURCE DEFINITIONS ("The Encyclopedia")
# ------------------------------------------------------------------------------
source_definitions:
  # OpenReview 定义
  openreview:
    ICLR: { venue_id: "ICLR.cc/YYYY/Conference", api_v1_years: [2019, 2020, 2021, 2022, 2023] }
    NeurIPS: { venue_id: "NeurIPS.cc/YYYY/Conference", api_v1_years: [2019, 2020, 2021, 2022] }

  # HTML 定义
  html_cvf:
    CVPR: "https://openaccess.thecvf.com/CVPRYYYY?day=all"
    ICCV: "https://openaccess.thecvf.com/ICCVYYYY?day=all"
  html_pmlr:
    ICML: "https://proceedings.mlr.press/"
  html_acl:
    ACL: "https://aclanthology.org/volumes/YYYY.acl-long/"
    EMNLP: "https://aclanthology.org/volumes/YYYY.emnlp-main/"
    NAACL: { pattern_map: { 2019: "2019.naacl-main", 2021: "2021.naacl-main", 2022: "2022.naacl-main", 2024: "2024.naacl-long" } }

  # 其他定义
  selenium:
    AAAI: "https://aaai.org/aaai-publications/aaai-conference-proceedings/"
    KDD: "https://dl.acm.org/conference/kdd/proceedings"
  arxiv:
    API: "http://export.arxiv.org/api/query?"


# ------------------------------------------------------------------------------
# 2. TASKS TO EXECUTE ("The Battle Plan")
# ------------------------------------------------------------------------------
tasks:

  # === TPAMI: 测试最新的 5 篇 Early Access 论文 ===
#  - name: 'TPAMI_Latest_Test'
#    conference: 'TPAMI'
#    year: 2025
#    source_type: 'tpami'
#    enabled: true
#    punumber: '34' #


  # === ICLR: 2022 - 2026 ===

#  - name: 'ICLR_2026'
#    conference: 'ICLR'
#    year: 2026
#    source_type: 'iclr'
#    enabled: true
#    fetch_reviews: false
#    download_pdfs: false


  - name: 'ICLR_2025'
    conference: 'ICLR'
    year: 2025
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2024'
    conference: 'ICLR'
    year: 2024
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2023'
    conference: 'ICLR'
    year: 2023
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2022'
    conference: 'ICLR'
    year: 2022
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  # === NeurIPS: 2022 - 2026 ===

  - name: 'NeurIPS_2026'
    conference: 'NeurIPS'
    year: 2026
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2025'
    conference: 'NeurIPS'
    year: 2025
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2024'
    conference: 'NeurIPS'
    year: 2024
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2023'
    conference: 'NeurIPS'
    year: 2023
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2022'
    conference: 'NeurIPS'
    year: 2022
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  # === ICML: 2022 - 2026 ===

  - name: 'ICML_2026'
    conference: 'ICML'
    year: 2026
    source_type: 'icml'
    enabled: true
    download_pdfs: false


  - name: 'ICML_2025'
    conference: 'ICML'
    year: 2025
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v237/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2024'
    conference: 'ICML'
    year: 2024
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v235/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2023'
    conference: 'ICML'
    year: 2023
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v202/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2022'
    conference: 'ICML'
    year: 2022
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v162/"
    enabled: true
    download_pdfs: false
    
  # === ACL: 2022 - 2026 ===
  # 对于 ACL, EMNLP, NAACL 等网站，由于其结构特殊，需要逐一访问论文详情页，
  # 速度较慢。我们为此类任务引入了并发爬取优化。
#
#  - name: 'ACL_2026'
#    conference: 'ACL'
#    year: 2026
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- 并发优化参数 ---
#    # 并发线程数 (推荐 8-32)。仅对 ACL, CVF 等需要逐页抓取的爬虫有效。
#    max_workers: 24
#    # 该任务最多爬取的论文数量上限。设置为 0 表示不限制，爬取所有找到的论文。
#    max_papers_limit: 100
#
#
#  - name: 'ACL_2025'
#    conference: 'ACL'
#    year: 2025
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- 并发优化参数 ---
#    max_workers: 24
#    max_papers_limit: 100 # 0 表示不限制
#
#
#  - name: 'ACL_2024'
#    conference: 'ACL'
#    year: 2024
#    source_type: 'acl'
#    url_override: "https://aclanthology.org/volumes/2024.acl-long/"
#    enabled: true
#    download_pdfs: false
#    # --- 并发优化参数 ---
#    max_workers: 24
#    # 示例：如果只想爬取前500篇，可以设置为 500
#    max_papers_limit: 500
#
#
#  - name: 'ACL_2023'
#    conference: 'ACL'
#    year: 2023
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- 并发优化参数 ---
#    max_workers: 24
#    max_papers_limit: 100
#
#
#  - name: 'ACL_2022'
#    conference: 'ACL'
#    year: 2022
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- 并发优化参数 ---
#    max_workers: 24
#    max_papers_limit: 100
#
#
#  # === CVF (CVPR, ICCV): 2022 - 2026 ===
#  # CVF 旗下会议网站与 ACL 结构类似，同样受益于并发爬取优化。
#
#  - name: 'CVPR_2026'
#    conference: 'CVPR'
#    year: 2026
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2025'
#    conference: 'CVPR'
#    year: 2025
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2024'
#    conference: 'CVPR'
#    year: 2024
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2023'
#    conference: 'CVPR'
#    year: 2023
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2022'
#    conference: 'CVPR'
#    year: 2022
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'ICCV_2025' # ICCV 为奇数年举办
#    conference: 'ICCV'
#    year: 2025
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'ICCV_2023' # ICCV 为奇数年举办
#    conference: 'ICCV'
#    year: 2023
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100


==================== End of: configs\tasks.yaml ====================



==================== Start of: configs\trends.yaml ====================

# FILE: configs/trends.yaml ("Flagship Edition")

# 定义AI研究领域及其子方向的关键词。这是一个全面、层级化、与时俱进的知识库。
# 关键词不区分大小写，并经过优化以提高匹配准确率。

# --- 大语言模型与基础模型 (LLMs & Foundation Models) ---
"LLMs & Foundation Models":
  keywords: ["language model", "foundation model", "llm", "large model"]
  sub_fields:
    "LLM Alignment & RLHF/DPO": ["alignment", "rlhf", "dpo", "instruction tuning", "human feedback", "constitutional ai", "preference optimization"]
    "LLM Evaluation": ["llm evaluation", "benchmark", "hallucination", "llm robustness", "truthfulness"]
    "LLM Reasoning & Planning": ["reasoning", "chain-of-thought", "tree-of-thought", "self-consistency", "planning"]
    "LLM-Based Agents": ["llm agent", "tool use", "toolformer", "react"]
    "Parameter-Efficient Fine-tuning (PEFT)": ["parameter-efficient", "peft", "lora", "qlora", "adapter tuning", "soft prompts"]
    "Retrieval-Augmented Generation (RAG)": ["retrieval-augmented", "rag", "in-context learning", "knowledge retrieval"]
    "Mixture of Experts (MoE)": ["mixture of experts", "moe", "sparse model"]
    "State Space Models (Mamba)": ["state space model", "ssm", "mamba", "s4"]
    "World Models": ["world model", "generative world model", "learning world models"]

# --- 多模态 AI (Multimodal AI) ---
"Multimodal AI":
  keywords: ["multimodal", "multi-modal", "cross-modal"]
  sub_fields:
    "Visual-Language Models (VLM)": ["visual-language", "vlm", "multi-modal llm", "vision-language", "llava", "gpt-4v"]
    "Text-to-Image Generation": ["text-to-image", "dall-e", "stable diffusion", "midjourney", "image generation"]
    "Video Generation & Editing": ["video generation", "video editing", "text-to-video", "sora", "video synthesis"]
    "Speech & Audio Generation": ["speech synthesis", "text-to-speech", "tts", "audio generation", "voice conversion"]
    "General Multimodality": ["audio-visual", "text-video", "image-audio", "speech recognition"] # 捕捉非VLM的多模态组合

# --- 计算机视觉 (CV) ---
"Computer Vision":
  keywords: ["image", "vision", "visual", "cnn", "convolutional", "scene"]
  sub_fields:
    "Diffusion Models & Generative Theory": ["diffusion model", "denoising diffusion", "score-based", "generative model"]
    "3D Vision & Gaussian Splatting": ["3d vision", "gaussian splatting", "nerf", "neural radiance", "reconstruction", "point cloud", "view synthesis"]
    "Object Detection & Segmentation": ["object detection", "segmentation", "yolo", "mask r-cnn", "instance segmentation", "panoptic"]
    "Video Understanding": ["video understanding", "action recognition", "video classification", "temporal understanding"]
    "Image Restoration": ["image restoration", "super-resolution", "denoising", "deblurring"]
    "Visual Transformers (ViT)": ["vision transformer", "vit", "visual transformer"]
    "Self-Supervised Learning (CV)": ["self-supervised", "contrastive learning", "simclr", "moco", "byol", "masked image modeling"]

# --- 自然语言处理 (NLP) ---
# Note: 很多NLP任务正被LLMs subsume，这里保留更经典的或非LLM-centric的任务
"Natural Language Processing":
  keywords: ["natural language", "nlp", "text", "corpus", "linguistic"]
  sub_fields:
    "Code Generation": ["code generation", "text-to-code", "program synthesis", "alphacode"]
    "Machine Translation": ["machine translation", "nmt", "cross-lingual"]
    "Information Extraction": ["information extraction", "named entity recognition", "ner", "relation extraction"]
    "Summarization": ["summarization", "text summarization", "abstractive", "extractive"]

# --- 强化学习 (RL) ---
"Reinforcement Learning":
  keywords: ["reinforcement learning", "rl", "q-learning", "reward", "policy", "markov decision"]
  sub_fields:
    "Reinforcement Learning (Algorithms)": ["actor-critic", "a2c", "a3c", "policy gradient", "sac", "ppo", "td3"]
    "Offline & Imitation Learning": ["offline rl", "imitation learning", "behavioral cloning", "inverse rl"]
    "Multi-Agent RL (MARL)": ["multi-agent rl", "marl", "cooperative", "competitive"]
    "Human Motion Generation": ["motion generation", "humanoid", "locomotion", "character animation"]

# --- 机器学习核心 (Core ML) ---
"Core Machine Learning":
  keywords: ["learning", "model", "network", "algorithm", "theory"]
  sub_fields:
    "Federated Learning (FL)": ["federated learning", "fl", "decentralized learning"]
    "Continual Learning": ["continual learning", "lifelong learning", "catastrophic forgetting"]
    "Transfer Learning": ["transfer learning", "domain adaptation", "fine-tuning"]
    "Meta-Learning": ["meta-learning", "learning to learn", "few-shot learning", "maml"]
    "Self-Supervised Learning (General)": ["self-supervised", "ssl", "contrastive learning"] # For non-CV applications
    "Graph Neural Networks (GNN)": ["graph neural network", "gnn", "graph representation", "message passing"]
    "Transformers & Attention": ["transformer", "attention mechanism", "self-attention"] # General, non-visual
    "Causal Discovery & Inference": ["causal discovery", "causal inference", "structural causal model", "scm", "treatment effect"]
    "Optimization Algorithms": ["optimization", "sgd", "adam", "gradient descent", "convergence", "second-order"]
    "Bayesian Methods": ["bayesian", "gaussian process", "variational inference", "probabilistic model"]
    "Quantization & Pruning": ["quantization", "pruning", "model compression", "8-bit", "4-bit", "int8", "binarization"]

# --- AI伦理、安全与可解释性 (Trustworthy AI) ---
"Trustworthy AI":
  keywords: ["trustworthy", "responsible", "ethical"]
  sub_fields:
    "Adversarial Robustness & Attacks": ["adversarial attack", "adversarial robustness", "defense", "adversarial example"]
    "Differential Privacy (DP)": ["differential privacy", "dp-sgd", "privacy-preserving", "private ml"]
    "AI Fairness & Bias": ["fairness", "bias", "algorithmic fairness", "group fairness", "debiasing"]
    "Model Interpretability (XAI)": ["interpretability", "explainable ai", "xai", "shap", "lime", "feature attribution"]
    "LLM Safety & Jailbreaking": ["llm safety", "jailbreaking", "red teaming", "model guardrails"] # LLM-specific safety

# --- AI for Science & Society ---
"AI for Science & Society":
  keywords: ["ai for", "applications", "applied ai"]
  sub_fields:
    "AI for Drug/Molecule Science": ["drug discovery", "molecule generation", "protein folding", "alphafold", "computational biology"]
    "AI for Healthcare": ["healthcare", "medical image", "ecg", "eeg", "patient data", "clinical notes", "radiology"]
    "AI for Weather & Climate": ["weather forecasting", "climate modeling", "physics-informed", "pinn"]
    "Robotics": ["robotics", "robot learning", "manipulation", "control", "embodied ai"]
    "Recommender Systems": ["recommender system", "collaborative filtering", "recommendation"]
    "AI for Chip Design (EDA)": ["chip design", "eda", "electronic design automation", "placement", "routing"]
    "Time Series Forecasting": ["time series", "forecasting", "temporal data", "sequential data"]

# --- 未来可扩展的“第三层级”结构示例 (代码暂不支持) ---
# "Example with Sub-Sub-Fields":
#  keywords: ["example"]
#  sub_fields:
#    "Generative Vision":
#      keywords: ["generative vision"]
#      sub_sub_fields:
#        "GANs": ["gan", "generative adversarial"]
#        "Diffusion Models": ["diffusion", "ddpm"]
#        "VAEs": ["variational autoencoder", "vae"]
#        "Autoregressive Models": ["pixelcnn", "imagen"]

==================== End of: configs\trends.yaml ====================



==================== Start of: src\__init__.py ====================

# FILE: src/__init__.py

# This file makes the 'src' directory a Python package.

# END OF FILE: src/__init__.py

==================== End of: src\__init__.py ====================



==================== Start of: src\ai\glm_chat_service.py ====================

# FILE: src/ai/glm_chat_service.py (CLI AI Interaction Layer - v1.1)

import sys
from typing import List, Dict, Any

# --- 从 search_service 导入必要的AI相关功能和配置 ---
from src.search.search_service import (
    generate_ai_response,
    AI_CONTEXT_PAPERS,
    ZHIPUAI_API_KEY,
    Colors,  # 颜色定义
    _ai_enabled  # 检查AI是否已初始化成功
)


# --- 定义CLI专属的 print_colored 函数 ---
# 确保在AI对话循环中能够正确打印彩色文本
def print_colored(text, color, end='\n'):
    if sys.stdout.isatty():
        print(f"{color}{text}{Colors.ENDC}", end=end)
    else:
        print(text, end=end)


def start_ai_chat_session(search_results: List[Dict[str, Any]]):
    """
    启动一个AI对话会话，处理CLI中的多轮交互。
    Args:
        search_results (list): 当前搜索结果的论文列表。
    """
    if not ZHIPUAI_API_KEY:
        print_colored("[!] 错误: 未找到 ZHIPUAI_API_KEY。请在 .env 文件中配置。", Colors.FAIL)
        return
    if not _ai_enabled:
        print_colored("[!] 错误: AI客户端初始化失败，无法启动对话。", Colors.FAIL)
        return
    if not search_results:
        print_colored("[!] 没有可供对话的搜索结果，请先执行一次查询。", Colors.WARNING)
        return

    print_colored("\n--- 🤖 AI 对话模式 ---", Colors.HEADER)
    print_colored(f"我已经阅读了与您查询最相关的 {AI_CONTEXT_PAPERS} 篇论文，请就这些论文向我提问。", Colors.OKCYAN)
    print_colored("输入 'exit' 或 'quit' 结束对话。", Colors.OKCYAN)

    messages = []

    initial_assistant_message = "好的，我已经理解了这几篇论文的核心内容。请问您想了解什么？"
    print(f"\nAI助手 > {initial_assistant_message}")
    messages.append({"role": "assistant", "content": initial_assistant_message})

    while True:
        try:
            user_question = input(f"\n{Colors.BOLD}您的问题是?{Colors.ENDC} > ").strip()
            if not user_question: continue
            if user_question.lower() in ['exit', 'quit']: break

            messages.append({"role": "user", "content": user_question})

            print_colored("🤖 GLM-4.5 正在思考...", Colors.OKCYAN, end="", flush=True)

            ai_response_content = generate_ai_response(
                chat_history=messages,
                search_results_context=search_results
            )

            print("\r" + " " * 30 + "\r", end="")  # 清除 "思考中..." 提示

            if ai_response_content.startswith("[!]"):
                print_colored(f"\nAI助手 > {ai_response_content}", Colors.FAIL)
            else:
                print_colored(f"\nAI助手 > {ai_response_content}")
                messages.append({"role": "assistant", "content": ai_response_content})

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"\n[!] 调用AI时出错: {e}", Colors.FAIL)
            break

==================== End of: src\ai\glm_chat_service.py ====================



==================== Start of: src\ai\__init__.py ====================

# FILE: src/ai/__init__.py
# Makes 'ai' a Python package.

==================== End of: src\ai\__init__.py ====================



==================== Start of: src\analysis\analyzer.py ====================

# FILE: src/analysis/analyzer.py

import re
import nltk
from wordcloud import WordCloud
from collections import Counter
from pathlib import Path

# --- NLTK Data Check ---
try:
    from nltk.corpus import stopwords

    STOPWORDS = set(stopwords.words('english'))
except LookupError:
    # This block executes if the stopwords data is not found.
    # We guide the user to download it manually for reliability.
    print("-" * 80)
    print("!!! NLTK DATA NOT FOUND !!!")
    print("Required 'stopwords' data package is missing.")
    print("Please run the following command in your terminal once to download it:")
    print("\n    python -m nltk.downloader stopwords\n")
    print("-" * 80)
    # Exit gracefully instead of attempting a download, which can be unreliable.
    exit(1)

# Add custom stopwords relevant to academic papers
CUSTOM_STOPWORDS = {
    'abstract', 'paper', 'introduction', 'method', 'methods', 'results', 'conclusion',
    'propose', 'proposed', 'present', 'presents', 'show', 'demonstrate', 'model', 'models',
    'state', 'art', 'state-of-the-art', 'sota', 'approach', 'novel', 'work', 'based',
    'data', 'dataset', 'datasets', 'training', 'learning', 'network', 'networks',
    'performance', 'task', 'tasks', 'key', 'using', 'use', 'et', 'al', 'figure',
    'table', 'results', 'analysis', 'system', 'systems', 'research', 'deep', 'large',
    'also', 'however', 'framework', 'well', 'effective', 'efficient'
}
ALL_STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)


def clean_text(text: str) -> list:
    """Cleans and tokenizes text, removing stopwords and non-alphanumeric characters."""
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    tokens = text.split()
    return [word for word in tokens if word.isalpha() and word not in ALL_STOPWORDS and len(word) > 2]


def generate_wordcloud_from_papers(papers: list, output_path: Path) -> bool:
    """
    Generates and saves a word cloud image from the titles and abstracts of papers.
    Returns True if successful, False otherwise.
    """
    if not papers:
        return False

    # Combine all titles and abstracts into a single string
    full_text = " ".join([p.get('title', '') + " " + p.get('abstract', '') for p in papers])

    if not full_text.strip():
        print("Warning: No text available to generate word cloud.")
        return False

    word_tokens = clean_text(full_text)

    if not word_tokens:
        print("Warning: No valid words left after cleaning to generate word cloud.")
        return False

    word_freq = Counter(word_tokens)

    try:
        wc = WordCloud(width=1200, height=600, background_color="white", collocations=False).generate_from_frequencies(
            word_freq)
        wc.to_file(str(output_path))
        print(f"Word cloud generated and saved to {output_path}")
        return True
    except Exception as e:
        print(f"Error generating word cloud: {e}")
        return False

# END OF FILE: src/analysis/analyzer.py

==================== End of: src\analysis\analyzer.py ====================



==================== Start of: src\analysis\trends.py ====================

# FILE: src/analysis/trends.py

import yaml
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import matplotlib.ticker as mtick

from src.crawlers.config import ROOT_DIR, get_logger

logger = get_logger(__name__)
TREND_CONFIG_FILE = ROOT_DIR / "configs" / "trends.yaml"
sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['figure.dpi'] = 300


def _load_trend_config():
    if not TREND_CONFIG_FILE.exists():
        logger.error(f"Trend config file not found: {TREND_CONFIG_FILE}")
        return None
    with open(TREND_CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def _classify_paper_subfields(paper: dict, trend_config: dict) -> list:
    text = str(paper.get('title', '')) + ' ' + str(paper.get('abstract', ''))
    if not text.strip(): return []
    text = text.lower()
    matched = set()
    for field, data in trend_config.items():
        if 'sub_fields' not in data: continue
        for sub_field, keywords in data.get('sub_fields', {}).items():
            if not isinstance(keywords, list): continue
            keyword_pattern = r'\b(' + '|'.join(re.escape(k) for k in keywords) + r')\b'
            if re.search(keyword_pattern, text, re.IGNORECASE):
                matched.add(sub_field)
    return list(matched)


def _create_analysis_df(df: pd.DataFrame, trend_config: dict) -> pd.DataFrame:
    df['sub_fields'] = df.apply(lambda row: _classify_paper_subfields(row, trend_config), axis=1)
    df_exploded = df.explode('sub_fields').dropna(subset=['sub_fields'])
    if df_exploded.empty:
        return pd.DataFrame()

    stats = df_exploded.groupby('sub_fields').size().reset_index(name='paper_count')

    if 'avg_rating' in df_exploded.columns and not df_exploded['avg_rating'].isnull().all():
        avg_ratings = df_exploded.groupby('sub_fields')['avg_rating'].mean().reset_index()
        stats = pd.merge(stats, avg_ratings, on='sub_fields', how='left')

    analysis_df = stats

    if 'decision' in df_exploded.columns:
        decisions = df_exploded.groupby(['sub_fields', 'decision']).size().unstack(fill_value=0)
        analysis_df = pd.merge(analysis_df, decisions, on='sub_fields', how='left').fillna(0)

        for dtype in ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']:
            if dtype not in analysis_df.columns:
                analysis_df[dtype] = 0

        accepted = analysis_df.get('Oral', 0) + analysis_df.get('Spotlight', 0) + analysis_df.get('Poster', 0)
        total_decision = accepted + analysis_df.get('Reject', 0)
        analysis_df['acceptance_rate'] = (accepted / total_decision.where(total_decision != 0, np.nan)).fillna(0)

    analysis_df.rename(columns={'sub_fields': 'Topic_Name'}, inplace=True)
    return analysis_df


def _plot_topic_ranking(df, metric, title, path, top_n=40):
    if metric not in df.columns:
        logger.warning(f"Metric '{metric}' not in DataFrame. Skipping plot: {title}")
        return
    df_sorted = df.dropna(subset=[metric]).sort_values(by=metric, ascending=False).head(top_n)
    if df_sorted.empty: return

    # --- 核心修复点: 为这个函数也添加最大高度限制 ---
    height = min(30, max(10, len(df_sorted) * 0.4))

    plt.figure(figsize=(16, height))
    palette = 'viridis' if metric == 'paper_count' else 'plasma_r'
    sns.barplot(x=metric, y='Topic_Name', data=df_sorted, hue='Topic_Name', palette=palette, legend=False)
    plt.title(title, fontsize=22, pad=20)
    plt.xlabel(metric.replace('_', ' ').title(), fontsize=16)
    plt.ylabel('Topic Name', fontsize=16)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(path)
    plt.close()


def _plot_decision_breakdown(df, title, path, top_n=40):
    if 'acceptance_rate' not in df.columns:
        logger.warning(f"Acceptance rate not available. Skipping plot: {title}")
        return
    df_sorted = df.sort_values(by='acceptance_rate', ascending=False).head(top_n)
    if df_sorted.empty: return
    cols = ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']
    plot_data = df_sorted.set_index('Topic_Name')[[c for c in cols if c in df_sorted.columns]]
    plot_norm = plot_data.div(plot_data.sum(axis=1), axis=0)

    # --- 核心修复点: 确保这个函数也保留了最大高度限制 ---
    height = min(30, max(12, len(plot_norm) * 0.5))

    fig, ax = plt.subplots(figsize=(20, height))
    plot_norm.plot(kind='barh', stacked=True, colormap='viridis', width=0.85, ax=ax)
    count_map = df_sorted.set_index('Topic_Name')['paper_count']
    for i, name in enumerate(plot_norm.index):
        ax.text(1.01, i, f"n={count_map.get(name, 0)}", va='center', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=24, pad=40)
    ax.set_xlabel('Proportion of Papers', fontsize=16)
    ax.set_ylabel('Topic Name (Sorted by Acceptance Rate)', fontsize=16)
    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))
    ax.set_xlim(0, 1)
    ax.invert_yaxis()
    ax.legend(title='Decision Type', loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=5, frameon=False)
    plt.tight_layout(rect=[0, 0, 0.95, 1])
    plt.savefig(path)
    plt.close()


def _save_summary_table(df, title, path_base, top_n=65):
    if 'acceptance_rate' not in df.columns:
        logger.warning(f"Acceptance rate not available. Skipping summary table: {title}")
        return
    df_sorted = df.sort_values(by='acceptance_rate', ascending=False).head(top_n)
    if df_sorted.empty: return
    cols = ['Topic_Name', 'paper_count', 'avg_rating', 'acceptance_rate', 'Oral', 'Spotlight', 'Poster', 'Reject',
            'N/A']
    final_table = df_sorted[[c for c in cols if c in df_sorted.columns]]
    final_table.to_csv(f"{path_base}.csv", index=False, encoding='utf-8-sig')
    styler = final_table.style.format({'avg_rating': '{:.2f}', 'acceptance_rate': '{:.2%}'}) \
        .bar(subset=['paper_count'], color='#6495ED', align='zero') \
        .bar(subset=['avg_rating'], color='#FFA07A', align='mean') \
        .background_gradient(subset=['acceptance_rate'], cmap='summer_r') \
        .set_caption(title) \
        .set_table_styles([{'selector': 'th, td', 'props': [('text-align', 'center')]}])
    with open(f"{path_base}.html", 'w', encoding='utf-8') as f:
        f.write(styler.to_html())


def _plot_cross_year_trends(df, title, path):
    df_exploded = df.explode('sub_fields').dropna(subset=['sub_fields'])
    if df_exploded.empty or df_exploded['year'].nunique() < 2:
        logger.warning(f"Skipping cross-year trend plot for '{title}': requires data from at least 2 years.")
        return
    pivot = df_exploded.groupby(['year', 'sub_fields']).size().unstack(fill_value=0)
    top_sub_fields = pivot.sum().nlargest(12).index
    pivot = pivot[top_sub_fields]
    pivot_percent = pivot.div(pivot.sum(axis=1), axis=0) * 100
    pivot_percent.sort_index(inplace=True)
    plt.figure(figsize=(16, 9))
    plt.stackplot(pivot_percent.index, pivot_percent.T.values, labels=pivot_percent.columns, alpha=0.8)
    plt.title(title, fontsize=22, weight='bold')
    plt.xlabel('Year', fontsize=16)
    plt.ylabel('Percentage of Papers (%)', fontsize=16)
    plt.xticks(pivot_percent.index.astype(int))
    plt.legend(title='Top Sub-Fields', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout(rect=[0, 0, 0.82, 1])
    plt.savefig(path)
    plt.close()


def run_single_task_analysis(papers: list, task_name: str, output_dir: Path):
    trend_config = _load_trend_config()
    if not trend_config or not papers: return

    df = pd.DataFrame(papers)
    analysis_df = _create_analysis_df(df, trend_config)
    if analysis_df.empty:
        logger.warning(f"No topics matched for {task_name}, skipping analysis plots.")
        return

    _plot_topic_ranking(analysis_df, 'paper_count', f"Topic Hotness at {task_name}", output_dir / "1_topic_hotness.png")

    has_review_data = 'avg_rating' in analysis_df.columns and 'acceptance_rate' in analysis_df.columns
    if has_review_data:
        _plot_topic_ranking(analysis_df, 'avg_rating', f"Topic Quality at {task_name}",
                            output_dir / "2_topic_quality.png")
        _plot_decision_breakdown(analysis_df, f"Decision Breakdown at {task_name}",
                                 output_dir / "3_decision_breakdown.png")
        _save_summary_table(analysis_df, f"Summary Table for {task_name}", output_dir / "4_summary_table")
    else:
        # 这个日志只会在没有审稿数据的任务中打印，是正常的
        logger.info(f"Skipping review-based analysis for {task_name}: missing review data.")

    logger.info(f"Single-task analysis for {task_name} completed.")


def run_cross_year_analysis(papers: list, conference_name: str, output_dir: Path):
    trend_config = _load_trend_config()
    if not trend_config or not papers: return

    df = pd.DataFrame(papers)
    if 'year' not in df.columns or df['year'].isnull().all():
        logger.warning(f"Skipping cross-year analysis for {conference_name}: 'year' column not found or is empty.")
        return

    df['sub_fields'] = df.apply(lambda row: _classify_paper_subfields(row, trend_config), axis=1)

    _plot_cross_year_trends(
        df,
        f"Sub-Field Trends at {conference_name} Over Time",
        output_dir / f"trends_{conference_name}.png"
    )
    logger.info(f"Cross-year analysis for {conference_name} completed.")

==================== End of: src\analysis\trends.py ====================



==================== Start of: src\analysis\__init__.py ====================

# FILE: src/analysis/__init__.py

# This file makes the 'analysis' directory a Python package.

# END OF FILE: src/analysis/__init__.py


==================== End of: src\analysis\__init__.py ====================



==================== Start of: src\api\main.py ====================

# FILE: src/api/main.py (FastAPI Backend for PubCrawler)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import sys

# --- 从 search_service 导入核心逻辑和初始化函数 ---
from src.search.search_service import (
    keyword_search,
    semantic_search,
    get_stats_summary,
    generate_ai_response,
    _initialize_search_components,
    _sqlite_conn  # 用于FastAPI关闭时关闭连接
)
from src.search.search_service import ZHIPUAI_API_KEY  # 导入API Key，用于检查AI可用性

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="PubCrawler AI Assistant API",
    description="为AI学术研究助手提供搜索、统计和AI对话功能。",
    version="1.0.0",
)


# --- Pydantic 模型用于请求体和响应 ---
class SearchQuery(BaseModel):
    query: str = Field(..., description="搜索查询字符串，以 'sem:' 开头表示语义搜索，否则为关键词搜索。")
    top_n: int = Field(20, description="语义搜索时返回的最相关论文数量。", ge=1, le=100)


class SearchResultPaper(BaseModel):
    title: str
    authors: str
    abstract: str
    conference: str
    year: str
    similarity: Optional[float] = None  # 语义搜索结果可能包含相似度


class SearchStats(BaseModel):
    total_found: int
    distribution: Dict[str, int]


class SearchResponse(BaseModel):
    results: List[SearchResultPaper]
    stats: SearchStats
    message: str = "搜索成功。"


class AIChatMessage(BaseModel):
    role: str
    content: str


class AIChatRequest(BaseModel):
    chat_history: List[AIChatMessage] = Field(..., description="之前的聊天历史，不包含当前用户消息。")
    current_message: str = Field(..., description="用户当前的最新消息。")
    search_results_context: List[SearchResultPaper] = Field(..., description="提供给AI作为上下文的搜索结果论文列表。")


class AIChatResponse(BaseModel):
    response: str
    message: str = "AI响应成功。"


# --- 生命周期事件 (启动时初始化组件，关闭时清理资源) ---
@app.on_event("startup")
async def startup_event():
    print("[*] FastAPI应用启动中，正在初始化搜索组件...")
    try:
        _initialize_search_components()
        print("[✔] 搜索组件初始化完成。")
    except Exception as e:
        print(f"[✖] 搜索组件初始化失败: {e}")
        # 这里可以选择终止应用启动，或者在后续API调用中处理错误


@app.on_event("shutdown")
async def shutdown_event():
    print("[*] FastAPI应用关闭中，正在清理资源...")
    if _sqlite_conn:
        _sqlite_conn.close()
        print("[✔] SQLite连接已关闭。")


# --- API 路由 ---
@app.post("/search", response_model=SearchResponse)
async def perform_search(search_query: SearchQuery):
    """
    执行关键词或语义搜索，并返回论文列表及统计信息。
    - 以 'sem:' 开头的查询字符串将触发语义搜索。
    - 其他查询字符串将触发关键词搜索。
    """
    query_text = search_query.query.strip()

    if query_text.lower().startswith('sem:'):
        actual_query = query_text[4:].strip()
        if not actual_query:
            raise HTTPException(status_code=400, detail="语义搜索查询内容不能为空。")
        results = semantic_search(actual_query, top_n=search_query.top_n)
    else:
        results = keyword_search(query_text)

    if not results:
        return SearchResponse(results=[], stats={"total_found": 0, "distribution": {}}, message="未找到相关结果。")

    stats_summary = get_stats_summary(results)
    return SearchResponse(results=results, stats=stats_summary, message="搜索成功。")


@app.post("/chat", response_model=AIChatResponse)
async def chat_with_ai(chat_request: AIChatRequest):
    """
    与AI助手进行对话，提供聊天历史和搜索结果作为上下文。
    """
    if not ZHIPUAI_API_KEY:
        raise HTTPException(status_code=503, detail="智谱AI API Key未配置，AI服务不可用。")

    if not chat_request.search_results_context:
        raise HTTPException(status_code=400, detail="未提供搜索结果上下文，AI无法进行对话。")

    # 构造完整的消息历史，包括系统消息和用户背景消息，然后传入AI服务
    # generate_ai_response 函数会处理系统消息和背景知识的注入

    # chat_history 包含之前的对话，current_message 是用户最新输入
    full_chat_history_for_service = chat_request.chat_history + [
        {"role": "user", "content": chat_request.current_message}]

    ai_response_text = generate_ai_response(
        chat_history=full_chat_history_for_service,
        search_results_context=[p.dict() for p in chat_request.search_results_context]  # 确保传递的是字典列表
    )

    if ai_response_text.startswith("[!]"):
        raise HTTPException(status_code=500, detail=ai_response_text)

    return AIChatResponse(response=ai_response_text)

==================== End of: src\api\main.py ====================



==================== Start of: src\api\__init__.py ====================

# FILE: src/api/__init__.py
# Makes 'api' a Python package.

==================== End of: src\api\__init__.py ====================



==================== Start of: src\crawlers\config.py ====================

# FILE: src/config.py (Structured Logging)

import logging
from pathlib import Path

# 导入 Tqdm 日志处理器和彩色格式化器
from src.utils.tqdm_logger import TqdmLoggingHandler
from src.utils.console_logger import ColoredFormatter, COLORS

# --- Project Structure ---
# 多加一个 .parent 就能正确地回到项目根目录
ROOT_DIR = Path(__file__).parent.parent.parent

OUTPUT_DIR = ROOT_DIR / "output"
LOG_DIR = ROOT_DIR / "logs" # <-- 现在这个 logs 路径也正确了
CONFIG_FILE = ROOT_DIR / "configs" / "tasks.yaml" # <-- 现在这个 configs 路径也正确了

METADATA_OUTPUT_DIR = OUTPUT_DIR / "metadata"
PDF_DOWNLOAD_DIR = OUTPUT_DIR / "pdfs"
TRENDS_OUTPUT_DIR = OUTPUT_DIR / "trends"

# --- Create Directories ---
# 这部分代码现在可以正常工作了
OUTPUT_DIR.mkdir(exist_ok=True)
LOG_DIR.mkdir(exist_ok=True)


# --- Logging Configuration (核心修改点) ---
def get_logger(name: str, log_file: Path = LOG_DIR / "pubcrawler.log") -> logging.Logger:
    """
    配置并返回一个日志记录器。
    - 控制台输出: 简洁、彩色、结构化的信息。
    - 文件输出: 包含完整 Traceback 的详细信息，用于调试。
    """
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)

        # 1. 控制台处理器 (使用 Tqdm 安全处理器和新的结构化彩色格式)
        tqdm_handler = TqdmLoggingHandler()
        tqdm_handler.setLevel(logging.INFO)
        # --- 新的结构化格式 ---
        # %(levelname)s 会被 ColoredFormatter 转换成带颜色的标识
        console_format = f"{COLORS['STEP']}[%(levelname)s]{COLORS['RESET']} %(message)s"
        console_formatter = ColoredFormatter(console_format)
        tqdm_handler.setFormatter(console_formatter)

        # 2. 文件处理器 (保持不变，用于记录全部细节)
        file_handler = logging.FileHandler(log_file, 'a', encoding='utf-8') # 使用 'a' 模式追加日志
        file_handler.setLevel(logging.INFO)
        file_format = '%(asctime)s - %(name)s - [%(levelname)s] - %(message)s'
        file_formatter = logging.Formatter(file_format)
        file_handler.setFormatter(file_formatter)

        logger.addHandler(tqdm_handler)
        logger.addHandler(file_handler)

        logger.propagate = False # 防止日志向上传播到 root logger

    return logger

==================== End of: src\crawlers\config.py ====================



==================== Start of: src\crawlers\models.py ====================

# FILE: src/models.py

from dataclasses import dataclass, field
from typing import List, Optional


@dataclass
class Paper:
    """
    一个用于存储论文信息的数据类，确保所有 scraper 返回统一的结构。
    """
    id: str
    title: str
    authors: List[str]
    summary: str
    published_date: str
    updated_date: str

    pdf_url: Optional[str] = None
    categories: List[str] = field(default_factory=list)
    primary_category: Optional[str] = None

    # 发表信息
    journal_ref: Optional[str] = None
    doi: Optional[str] = None

    # 额外备注，例如项目主页
    comment: Optional[str] = None

    # 作者及其单位的详细信息
    author_details: List[str] = field(default_factory=list)

# END OF FILE: src/models.py

==================== End of: src\crawlers\models.py ====================



==================== Start of: src\crawlers\processor.py ====================

# FILE: src/processor.py

import logging
import os
import requests
import zipfile
import re
from typing import Iterator, Dict, Any
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class Processor:
    """
    Processes a stream of paper data to generate output files.
    - A summary.txt file for LLM analysis.
    - A compressed .zip file containing all downloaded PDFs.
    """

    def __init__(self, output_dir: str = 'output', download_pdfs: bool = False):
        self.output_dir = output_dir
        self.download_pdfs = download_pdfs
        self.summary_path = os.path.join(self.output_dir, 'summary.txt')
        self.zip_path = os.path.join(self.output_dir, 'papers.zip')

        # Ensure output directory exists
        os.makedirs(self.output_dir, exist_ok=True)

    def _sanitize_filename(self, title: str) -> str:
        """Creates a safe filename from a paper title."""
        # Remove invalid characters
        sanitized = re.sub(r'[\\/*?:"<>|]', "", title)
        # Truncate to a reasonable length
        return (sanitized[:100] + '.pdf') if len(sanitized) > 100 else (sanitized + '.pdf')

    def _format_summary_entry(self, paper_data: Dict[str, Any]) -> str:
        """Formats a single paper's data into the specified text format."""
        # Safely get all required fields
        title = paper_data.get('title', 'N/A')
        authors = ", ".join(paper_data.get('authors', []))
        conference = paper_data.get('conference', 'N/A')
        year = paper_data.get('year', 'N/A')
        source_url = paper_data.get('source_url', 'N/A')
        pdf_link = paper_data.get('pdf_link', 'N/A')
        abstract = paper_data.get('abstract', 'No abstract available.')
        reviews = paper_data.get('reviews', [])

        # Build the entry string
        entry = []
        entry.append("=" * 80)
        entry.append(f"Title: {title}")
        entry.append(f"Authors: {authors}")
        entry.append(f"Conference: {conference} {year}")
        entry.append(f"Source URL: {source_url}")
        entry.append(f"PDF Link: {pdf_link}")
        entry.append("\n--- Abstract ---")
        entry.append(abstract)

        if reviews:
            entry.append(f"\n--- Reviews ({len(reviews)}) ---")
            for i, review in enumerate(reviews, 1):
                review_title = review.get('title', 'N/A')
                review_comment = review.get('comment', 'No comment.')
                review_decision = review.get('decision', None)
                review_rating = review.get('rating', None)

                entry.append(f"\n[Review {i}]")
                entry.append(f"Title: {review_title}")
                if review_decision:
                    entry.append(f"Decision: {review_decision}")
                if review_rating:
                    entry.append(f"Rating: {review_rating}")
                entry.append(f"Comment: {review_comment}")

        entry.append("=" * 80 + "\n\n")
        return "\n".join(entry)

    def _download_pdf(self, pdf_url: str, filename: str, zip_file: zipfile.ZipFile):
        """Downloads a PDF in streaming fashion and adds it to the zip archive."""
        if not pdf_url:
            logging.warning(f"Skipping download for '{filename}' due to missing URL.")
            return

        temp_pdf_path = os.path.join(self.output_dir, filename)
        try:
            logging.info(f"Downloading: {pdf_url}")
            with requests.get(pdf_url, stream=True, timeout=30, headers=HEADERS) as r:
                r.raise_for_status()
                with open(temp_pdf_path, 'wb') as f:
                    # Download in chunks to keep memory usage low
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)

            # Add the downloaded file to the zip archive
            zip_file.write(temp_pdf_path, arcname=filename)
            logging.info(f"Added to zip: {filename}")

        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to download {pdf_url}: {e}")
        except Exception as e:
            logging.error(f"An error occurred while handling {filename}: {e}")
        finally:
            # Clean up the temporary PDF file
            if os.path.exists(temp_pdf_path):
                os.remove(temp_pdf_path)

    def process_papers(self, papers_iterator: Iterator[Dict[str, Any]], total: int):
        """
        The main processing pipeline. Iterates through papers and writes to files.
        """
        logging.info("Starting paper processing pipeline...")
        logging.info(f"Summary will be saved to: {self.summary_path}")
        if self.download_pdfs:
            logging.info(f"PDFs will be saved to: {self.zip_path}")
        else:
            logging.info("PDF download is disabled.")

        # Clear summary file at the start of a run
        with open(self.summary_path, 'w', encoding='utf-8') as f:
            f.write("--- PubCrawler Summary ---\n\n")

        try:
            with zipfile.ZipFile(self.zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Use tqdm for a nice progress bar
                pbar = tqdm(papers_iterator, total=total, desc="Processing papers")
                for paper_data in pbar:
                    # 1. Format and append to summary.txt
                    summary_entry = self._format_summary_entry(paper_data)
                    with open(self.summary_path, 'a', encoding='utf-8') as f:
                        f.write(summary_entry)

                    # 2. Download PDF if enabled
                    if self.download_pdfs:
                        filename = self._sanitize_filename(paper_data.get('title', 'untitled'))
                        self._download_pdf(paper_data.get('pdf_link'), filename, zipf)

        except Exception as e:
            logging.error(f"A critical error occurred during processing: {e}")

        logging.info("Processing pipeline complete.")

# END OF FILE: src/processor.py

==================== End of: src\crawlers\processor.py ====================



==================== Start of: src\crawlers\run_crawler.py ====================

# FILE: src/main.py (Optimized for Memory)

import time
import yaml
import re
import pandas as pd
from collections import defaultdict
from pathlib import Path
from tqdm import tqdm

# --- 导入所有独立的 Scraper ---
from src.scrapers.iclr_scraper import IclrScraper
from src.scrapers.neurips_scraper import NeuripsScraper
from src.scrapers.icml_scraper import IcmlScraper
from src.scrapers.acl_scraper import AclScraper
from src.scrapers.arxiv_scraper import ArxivScraper
from src.scrapers.cvf_scraper import CvfScraper
from src.scrapers.aaai_scraper import AaaiScraper
from src.scrapers.kdd_scraper import KddScraper

from src.crawlers.config import get_logger, CONFIG_FILE, METADATA_OUTPUT_DIR, PDF_DOWNLOAD_DIR, TRENDS_OUTPUT_DIR, \
    LOG_DIR
from src.scrapers.tpami_scraper import TpamiScraper
# --- 【修改点】: 导入 save_as_markdown 和 generate_wordcloud_from_papers ---
from src.utils.formatter import save_as_csv, save_as_markdown
from src.analysis.analyzer import generate_wordcloud_from_papers
# ----------------------------------------------------------------------
from src.utils.downloader import download_single_pdf
from src.analysis.trends import run_single_task_analysis, run_cross_year_analysis
from src.utils.console_logger import print_banner, COLORS

OPERATION_MODE = "collect_and_analyze"

logger = get_logger(__name__)

# --- Scraper 和 Conference 定义 (保持不变) ---
SCRAPER_MAPPING = {"iclr": IclrScraper, "neurips": NeuripsScraper, "icml": IcmlScraper, "acl": AclScraper,
                   "cvf": CvfScraper, "aaai": AaaiScraper, "kdd": KddScraper, "arxiv": ArxivScraper,
                   "tpami": TpamiScraper}
CONF_TO_DEF_SOURCE = {'ICLR': 'openreview', 'NeurIPS': 'openreview', 'ICML': 'html_pmlr', 'ACL': 'html_acl',
                      'EMNLP': 'html_acl', 'NAACL': 'html_acl', 'CVPR': 'html_cvf', 'ICCV': 'html_cvf',
                      'AAAI': 'selenium', 'KDD': 'selenium'}

# 定义哪些爬虫类型支持并发优化，以便在主程序中给出提示
CONCURRENT_SCRAPER_TYPES = ['acl', 'cvf']


def load_config():
    if not CONFIG_FILE.exists():
        logger.error(f"[✖ ERROR] Config file not found at {CONFIG_FILE}")
        return None
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def build_task_info(task: dict, source_definitions: dict) -> dict:
    # 此函数逻辑保持不变
    task_info = task.copy()
    conf, year, source_type = task.get('conference'), task.get('year'), task.get('source_type')
    if source_type in ['arxiv', 'tpami']: return task_info
    if not conf or not year:
        logger.error(f"[✖ ERROR] Task '{task.get('name')}' is missing 'conference' or 'year'.");
        return None
    def_source_key = CONF_TO_DEF_SOURCE.get(conf)
    if not def_source_key:
        logger.error(f"[✖ ERROR] No definition source found for conference '{conf}'.");
        return None
    if 'url_override' not in task:
        try:
            definition = source_definitions[def_source_key][conf]
            if isinstance(definition, dict):
                if 'venue_id' in definition:
                    task_info['venue_id'] = definition['venue_id'].replace('YYYY', str(year))
                    task_info['api_version'] = 'v1' if year in definition.get('api_v1_years', []) else 'v2'
                elif 'pattern_map' in definition:
                    base_url = "https://aclanthology.org/"
                    pattern = definition['pattern_map'].get(year)
                    if not pattern:
                        logger.error(f"[✖ ERROR] No URL pattern defined for {conf} in year {year}");
                        return None
                    task_info['url'] = f"{base_url}{pattern}/"
            else:
                task_info['url'] = definition.replace('YYYY', str(year))
        except KeyError:
            logger.error(f"[✖ ERROR] No definition for source='{def_source_key}' and conf='{conf}'");
            return None
    else:
        task_info['url'] = task['url_override']
    return task_info


def filter_papers(papers: list, filters: list) -> list:
    # 此函数逻辑保持不变
    if not filters: return papers
    original_count = len(papers)
    filter_regex = re.compile('|'.join(filters), re.IGNORECASE)
    filtered_papers = [p for p in papers if filter_regex.search(p.get('title', '') + ' ' + p.get('abstract', ''))]
    logger.info(
        f"    {COLORS['STEP']}-> Filtered papers: {original_count} -> {len(filtered_papers)} using filters: {filters}")
    return filtered_papers


def run_tasks_sequentially(tasks_to_run: list, source_definitions: dict, perform_single_analysis: bool) -> list:
    """
    顺序执行每个任务，并在每个任务完成后立即处理和保存结果，以节省内存。
    返回所有任务收集到的论文总列表，用于后续的跨年分析。
    """
    all_collected_papers = []

    for task in tasks_to_run:
        task_name = task.get('name', f"{task.get('conference')}_{task.get('year')}")
        if not task.get('enabled', False):
            continue

        logger.info(f"{COLORS['TASK_START']}[▶] STARTING TASK: {task_name}{COLORS['RESET']}")

        # --- 新增的提示信息 ---
        source_type = task.get('source_type')
        if source_type in CONCURRENT_SCRAPER_TYPES:
            max_workers = task.get('max_workers', 1)  # 默认为1保证安全
            logger.info(f"    {COLORS['STEP']}[!] 注意: 此任务类型 ({source_type}) 需要逐一访问论文详情页。")
            logger.info(
                f"    {COLORS['STEP']}    已启用 {max_workers} 个并发线程进行加速。尽管如此，如果论文数量庞大，仍可能需要较长时间。")

        scraper_class = SCRAPER_MAPPING.get(source_type)
        if not scraper_class:
            logger.error(
                f"{COLORS['ERROR']}[✖ FAILURE] No scraper for source: '{task['source_type']}'{COLORS['RESET']}\n");
            continue

        task_info = build_task_info(task, source_definitions)
        if not task_info:
            logger.error(
                f"{COLORS['ERROR']}[✖ FAILURE] Could not build task info for '{task_name}'.{COLORS['RESET']}\n");
            continue

        try:
            scraper = scraper_class(task_info, logger)
            papers = scraper.scrape()
            papers = filter_papers(papers, task.get('filters', []))

            if papers:
                for paper in papers:
                    paper['year'] = task.get('year')
                    paper['conference'] = task.get('conference')

                logger.info(f"    {COLORS['STEP']}-> Successfully processed {len(papers)} papers for '{task_name}'.")

                logger.info(f"{COLORS['PHASE']}--- Processing & Saving Results for '{task_name}' ---{COLORS['RESET']}")
                conf, year = task.get('conference', 'Misc'), task.get('year', 'Latest')

                metadata_dir = METADATA_OUTPUT_DIR / conf / str(year)
                metadata_dir.mkdir(exist_ok=True, parents=True)

                # --- 【新增功能】: 生成词云图和Markdown报告 ---
                logger.info(f"    -> Generating word cloud...")
                wordcloud_path = metadata_dir / f"{task_name}_wordcloud.png"
                wordcloud_success = generate_wordcloud_from_papers(papers, wordcloud_path)
                final_wordcloud_path = str(wordcloud_path) if wordcloud_success else None

                logger.info(f"    -> Saving results to Markdown report...")
                save_as_markdown(papers, task_name, metadata_dir, wordcloud_path=final_wordcloud_path)
                # ----------------------------------------------------

                logger.info(f"    -> Saving metadata to {metadata_dir}")
                save_as_csv(papers, task_name, metadata_dir)

                if task.get('download_pdfs', False):
                    logger.info(f"    -> Starting PDF download...")
                    pdf_dir = PDF_DOWNLOAD_DIR / conf / str(year)
                    pdf_dir.mkdir(exist_ok=True, parents=True)
                    pbar_desc = f"    -> Downloading PDFs for {task_name}"
                    for paper in tqdm(papers, desc=pbar_desc, leave=True):
                        download_single_pdf(paper, pdf_dir)

                if perform_single_analysis:
                    analysis_output_dir = metadata_dir / "analysis"
                    analysis_output_dir.mkdir(exist_ok=True)
                    logger.info(f"    -> Running single-task analysis...")
                    run_single_task_analysis(papers, task_name, analysis_output_dir)

                all_collected_papers.extend(papers)
                logger.info(
                    f"{COLORS['SUCCESS']}[✔ SUCCESS] Task '{task_name}' completed and saved.{COLORS['RESET']}\n")

            else:
                logger.warning(f"[⚠ WARNING] No papers found for task: {task_name} (or none matched filters)")
                logger.info(f"{COLORS['WARNING']}[!] Task '{task_name}' finished with no results.{COLORS['RESET']}\n")

        except Exception as e:
            logger.critical(f"任务 '{task_name}' 遭遇严重错误，已终止。错误: {e}")
            logger.info(f"详细的错误堆栈信息已记录到日志文件: {LOG_DIR / 'pubcrawler.log'}")

        time.sleep(0.5)

    return all_collected_papers


def load_all_data_for_cross_analysis(metadata_dir: Path) -> list:
    """在 'analyze' 模式下，从磁盘加载所有之前保存的 CSV 文件。"""
    if not metadata_dir.exists():
        logger.error(f"[✖ ERROR] Data directory not found: {metadata_dir}.");
        return []

    all_papers = []
    csv_files = list(metadata_dir.rglob("*_data_*.csv"))
    if not csv_files:
        logger.warning("[⚠ WARNING] No CSV data files found for cross-year analysis.");
        return []

    logger.info(f"    -> Loading {len(csv_files)} previously collected CSV file(s) from disk...")
    for csv_path in csv_files:
        try:
            df = pd.read_csv(csv_path)
            df.fillna('', inplace=True)
            all_papers.extend(df.to_dict('records'))
        except Exception as e:
            logger.error(f"[✖ ERROR] Failed to load data from {csv_path}: {e}")
    return all_papers


def main():
    print_banner()
    logger.info("=====================================================================================")
    logger.info(f"Starting PubCrawler in mode: '{OPERATION_MODE}'")
    logger.info("=====================================================================================\n")

    config = load_config()
    if not config: return

    all_papers_for_analysis = []

    if OPERATION_MODE in ["collect", "collect_and_analyze"]:
        logger.info(f"{COLORS['PHASE']}+----------------------------------------------------------+")
        logger.info(f"|    PHASE 1: PAPER COLLECTION & SINGLE-TASK ANALYSIS      |")
        logger.info(f"+----------------------------------------------------------+{COLORS['RESET']}\n")

        all_papers_for_analysis = run_tasks_sequentially(
            config.get('tasks', []),
            config.get('source_definitions', {}),
            perform_single_analysis=True
        )

    if OPERATION_MODE in ["analyze", "collect_and_analyze"]:
        logger.info(f"\n{COLORS['PHASE']}+----------------------------------------------------------+")
        logger.info(f"|          PHASE 2: CROSS-YEAR TREND ANALYSIS              |")
        logger.info(f"+----------------------------------------------------------+{COLORS['RESET']}\n")

        if OPERATION_MODE == "collect_and_analyze" and not all_papers_for_analysis:
            logger.warning("[⚠ WARNING] No data was collected in Phase 1 to perform cross-year analysis.")

        elif OPERATION_MODE == "analyze":
            all_papers_for_analysis = load_all_data_for_cross_analysis(METADATA_OUTPUT_DIR)

        if all_papers_for_analysis:
            all_data_by_conf = defaultdict(list)
            for paper in all_papers_for_analysis:
                if paper.get('conference'):
                    all_data_by_conf[paper['conference']].append(paper)

            for conference, papers in all_data_by_conf.items():
                if not papers: continue
                conf_trend_dir = TRENDS_OUTPUT_DIR / conference
                conf_trend_dir.mkdir(exist_ok=True, parents=True)
                logger.info(f"{COLORS['TASK_START']}[▶] Analyzing trends for: {conference}{COLORS['RESET']}")
                run_cross_year_analysis(papers, conference, conf_trend_dir)
                logger.info(
                    f"{COLORS['SUCCESS']}[✔ SUCCESS] Cross-year analysis for '{conference}' completed.{COLORS['RESET']}\n")

    logger.info("=====================================================================================")
    logger.info("PubCrawler run finished successfully.")
    logger.info("=====================================================================================")


if __name__ == "__main__":
    main()

==================== End of: src\crawlers\run_crawler.py ====================



==================== Start of: src\crawlers\__init__.py ====================



==================== End of: src\crawlers\__init__.py ====================



==================== Start of: src\scrapers\aaai_scraper.py ====================

# FILE: src/scrapers/aaai_scraper.py

import time
from typing import List, Dict, Any

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from .base_scraper import BaseScraper


class AaaiScraper(BaseScraper):
    """专门用于 AAAI 网站的爬虫 (使用 Selenium)。"""

    def scrape(self) -> List[Dict[str, Any]]:
        url = self.task_info["url"]
        limit = self.task_info.get("limit")

        # AAAI 特定的选择器
        paper_link_selector = 'h5.toc-title > a'

        self.logger.info(f"    -> 正在启动 Selenium 访问 (AAAI): {url}")
        driver = None
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument(
                'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            driver.get(url)
            self.logger.info("    -> 页面已加载. 等待 10 秒以处理动态内容...")
            time.sleep(10)

            link_elements = driver.find_elements(By.CSS_SELECTOR, paper_link_selector)
            if not link_elements:
                self.logger.warning(f"    -> Selenium 未找到任何论文链接，使用的选择器是: '{paper_link_selector}'")
                return []

            self.logger.info(f"    -> 找到了 {len(link_elements)} 个潜在的论文链接。")
            if limit and len(link_elements) > limit:
                self.logger.info(f"    -> 应用限制：处理前 {limit} 个链接。")
                link_elements = link_elements[:limit]

            papers = []
            for i, link_elem in enumerate(link_elements):
                paper_url = link_elem.get_attribute('href')
                paper_title = link_elem.text
                if paper_url and paper_title:
                    papers.append({
                        'id': f"aaai_{self.task_info['year']}_{i}",
                        'title': paper_title.strip(),
                        'authors': 'N/A (AAAI Selenium)',
                        'abstract': 'N/A (AAAI Selenium)',
                        'pdf_url': None,
                        'source_url': paper_url
                    })
            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] AAAI Selenium 抓取失败 {url}: {e}", exc_info=True)
            return []
        finally:
            if driver:
                driver.quit()

==================== End of: src\scrapers\aaai_scraper.py ====================



==================== Start of: src\scrapers\acl_scraper.py ====================

# FILE: src/scrapers/acl_scraper.py (Concurrent Version)

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get


class AclScraper(BaseScraper):
    """
    专门用于 ACL Anthology 网站的爬虫。
    此版本经过优化，使用多线程并发获取论文详情，以大幅提高速度。
    """

    def _scrape_details_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        抓取并解析单个 ACL 论文详情页。这是将被并发执行的核心工作函数。
        """
        response = robust_get(url, timeout=20)
        if not response:
            self.logger.debug(f"    -> 请求详情页失败 (已重试): {url}")
            return None

        try:
            soup = BeautifulSoup(response.content, 'lxml')

            title_tag = soup.select_one("h2#title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            author_tags = soup.select("p.lead a")
            authors = ", ".join([a.get_text(strip=True) for a in author_tags]) if author_tags else "N/A"

            abstract_tag = soup.select_one("div.acl-abstract > span")
            abstract = abstract_tag.get_text(strip=True) if abstract_tag else "N/A"

            pdf_url_tag = soup.select_one('meta[name="citation_pdf_url"]')
            pdf_url = pdf_url_tag['content'] if pdf_url_tag else None
            if pdf_url and not pdf_url.startswith('http'):
                pdf_url = urljoin(url, pdf_url)

            paper_id = url.strip('/').split('/')[-1]

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': url}
        except Exception as e:
            self.logger.debug(f"    -> 解析 ACL 详情页失败 {url}: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]

        # 从配置中读取并发参数，并提供安全的默认值
        max_workers = self.task_info.get("max_workers", 1)
        max_papers_limit = self.task_info.get("max_papers_limit", 0)

        # 1. 首先，获取包含所有论文链接的索引页
        self.logger.info(f"    -> 正在抓取 ACL 索引页: {index_url}")
        response = robust_get(index_url)
        if not response:
            return []

        if response.status_code == 404:
            self.logger.warning(f"    -> 页面未找到 (404): {index_url}")
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            link_tags = soup.select('p.d-sm-flex strong a.align-middle')

            detail_urls = [urljoin(index_url, tag['href']) for tag in link_tags if
                           f'{self.task_info["year"]}.acl-long.0' not in tag['href']]
            self.logger.info(f"    -> 索引页解析完成，共找到 {len(detail_urls)} 篇有效论文。")

            # 2. 应用数量限制
            urls_to_crawl = detail_urls
            if max_papers_limit > 0:
                # 智能限制：取用户设置和实际数量中的较小值
                actual_limit = min(max_papers_limit, len(detail_urls))
                urls_to_crawl = detail_urls[:actual_limit]
                self.logger.info(f"    -> 已应用数量限制，将爬取前 {len(urls_to_crawl)} 篇论文。")

            if not urls_to_crawl:
                return []

            # 3. 使用 ThreadPoolExecutor 进行并发爬取
            papers = []
            pbar_desc = f"    -> 并发解析 {self.task_info.get('conference')} 详情页"

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self._scrape_details_page, url): url for url in urls_to_crawl}

                # 使用 tqdm 显示进度
                for future in tqdm(as_completed(future_to_url), total=len(urls_to_crawl), desc=pbar_desc, leave=True):
                    result = future.result()
                    if result:
                        papers.append(result)

            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] 解析 ACL 页面时发生未知错误: {e}", exc_info=True)
            return []

==================== End of: src\scrapers\acl_scraper.py ====================



==================== Start of: src\scrapers\arxiv_scraper.py ====================

# FILE: src/scrapers/arxiv_scraper.py

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from typing import List, Dict, Any
import logging

from .base_scraper import BaseScraper


class ArxivScraper(BaseScraper):
    """Scraper for the arXiv API."""
    BASE_URL = 'http://export.arxiv.org/api/query?'

    def __init__(self, task_info: Dict[str, Any], logger: logging.Logger):
        super().__init__(task_info, logger)
        self.search_query = self.task_info.get('search_query', 'cat:cs.AI')
        self.limit = self.task_info.get('limit')
        self.max_results = self.limit if self.limit is not None else self.task_info.get('max_results', 10)
        self.sort_by = self.task_info.get('sort_by', 'submittedDate')
        self.sort_order = self.task_info.get('sort_order', 'descending')

    def _build_url(self) -> str:
        encoded_query = urllib.parse.quote(self.search_query)
        query_params = (f'search_query={encoded_query}&start=0&max_results={self.max_results}&'
                        f'sortBy={self.sort_by}&sortOrder={self.sort_order}')
        return self.BASE_URL + query_params

    def _parse_xml_entry(self, entry: ET.Element, ns: Dict[str, str]) -> Dict[str, Any]:
        def _get_text(element_name: str, namespace: str = 'atom'):
            element = entry.find(f'{namespace}:{element_name}', ns)
            return element.text.strip().replace('\n', ' ') if element is not None and element.text else None

        author_elements = entry.findall('atom:author', ns)
        authors_list = [author.find('atom:name', ns).text for author in author_elements if
                        author.find('atom:name', ns) is not None]

        pdf_url = None
        for link in entry.findall('atom:link', ns):
            if link.attrib.get('title') == 'pdf':
                pdf_url = link.attrib.get('href')
                break

        arxiv_id_url = _get_text('id')
        arxiv_id = arxiv_id_url.split('/abs/')[-1] if arxiv_id_url else "N/A"

        return {"id": arxiv_id, "title": _get_text('title'), "authors": ", ".join(authors_list),
                "abstract": _get_text('summary'), "pdf_url": pdf_url, "source_url": arxiv_id_url}

    def scrape(self) -> List[Dict[str, Any]]:
        full_url = self._build_url()
        self.logger.info(f"    -> Requesting data from arXiv: {self.search_query}")
        papers: List[Dict[str, Any]] = []
        try:
            with urllib.request.urlopen(full_url) as response:
                if response.status != 200:
                    self.logger.error(f"    [✖ ERROR] HTTP request to arXiv failed with status code: {response.status}")
                    return papers
                xml_data = response.read().decode('utf-8')
                ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}
                root = ET.fromstring(xml_data)
                entries = root.findall('atom:entry', ns)
                for entry in entries:
                    papers.append(self._parse_xml_entry(entry, ns))
                return papers
        except Exception as e:
            self.logger.error(f"    [✖ ERROR] An unexpected error occurred during arXiv scraping: {e}", exc_info=True)
            return papers

==================== End of: src\scrapers\arxiv_scraper.py ====================



==================== Start of: src\scrapers\base_scraper.py ====================

# FILE: src/scrapers/base_scraper.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any
import logging

class BaseScraper(ABC):
    """
    所有抓取器类的抽象基类。
    定义了所有具体抓取器必须遵循的接口。
    """

    def __init__(self, task_info: Dict[str, Any], logger: logging.Logger):
        """
        初始化抓取器。

        Args:
            task_info (Dict[str, Any]): 从 tasks.yaml 中读取并构建的特定任务配置。
            logger (logging.Logger): 从主程序传递过来的共享日志记录器。
        """
        self.task_info = task_info
        self.logger = logger

    @abstractmethod
    def scrape(self) -> List[Dict[str, Any]]:
        """
        执行抓取的核心方法。

        每个子类必须实现此方法，以执行其特定的抓取逻辑，
        并返回一个包含标准字典结构的论文列表。

        Returns:
            List[Dict[str, Any]]: 抓取到的论文信息列表。
        """
        raise NotImplementedError("每个 scraper 子类必须实现 scrape 方法。")

==================== End of: src\scrapers\base_scraper.py ====================



==================== Start of: src\scrapers\cvf_scraper.py ====================

# FILE: src/scrapers/cvf_scraper.py (Concurrent Version)

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get


class CvfScraper(BaseScraper):
    """
    专门用于 CVF (CVPR, ICCV) 网站的爬虫。
    此版本经过优化，使用多线程并发获取论文详情，以大幅提高速度。
    """

    def _scrape_details_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        抓取并解析单个 CVF 论文详情页。这是将被并发执行的核心工作函数。
        """
        response = robust_get(url, timeout=20)
        if not response:
            self.logger.debug(f"    -> 请求详情页失败 (已重试): {url}")
            return None

        try:
            soup = BeautifulSoup(response.content, 'lxml')

            title_tag = soup.select_one("#papertitle")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            author_tags = soup.select("#authors > b > i")
            authors = ", ".join([a.get_text(strip=True) for a in author_tags]) if author_tags else "N/A"

            abstract_tag = soup.select_one("#abstract")
            abstract = abstract_tag.get_text(strip=True) if abstract_tag else "N/A"

            pdf_url_tag = soup.select_one('meta[name="citation_pdf_url"]')
            pdf_url = pdf_url_tag['content'] if pdf_url_tag else None
            if pdf_url and not pdf_url.startswith('http'):
                pdf_url = urljoin(url, pdf_url)

            paper_id = url.strip('/').split('/')[-1].replace('.html', '')

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': url}
        except Exception as e:
            self.logger.debug(f"    -> 解析 CVF 详情页失败 {url}: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]
        max_workers = self.task_info.get("max_workers", 1)
        max_papers_limit = self.task_info.get("max_papers_limit", 0)

        self.logger.info(f"    -> 正在抓取 CVF 索引页: {index_url}")
        response = robust_get(index_url)
        if not response:
            return []

        if response.status_code == 404:
            self.logger.warning(f"    -> 页面未找到 (404): {index_url}")
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            link_tags = soup.select('dt.ptitle > a[href$=".html"]')

            detail_urls = [urljoin(index_url, tag['href']) for tag in link_tags]
            self.logger.info(f"    -> 索引页解析完成，共找到 {len(detail_urls)} 篇论文。")

            urls_to_crawl = detail_urls
            if max_papers_limit > 0:
                actual_limit = min(max_papers_limit, len(detail_urls))
                urls_to_crawl = detail_urls[:actual_limit]
                self.logger.info(f"    -> 已应用数量限制，将爬取前 {len(urls_to_crawl)} 篇论文。")

            if not urls_to_crawl:
                return []

            papers = []
            pbar_desc = f"    -> 并发解析 {self.task_info.get('conference')} 详情页"

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self._scrape_details_page, url): url for url in urls_to_crawl}

                for future in tqdm(as_completed(future_to_url), total=len(urls_to_crawl), desc=pbar_desc, leave=True):
                    result = future.result()
                    if result:
                        papers.append(result)

            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] 解析 CVF 页面时发生未知错误: {e}", exc_info=True)
            return []

==================== End of: src\scrapers\cvf_scraper.py ====================



==================== Start of: src\scrapers\iclr_scraper.py ====================

# FILE: src/scrapers/iclr_scraper.py

import openreview
import openreview.api
import re
import numpy as np
from tqdm import tqdm
from itertools import islice
import time
from typing import List, Dict, Any

from .base_scraper import BaseScraper

class IclrScraper(BaseScraper):
    """专门用于 ICLR (OpenReview) 的爬虫。"""

    def _get_v1_notes_with_retry(self, client, venue_id, limit, max_retries=3):
        """为 openreview v1 的 get_all_notes 添加简单的重试逻辑。"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"    -> [V1 API] 正在尝试获取笔记 (第 {attempt + 1}/{max_retries} 次)...")
                notes_iterator = client.get_all_notes(content={'venueid': venue_id})
                notes_list = list(islice(notes_iterator, limit)) if limit else list(notes_iterator)
                return notes_list
            except Exception as e:
                self.logger.warning(f"    -> [V1 API] 第 {attempt + 1} 次尝试失败: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # 等待时间逐渐增加
                else:
                    self.logger.error(f"    -> [V1 API] 达到最大重试次数，获取失败。")
                    raise e # 抛出最终的异常

    def scrape(self) -> List[Dict[str, Any]]:
        api_version = self.task_info.get("api_version", "v2")
        venue_id = self.task_info["venue_id"]
        limit = self.task_info.get("limit")
        fetch_reviews = self.task_info.get("fetch_reviews", False)

        self.logger.info(f"    -> 使用 OpenReview API v{api_version} for venue: {venue_id}")
        if fetch_reviews:
            self.logger.info("    -> 已启用审稿信息获取。由于API速率限制，速度会变慢。")

        try:
            notes_list = []
            if api_version == "v1":
                client = openreview.Client(baseurl='https://api.openreview.net')
                # <-- 使用带重试的函数
                notes_list = self._get_v1_notes_with_retry(client, venue_id, limit)
            else:  # API v2
                client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')
                # V2 API 通常更稳定，但也可以为其添加重试
                notes_list = client.get_notes(content={'venueid': venue_id}, limit=limit) if limit else list(
                    client.get_all_notes(content={'venueid': venue_id}))

            if not notes_list:
                return []

            self.logger.info(f"    -> 找到了 {len(notes_list)} 份提交进行处理。")
            papers = []
            client_v2_for_reviews = openreview.api.OpenReviewClient(
                baseurl='https://api2.openreview.net') if fetch_reviews else None

            pbar_desc = f"    -> 正在解析 ICLR 论文"
            for note in tqdm(notes_list, desc=pbar_desc, leave=True):
                paper_details = self._parse_note(note)
                if fetch_reviews and client_v2_for_reviews:
                    time.sleep(0.3)
                    review_details = self._fetch_review_details(client_v2_for_reviews, note.id)
                    paper_details.update(review_details)
                papers.append(paper_details)
            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] ICLR OpenReview 抓取失败: {e}", exc_info=True)
            return []

    def _parse_note(self, note: Any) -> Dict[str, Any]:
        """解析单个 OpenReview note 对象。"""
        content = note.content
        def get_field_robust(field_name, default_value):
            field_data = content.get(field_name)
            if isinstance(field_data, dict):
                return field_data.get('value', default_value)
            return field_data if field_data is not None else default_value
        return {'id': note.id, 'title': get_field_robust('title', 'N/A'), 'authors': ', '.join(get_field_robust('authors', [])), 'abstract': get_field_robust('abstract', 'N/A'), 'pdf_url': f"https://openreview.net/pdf?id={note.id}", 'source_url': f"https://openreview.net/forum?id={note.id}"}

    def _fetch_review_details(self, client: openreview.api.OpenReviewClient, forum_id: str) -> Dict[str, Any]:
        """获取单个论文的审稿信息。"""
        ratings, decision = [], 'N/A'
        try:
            related_notes = client.get_notes(forum=forum_id)
            for note in related_notes:
                if any(re.search(r'/Decision', inv, re.IGNORECASE) for inv in note.invitations):
                    decision_value = note.content.get('decision', {}).get('value')
                    if decision_value: decision = str(decision_value)
                if any(re.search(r'/Review|/Official_Review', inv, re.IGNORECASE) for inv in note.invitations):
                    rating_val = note.content.get('rating', {}).get('value')
                    if isinstance(rating_val, str):
                        match = re.search(r'^\d+', rating_val)
                        if match: ratings.append(int(match.group(0)))
                    elif isinstance(rating_val, (int, float)):
                        ratings.append(int(rating_val))
        except Exception as e:
            self.logger.debug(f"获取审稿信息失败 forum_id={forum_id}: {e}")
        return {'decision': decision, 'avg_rating': round(np.mean(ratings), 2) if ratings else None, 'review_ratings': ratings}

==================== End of: src\scrapers\iclr_scraper.py ====================



==================== Start of: src\scrapers\icml_scraper.py ====================

# FILE: src/scrapers/icml_scraper.py

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from bs4.element import Tag

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get  # <-- 导入新的工具函数


class IcmlScraper(BaseScraper):
    """专门用于 ICML (PMLR) 网站的爬虫。"""

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]
        limit = self.task_info.get("limit")
        papers = []

        self.logger.info(f"    -> 正在抓取 ICML 索引页: {index_url}")

        response = robust_get(index_url, timeout=45)  # <-- 使用 robust_get 并增加超时
        if not response:
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            paper_containers = soup.select('div.paper')
            self.logger.info(f"    -> 找到了 {len(paper_containers)} 篇论文。")

            if limit:
                paper_containers = paper_containers[:limit]
                self.logger.info(f"    -> 应用限制：处理前 {limit} 篇论文。")

            pbar_desc = f"    -> 正在解析 {self.task_info.get('conference')} 页面"
            for paper_div in tqdm(paper_containers, desc=pbar_desc, leave=True):
                paper_data = self._parse_paper_div(paper_div, index_url)
                if paper_data:
                    papers.append(paper_data)

            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] 解析 ICML 页面时发生未知错误: {e}", exc_info=True)
            return []

    def _parse_paper_div(self, paper_div: Tag, base_url: str) -> Optional[Dict[str, Any]]:
        """从单个 <div class="paper"> 中解析出所有信息。"""
        try:
            title_tag = paper_div.select_one('p.title')
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            authors_tag = paper_div.select_one('p.details span.authors')
            authors = authors_tag.get_text(strip=True).replace(';', ', ') if authors_tag else "N/A"

            links_p = paper_div.select_one('p.links')
            if not links_p:
                return None

            source_url_tag = links_p.select_one('a:-soup-contains("abs")')
            source_url = urljoin(base_url, source_url_tag['href']) if source_url_tag else 'N/A'

            pdf_url_tag = links_p.select_one('a:-soup-contains("Download PDF")')
            pdf_url = urljoin(base_url, pdf_url_tag['href']) if pdf_url_tag else 'N/A'

            paper_id = source_url.split('/')[-1].replace('.html', '') if source_url != 'N/A' else title
            abstract = "N/A (摘要需访问详情页)"

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': source_url}
        except Exception as e:
            self.logger.debug(f"    -> 从 ICML 容器解析失败: {e}")
            return None

==================== End of: src\scrapers\icml_scraper.py ====================



==================== Start of: src\scrapers\kdd_scraper.py ====================

# FILE: src/scrapers/kdd_scraper.py

import time
from typing import List, Dict, Any

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from .base_scraper import BaseScraper


class KddScraper(BaseScraper):
    """专门用于 KDD 网站的爬虫 (使用 Selenium)。"""

    def scrape(self) -> List[Dict[str, Any]]:
        url = self.task_info["url"]
        limit = self.task_info.get("limit")

        # KDD 特定的选择器
        paper_link_selector = 'a.item-title'

        self.logger.info(f"    -> 正在启动 Selenium 访问 (KDD): {url}")
        driver = None
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument(
                'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            driver.get(url)
            self.logger.info("    -> 页面已加载. 等待 10 秒以处理动态内容...")
            time.sleep(10)

            link_elements = driver.find_elements(By.CSS_SELECTOR, paper_link_selector)
            if not link_elements:
                self.logger.warning(f"    -> Selenium 未找到任何论文链接，使用的选择器是: '{paper_link_selector}'")
                return []

            self.logger.info(f"    -> 找到了 {len(link_elements)} 个潜在的论文链接。")
            if limit and len(link_elements) > limit:
                self.logger.info(f"    -> 应用限制：处理前 {limit} 个链接。")
                link_elements = link_elements[:limit]

            papers = []
            for i, link_elem in enumerate(link_elements):
                paper_url = link_elem.get_attribute('href')
                paper_title = link_elem.text
                if paper_url and paper_title:
                    papers.append({
                        'id': f"kdd_{self.task_info['year']}_{i}",
                        'title': paper_title.strip(),
                        'authors': 'N/A (KDD Selenium)',
                        'abstract': 'N/A (KDD Selenium)',
                        'pdf_url': None,
                        'source_url': paper_url
                    })
            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] KDD Selenium 抓取失败 {url}: {e}", exc_info=True)
            return []
        finally:
            if driver:
                driver.quit()

==================== End of: src\scrapers\kdd_scraper.py ====================



==================== Start of: src\scrapers\neurips_scraper.py ====================

# FILE: src/scrapers/neurips_scraper.py

import openreview
import openreview.api
import re
import numpy as np
from tqdm import tqdm
from itertools import islice
import time
from typing import List, Dict, Any

from .base_scraper import BaseScraper


class NeuripsScraper(BaseScraper):
    """专门用于 NeurIPS (OpenReview) 的爬虫。"""

    def scrape(self) -> List[Dict[str, Any]]:
        api_version = self.task_info.get("api_version", "v2")
        venue_id = self.task_info["venue_id"]
        limit = self.task_info.get("limit")
        fetch_reviews = self.task_info.get("fetch_reviews", False)

        self.logger.info(f"    -> 使用 OpenReview API v{api_version} for venue: {venue_id}")
        if fetch_reviews:
            self.logger.info("    -> 已启用审稿信息获取。由于API速率限制，速度会变慢。")

        try:
            notes_list = []
            if api_version == "v1":
                client = openreview.Client(baseurl='https://api.openreview.net')
                notes_iterator = client.get_all_notes(content={'venueid': venue_id})
                notes_list = list(islice(notes_iterator, limit)) if limit else list(notes_iterator)
            else:  # API v2
                client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')
                notes_list = client.get_notes(content={'venueid': venue_id}, limit=limit) if limit else list(
                    client.get_all_notes(content={'venueid': venue_id}))

            if not notes_list:
                return []

            self.logger.info(f"    -> 找到了 {len(notes_list)} 份提交进行处理。")
            papers = []
            client_v2_for_reviews = openreview.api.OpenReviewClient(
                baseurl='https://api2.openreview.net') if fetch_reviews else None

            pbar_desc = f"    -> 正在解析 NeurIPS 论文"
            for note in tqdm(notes_list, desc=pbar_desc, leave=True):
                paper_details = self._parse_note(note)
                if fetch_reviews and client_v2_for_reviews:
                    time.sleep(0.3)
                    review_details = self._fetch_review_details(client_v2_for_reviews, note.id)
                    paper_details.update(review_details)
                papers.append(paper_details)
            return papers

        except Exception as e:
            self.logger.error(f"    [✖ ERROR] NeurIPS OpenReview 抓取失败: {e}", exc_info=True)
            return []

    def _parse_note(self, note: Any) -> Dict[str, Any]:
        """解析单个 OpenReview note 对象。"""
        content = note.content

        def get_field_robust(field_name, default_value):
            field_data = content.get(field_name)
            if isinstance(field_data, dict):
                return field_data.get('value', default_value)
            return field_data if field_data is not None else default_value

        return {
            'id': note.id,
            'title': get_field_robust('title', 'N/A'),
            'authors': ', '.join(get_field_robust('authors', [])),
            'abstract': get_field_robust('abstract', 'N/A'),
            'pdf_url': f"https://openreview.net/pdf?id={note.id}",
            'source_url': f"https://openreview.net/forum?id={note.id}"
        }

    def _fetch_review_details(self, client: openreview.api.OpenReviewClient, forum_id: str) -> Dict[str, Any]:
        """获取单个论文的审稿信息。"""
        ratings, decision = [], 'N/A'
        try:
            related_notes = client.get_notes(forum=forum_id)
            for note in related_notes:
                if any(re.search(r'/Decision', inv, re.IGNORECASE) for inv in note.invitations):
                    decision_value = note.content.get('decision', {}).get('value')
                    if decision_value: decision = str(decision_value)
                if any(re.search(r'/Review|/Official_Review', inv, re.IGNORECASE) for inv in note.invitations):
                    rating_val = note.content.get('rating', {}).get('value')
                    if isinstance(rating_val, str):
                        match = re.search(r'^\d+', rating_val)
                        if match: ratings.append(int(match.group(0)))
                    elif isinstance(rating_val, (int, float)):
                        ratings.append(int(rating_val))
        except Exception as e:
            self.logger.debug(f"获取审稿信息失败 forum_id={forum_id}: {e}")

        return {'decision': decision, 'avg_rating': round(np.mean(ratings), 2) if ratings else None,
                'review_ratings': ratings}

==================== End of: src\scrapers\neurips_scraper.py ====================



==================== Start of: src\scrapers\tpami_scraper.py ====================

# FILE: src/scrapers/tpami_scraper.py (API Version)

import requests
import json
from typing import List, Dict, Any
from tqdm import tqdm
import time

from .base_scraper import BaseScraper


class TpamiScraper(BaseScraper):
    """
    专门用于 IEEE TPAMI 期刊的爬虫 (使用后台 API)。
    这是一个更稳定、更高效的方案，取代了 Selenium。
    """
    BASE_URL = "https://ieeexplore.ieee.org"

    def _get_issue_number(self, punumber: str) -> str:
        """
        第一步: 调用 metadata API 获取最新的 'issueNumber'。
        这个 issueNumber 是获取论文列表的关键。
        """
        metadata_url = f"{self.BASE_URL}/rest/publication/home/metadata?pubid={punumber}"
        headers = {
            # 关键请求头，模拟从期刊主页发起的请求
            'Referer': f'{self.BASE_URL}/xpl/conhome/{punumber}/proceeding',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.logger.info(f"    -> 正在获取 issue number from: {metadata_url}")
        try:
            response = requests.get(metadata_url, headers=headers, timeout=20)
            response.raise_for_status()
            data = response.json()
            # issueNumber 可以是 'Early Access' 的 ID，也可以是最新一期的 ID
            issue_number = str(data['currentIssue']['issueNumber'])
            self.logger.info(f"    -> 成功获取 issue number: {issue_number}")
            return issue_number
        except Exception as e:
            self.logger.error(f"    [✖ ERROR] 获取 issue number 失败: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        punumber = self.task_info.get("punumber")
        if not punumber:
            self.logger.error("    [✖ ERROR] TPAMI task in YAML must have a 'punumber'. For TPAMI, it's '34'.")
            return []

        limit = self.task_info.get("limit")

        issue_number = self._get_issue_number(punumber)
        if not issue_number:
            return []

        papers = []
        page_number = 1
        total_records = 0
        total_pages = 1  # 先假设只有一页

        self.logger.info("    -> 开始逐页获取论文列表...")
        pbar = tqdm(total=total_records or limit or 25, desc=f"    -> Scraping TPAMI page {page_number}")

        while True:
            toc_url = f"{self.BASE_URL}/rest/search/pub/{punumber}/issue/{issue_number}/toc"
            payload = {
                "pageNumber": str(page_number),
                "punumber": str(punumber),
                "isnumber": str(issue_number)
            }
            headers = {
                'Referer': f'{self.BASE_URL}/xpl/conhome/{punumber}/proceeding?pageNumber={page_number}',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Content-Type': 'application/json;charset=UTF-8'
            }

            try:
                response = requests.post(toc_url, headers=headers, data=json.dumps(payload), timeout=20)
                response.raise_for_status()
                data = response.json()

                if page_number == 1:
                    total_records = data.get('totalRecords', 0)
                    total_pages = data.get('totalPages', 1)
                    pbar.total = limit if limit and limit < total_records else total_records
                    self.logger.info(f"    -> 共发现 {total_records} 篇论文，分布在 {total_pages} 页。")

                records = data.get('records', [])
                if not records:
                    self.logger.info("    -> 当前页没有更多论文，抓取结束。")
                    break

                for record in records:
                    papers.append({
                        'id': record.get('articleNumber', ''),
                        'title': record.get('highlightedTitle', 'N/A').replace('<br>', ' '),
                        'authors': ', '.join([author['name'] for author in record.get('authors', [])]),
                        'abstract': record.get('abstract', 'N/A'),
                        'pdf_url': f"请访问源页面查看PDF（可能需要订阅）",
                        'source_url': self.BASE_URL + record.get('documentLink', ''),
                        'conference': 'TPAMI'
                    })
                    pbar.update(1)
                    if limit and len(papers) >= limit:
                        break

                if (limit and len(papers) >= limit) or page_number >= total_pages:
                    break

                page_number += 1
                pbar.set_description(f"    -> Scraping TPAMI page {page_number}")
                time.sleep(1)  # 友好访问

            except Exception as e:
                self.logger.error(f"    [✖ ERROR] 在第 {page_number} 页抓取失败: {e}")
                break

        pbar.close()
        return papers

==================== End of: src\scrapers\tpami_scraper.py ====================



==================== Start of: src\scrapers\__init__.py ====================

# FILE: src/scrapers/__init__.py

# This file makes the 'scrapers' directory a Python package.

# END OF FILE: src/scrapers/__init__.py

==================== End of: src\scrapers\__init__.py ====================



==================== Start of: src\search\embedder_chroma.py ====================

# FILE: src/search/embedder_multiprocess.py (Final Version with LIMIT and Incremental Update)

import sqlite3
import chromadb
from sentence_transformers import SentenceTransformer, util
from pathlib import Path
import time
import torch
import os

# --- 配置 ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"

# --- 【核心控制开关】 ---
# 设置为数字 (如 2000) 来开启“快速测试模式”，只处理指定数量的论文。
# 设置为 None 来开启“智能增量模式”，自动处理所有新论文。
PAPER_LIMIT = None


# ------------------------

def embed_and_store_parallel():
    if not DB_PATH.exists():
        print(f"[!] 错误: SQLite数据库文件 {DB_PATH} 不存在。请先运行 indexer.py。")
        return

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"[*] 1. 初始化模型 '{MODEL_NAME}' (设备: {device})...")
    model = SentenceTransformer(MODEL_NAME, device=device)
    print("[✔] 模型加载成功。")

    print(f"[*] 2. 连接并设置ChromaDB (路径: {CHROMA_DB_PATH})...")
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    collection = client.get_or_create_collection(name=COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    print(f"[✔] ChromaDB集合 '{COLLECTION_NAME}' 准备就绪。")

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    papers_to_process = []

    # --- 【核心逻辑：模式选择】 ---
    if PAPER_LIMIT:
        # 模式一：快速测试模式
        print(f"[*] [快速测试模式] 已启用，将强制处理数据库中的前 {PAPER_LIMIT} 篇论文。")
        cursor.execute(f"SELECT rowid, title, abstract, conference, year, source_file FROM papers_fts LIMIT ?",
                       (PAPER_LIMIT,))
        papers_to_process = cursor.fetchall()

    else:
        # 模式二：智能增量模式
        print(f"[*] [智能增量模式] 启动，开始计算需要更新的论文...")

        # 1. 从ChromaDB获取已存在的ID
        existing_ids_in_chroma = set(collection.get(include=[])['ids'])
        print(f"    -> ChromaDB中已存在 {len(existing_ids_in_chroma)} 个向量。")

        # 2. 从SQLite获取所有ID
        cursor.execute("SELECT rowid FROM papers_fts")
        all_ids_in_sqlite = {str(row[0]) for row in cursor.fetchall()}
        print(f"    -> SQLite中总共有 {len(all_ids_in_sqlite)} 篇论文。")

        # 3. 计算差集，得到需要处理的新ID
        new_paper_ids = list(all_ids_in_sqlite - existing_ids_in_chroma)

        if not new_paper_ids:
            print("\n[✔] 数据库已是最新，无需更新。任务结束。")
            conn.close()
            return

        print(f"[✔] 发现 {len(new_paper_ids)} 篇新论文需要处理。")

        # 4. 只从SQLite中获取这些新论文的详细信息
        placeholders = ','.join('?' for _ in new_paper_ids)
        query = f"SELECT rowid, title, abstract, conference, year, source_file FROM papers_fts WHERE rowid IN ({placeholders})"
        cursor.execute(query, new_paper_ids)
        papers_to_process = cursor.fetchall()

    conn.close()

    if not papers_to_process:
        print("[!] 本次运行没有需要处理的论文。任务结束。")
        return

    print(f"[✔] 本次共需处理 {len(papers_to_process)} 篇论文。")

    worker_processes = 2  # 固定使用2个进程
    print(f"[*] 将使用 {worker_processes} 个进程进行并行处理。")

    print("[*] 4. 启动多进程池...")
    pool = model.start_multi_process_pool(target_devices=[device] * worker_processes)
    print("[✔] 进程池已启动。")

    print("[*] 5. 开始并行生成向量...")
    start_time = time.time()

    documents_to_embed = [f"{p[1]}. {p[2]}" for p in papers_to_process]
    embeddings = model.encode_multi_process(documents_to_embed, pool, batch_size=64)

    end_time_encoding = time.time()
    print(f"[✔] 向量生成完毕! 耗时: {end_time_encoding - start_time:.2f} 秒。")

    model.stop_multi_process_pool(pool)
    print("[✔] 进程池已关闭。")

    print("[*] 6. 开始将向量批量存入ChromaDB...")
    start_time_storing = time.time()

    ids = [str(p[0]) for p in papers_to_process]
    metadatas = [{"title": p[1], "conference": p[3], "year": p[4], "source_file": p[5]} for p in papers_to_process]

    # ChromaDB的add方法是幂等的，如果ID已存在，它会更新内容。对于我们的场景，用upsert更语义化。
    db_batch_size = 1024
    for i in range(0, len(papers_to_process), db_batch_size):
        collection.upsert(
            ids=ids[i:i + db_batch_size],
            embeddings=embeddings[i:i + db_batch_size].tolist(),
            metadatas=metadatas[i:i + db_batch_size],
            documents=documents_to_embed[i:i + db_batch_size]
        )

    end_time_storing = time.time()
    print(f"[✔] 数据存储/更新完毕! 耗时: {end_time_storing - start_time_storing:.2f} 秒。")

    print("\n" + "=" * 50)
    print(f"[✔] 所有任务完成！")
    print(f"    - 本次处理论文: {len(papers_to_process)} 篇")
    print(f"    - 向量数据库中的条目总数: {collection.count()}")
    print(f"    - 总耗时: {end_time_storing - start_time:.2f} 秒")
    print("=" * 50)


if __name__ == "__main__":
    try:
        import torch
    except ImportError:
        print("错误: PyTorch 未安装。请运行 'uv pip install torch'")
        exit()

    embed_and_store_parallel()

==================== End of: src\search\embedder_chroma.py ====================



==================== Start of: src\search\indexer.py ====================

# FILE: src/search/indexer.py

import sqlite3
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import time

# --- 配置 ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
METADATA_DIR = PROJECT_ROOT / "output" / "metadata"
# 新增：定义统一的数据库存放目录
DB_DIR = PROJECT_ROOT / "database"

# 修改：让 DB_PATH 指向新目录中的文件
DB_PATH = DB_DIR / "papers.db"
# 定义需要的列，与数据库表结构对应
REQUIRED_COLUMNS = ['title', 'authors', 'abstract', 'conference', 'year', 'pdf_url', 'source_file']


def create_fts_table(conn):
    """创建支持全文搜索的 FTS5 虚拟表"""
    cursor = conn.cursor()
    # 如果表已存在，先删除它，确保每次重建索引都是最新的
    cursor.execute("DROP TABLE IF EXISTS papers_fts")
    # 创建 FTS5 表。这里定义了我们想对其进行全文搜索的所有字段。
    cursor.execute("""
        CREATE VIRTUAL TABLE papers_fts USING fts5(
            title,
            authors,
            abstract,
            conference UNINDEXED,  -- UNINDEXED 表示这个字段存储但不建立全文索引(节省空间)，因为我们通常不需要全文搜它
            year UNINDEXED,
            pdf_url UNINDEXED,
            source_file UNINDEXED,
            tokenize='porter'      -- 使用 porter 分词器，支持英文词干提取(例如搜 searching 能匹配 search)
        )
    """)
    conn.commit()


def index_csv_files():
    print(f"[*] 开始构建索引...")
    print(f"    - 数据源目录: {METADATA_DIR}")
    print(f"    - 数据库路径: {DB_PATH}")

    csv_files = list(METADATA_DIR.rglob("*_data_*.csv"))
    if not csv_files:
        print("[!] 错误: 没有找到任何 CSV 文件。请先运行爬虫采集数据。")
        return

    conn = sqlite3.connect(str(DB_PATH))
    create_fts_table(conn)

    total_files = len(csv_files)
    total_papers = 0
    start_time = time.time()

    print(f"[*] 发现 {total_files} 个文件，开始处理...")

    for i, csv_path in enumerate(csv_files, 1):
        try:
            # 使用 chunksize 分块读取，核心内存优化点！
            # 每次只读 5000 行到内存，处理完就释放，绝不爆内存。
            chunk_iterator = pd.read_csv(csv_path, chunksize=5000, dtype=str)

            for chunk_df in chunk_iterator:
                if chunk_df.empty: continue

                # 数据清洗和标准化
                chunk_df = chunk_df.fillna('')
                if 'source_file' not in chunk_df.columns:
                    chunk_df['source_file'] = csv_path.name

                # 确保所有需要的列都存在
                for col in REQUIRED_COLUMNS:
                    if col not in chunk_df.columns:
                        chunk_df[col] = ''

                # 选取并排序特定的列以匹配数据库结构
                data_to_insert = chunk_df[REQUIRED_COLUMNS].values.tolist()

                # 批量插入数据
                conn.executemany(
                    "INSERT INTO papers_fts(title, authors, abstract, conference, year, pdf_url, source_file) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    data_to_insert
                )
                total_papers += len(data_to_insert)

            print(f"    [{i}/{total_files}] 已索引: {csv_path.name}")

        except Exception as e:
            print(f"    [!] 处理文件失败 {csv_path.name}: {e}")

    # 提交事务并进行优化
    print("[*] 正在提交并优化数据库 (可能需要一点时间)...")
    conn.commit()
    # Optimize 命令会重组数据库文件，使其在搜索时更快
    conn.execute("INSERT INTO papers_fts(papers_fts) VALUES('optimize')")
    conn.close()

    end_time = time.time()
    print(f"\n[✔] 索引构建完成！")
    print(f"    - 总计索引论文: {total_papers} 篇")
    print(f"    - 总耗时: {end_time - start_time:.2f} 秒")
    print(f"    - 数据库文件大小: {DB_PATH.stat().st_size / (1024 * 1024):.2f} MB")


if __name__ == "__main__":
    index_csv_files()

==================== End of: src\search\indexer.py ====================



==================== Start of: src\search\search_ai_assistant.py ====================

# FILE: src/search/search_ai_assistant.py (CLI Launcher - v1.1)

import sys
import math
import textwrap
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# --- 从 search_service 导入所有功能和配置 ---
from src.search.search_service import (
    initialize_components,
    keyword_search,
    semantic_search,
    get_stats_summary,
    save_results_to_markdown,
    _sqlite_conn,
    PROJECT_ROOT,
    SEARCH_RESULTS_DIR,
    RESULTS_PER_PAGE,
    Colors, # 导入Colors
    _initialized
)
# --- 导入CLI专属的AI对话交互函数 ---
from src.ai.glm_chat_service import start_ai_chat_session

# --- 定义CLI专属的 print_colored 函数 ---
# 确保在CLI交互中能够正确打印彩色文本
def print_colored(text, color, end='\n'):
    if sys.stdout.isatty():
        print(f"{color}{text}{Colors.ENDC}", end=end)
    else:
        print(text, end=end)

# --- CLI特有的Banner ---
def print_banner():
    banner_text = "--- PubCrawler v7.4: CLI AI Assistant (Refactored) ---"
    print_colored(banner_text, Colors.HEADER)
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    print_colored(f"[*] 结果将保存至: {SEARCH_RESULTS_DIR.resolve()}", Colors.UNDERLINE)

# --- CLI特有的结果统计打印 ---
def print_cli_stats_summary(stats_summary: Dict[str, Any]):
    if not stats_summary['total_found']: return
    print_colored("\n--- 查询结果统计 ---", Colors.HEADER)
    print(f"总计找到 {Colors.BOLD}{stats_summary['total_found']}{Colors.ENDC} 篇相关论文。")
    if stats_summary['distribution']:
        print("分布情况:")
        for conf_year, count in stats_summary['distribution'].items():
            print(f"  - {conf_year}: {count} 篇")
    print_colored("--------------------", Colors.HEADER)

# --- CLI特有的分页逻辑 ---
def interactive_pagination_cli(results: List[Dict[str, Any]], query: str, session_dir: Path):
    num_results = len(results)
    if num_results == 0:
        print_colored("[!] 未找到相关结果。", Colors.WARNING)
        return

    stats_summary = get_stats_summary(results)
    print_cli_stats_summary(stats_summary)

    total_pages = math.ceil(num_results / RESULTS_PER_PAGE)
    current_page = 1

    while True:
        start_idx, end_idx = (current_page - 1) * RESULTS_PER_PAGE, current_page * RESULTS_PER_PAGE
        page_results = results[start_idx:end_idx]

        print_colored(f"\n--- 结果预览 (第 {current_page}/{total_pages} 页) ---", Colors.HEADER)
        for i, paper in enumerate(page_results, start=start_idx + 1):
            title, authors, conf, year = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            display_line = f"  {Colors.OKCYAN}{conf} {year}{Colors.ENDC} | 作者: {textwrap.shorten(authors, 70)}"
            if 'similarity' in paper: display_line = f"  {Colors.OKGREEN}相似度: {paper['similarity']:.2f}{Colors.ENDC} |" + display_line
            print(f"\n{Colors.BOLD}[{i}]{Colors.ENDC} {title}\n{display_line}")

        if current_page >= total_pages: print("\n--- 已是最后一页 ---"); break
        try:
            choice = input(
                f"\n按 {Colors.BOLD}[Enter]{Colors.ENDC} 下一页, '{Colors.BOLD}s{Colors.ENDC}' 保存, '{Colors.BOLD}ai{Colors.ENDC}' 对结果提问, '{Colors.BOLD}q{Colors.ENDC}' 返回: ").lower()
            if choice == 'q': return
            if choice == 's': break
            if choice == 'ai':
                start_ai_chat_session(results)
                print_colored("\n[i] AI对话结束，返回结果列表。", Colors.OKBLUE)
                continue
            current_page += 1
        except KeyboardInterrupt:
            return

    if input(f"\n是否将这 {num_results} 条结果全部保存到 Markdown? (y/n, 默认y): ").lower() != 'n':
        save_results_to_markdown(results, query)

# --- 主程序 (CLI入口) ---
def main():
    # 确保在main函数开始时调用初始化，而不是在模块加载时
    initialize_components()

    if not _initialized: # 检查初始化是否成功
        print_colored(f"[{Colors.FAIL}✖{Colors.ENDC}] 严重错误: 搜索后端服务初始化失败，无法运行CLI。", Colors.FAIL)
        sys.exit(1)

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    print_banner()
    print_colored("\n--- 搜索语法 (AI已集成 & FTS5已修正) ---", Colors.OKBLUE)
    print("  - `transformer author:vaswani`   (关键词 + 作者字段搜索)")
    print("  - `title:\"vision transformer\"`     (精确标题搜索)")
    print("  - `\"large language model\" AND efficient` (短语和关键词组合)")
    print(f"  - `{Colors.BOLD}sem:{Colors.ENDC} efficiency of few-shot learning` (语义搜索！)")

    while True:
        try:
            q = input(f"\n🔍 {Colors.BOLD}请输入查询{Colors.ENDC} (或 'exit' 退出): ").strip()
            if not q: continue
            if q.lower() == 'exit': break

            results = []
            if q.lower().startswith('sem:'):
                semantic_query = q[4:].strip()
                if semantic_query: results, _ = semantic_search(semantic_query)
            else:
                results, _ = keyword_search(q)

            interactive_pagination_cli(results, q, session_dir)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"发生未知错误: {e}", Colors.FAIL)

    if _sqlite_conn:
        _sqlite_conn.close()
        print_colored("\n[✔] SQLite连接已关闭。", Colors.OKBLUE)
    print("\n再见！")


if __name__ == "__main__":
    main()

==================== End of: src\search\search_ai_assistant.py ====================



==================== Start of: src\search\search_service.py ====================

# FILE: src/search/search_service.py (Core Backend Services - v1.1)

import sqlite3
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pathlib import Path
import time
import re
import torch
from datetime import datetime
from collections import Counter
import os
from dotenv import load_dotenv
from zai import ZhipuAiClient
from typing import List, Dict, Any, Optional, Tuple

# --- 全局配置 (统一管理，其他模块通过导入这个文件来访问) ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
SEARCH_RESULTS_DIR = PROJECT_ROOT / "search_results"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"
RESULTS_PER_PAGE = 10  # 用于分页的默认值
AI_CONTEXT_PAPERS = 5  # 每次提问时，发送给AI的最相关的论文数量

# --- 加载环境变量 ---
load_dotenv(PROJECT_ROOT / '.env')
ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY")

# --- 全局可访问的后端组件实例 (使用单例模式，通过 initialize_components 函数初始化) ---
_sqlite_conn: Optional[sqlite3.Connection] = None
_sentence_transformer_model: Optional[SentenceTransformer] = None
_chroma_collection: Optional[chromadb.api.models.Collection.Collection] = None
_zhipu_ai_client: Optional[ZhipuAiClient] = None
_ai_enabled: bool = False
_initialized: bool = False  # 标记是否已初始化


# --- 颜色定义 (保留在服务层，作为通用常量) ---
class Colors:
    HEADER = '\033[95m';
    OKBLUE = '\033[94m';
    OKCYAN = '\033[96m';
    OKGREEN = '\033[92m'
    WARNING = '\033[93m';
    FAIL = '\033[91m';
    ENDC = '\033[0m';
    BOLD = '\033[1m';
    UNDERLINE = '\033[4m'


# --- 初始化函数 (所有后端组件的单一入口，确保只初始化一次) ---
def initialize_components() -> None:
    """
    初始化所有搜索和AI后端组件。确保只运行一次。
    此函数会打印状态信息，但颜色和输出方式由调用者决定。
    """
    global _sqlite_conn, _sentence_transformer_model, _chroma_collection, _zhipu_ai_client, _ai_enabled, _initialized

    if _initialized:
        # print("搜索后端服务已初始化，跳过重复初始化。") # 调试用
        return

    print(f"[{Colors.OKBLUE}*{Colors.ENDC}] 正在初始化搜索后端服务...")

    # 1. 初始化SQLite连接
    try:
        _sqlite_conn = sqlite3.connect(str(DB_PATH), uri=True, check_same_thread=False)
        print(f"[{Colors.OKGREEN}✔{Colors.ENDC}] SQLite数据库 '{DB_PATH.name}' 连接成功。")
    except Exception as e:
        print(f"[{Colors.FAIL}✖{Colors.ENDC}] 错误: 无法连接SQLite数据库: {e}")
        _sqlite_conn = None
        _initialized = True
        return

        # 2. 初始化SentenceTransformer模型和ChromaDB
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        _sentence_transformer_model = SentenceTransformer(MODEL_NAME, device=device)
        print(f"[{Colors.OKGREEN}✔{Colors.ENDC}] SentenceTransformer模型 '{MODEL_NAME}' ({device}) 加载成功。")

        _chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH, settings=Settings(anonymized_telemetry=False))
        _chroma_collection = _chroma_client.get_or_create_collection(name=COLLECTION_NAME,
                                                                     metadata={"hnsw:space": "cosine"})
        print(
            f"[{Colors.OKGREEN}✔{Colors.ENDC}] ChromaDB集合 '{COLLECTION_NAME}' ({_chroma_collection.count()} 个向量) 已加载。")
    except Exception as e:
        print(f"[{Colors.FAIL}✖{Colors.ENDC}] 错误: 无法初始化语义搜索组件: {e}")
        _sentence_transformer_model = None
        _chroma_collection = None
        _initialized = True
        if _sqlite_conn: _sqlite_conn.close()
        return

    # 3. 初始化智谱AI客户端
    if ZHIPUAI_API_KEY:
        try:
            _zhipu_ai_client = ZhipuAiClient(api_key=ZHIPUAI_API_KEY)
            _ai_enabled = True
            print(f"[{Colors.OKGREEN}✔{Colors.ENDC}] 智谱AI客户端初始化成功。")
        except Exception as e:
            print(f"[{Colors.WARNING}⚠{Colors.ENDC}] 警告: 无法初始化智谱AI客户端: {e}. AI对话功能将不可用。")
            _zhipu_ai_client = None
            _ai_enabled = False
    else:
        print(f"[{Colors.WARNING}⚠{Colors.ENDC}] 警告: 未设置 ZHIPUAI_API_KEY. AI对话功能将不可用。")
        _ai_enabled = False

    _initialized = True
    print(f"[{Colors.OKBLUE}*{Colors.ENDC}] 搜索后端服务初始化完成。")


# --- 核心搜索功能 ---

def keyword_search(raw_query: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    执行关键词搜索，返回结果列表和统计摘要。
    """
    if not _initialized or _sqlite_conn is None:
        return [], {"error": "搜索服务未初始化或SQLite连接失败。"}

    COLUMN_MAP = {'author': 'authors', 'title': 'title', 'abstract': 'abstract'}
    parsed_query_parts = []
    pattern = re.compile(r'(\b\w+):(?:"([^"]*)"|(\S+))')
    remaining_query = raw_query

    for match in list(pattern.finditer(raw_query)):
        full_match_text = match.group(0)
        field_alias = match.group(1).lower()
        value = match.group(2) if match.group(2) is not None else match.group(3)
        if field_alias in COLUMN_MAP:
            db_column = COLUMN_MAP[field_alias]
            safe_value = value.replace('"', '""')
            if ' ' in value or not value.isalnum():
                parsed_query_parts.append(f'{db_column}:"{safe_value}"')
            else:
                parsed_query_parts.append(f'{db_column}:{safe_value}')
            remaining_query = re.sub(re.escape(full_match_text), '', remaining_query, 1)

    general_terms = re.findall(r'"[^"]*"|\S+', remaining_query.strip())
    for term in general_terms:
        safe_term = term.replace('"', '""')
        if term.startswith('"') and term.endswith('"'):
            parsed_query_parts.append(f'{safe_term}')
        else:
            parsed_query_parts.append(safe_term)

    final_fts_query = ' AND '.join(filter(None, parsed_query_parts))

    if not final_fts_query:
        return [], {"total_found": 0, "distribution": {}, "message": "关键词搜索查询为空或解析失败。"}

    try:
        cursor = _sqlite_conn.execute(
            "SELECT title, authors, abstract, conference, year FROM papers_fts WHERE papers_fts MATCH ? ORDER BY rank",
            (final_fts_query,)
        )
        raw_results = cursor.fetchall()
        results = [{"title": r[0], "authors": r[1], "abstract": r[2], "conference": r[3], "year": r[4]} for r in
                   raw_results]

        stats = get_stats_summary(results)
        stats['message'] = f"关键词搜索完成，找到 {len(results)} 篇。"
        return results, stats
    except sqlite3.OperationalError as e:
        return [], {"total_found": 0, "distribution": {},
                    "message": f"关键词搜索失败: {e}. FTS5 Query: '{final_fts_query}'"}


def semantic_search(query: str, top_n: int = 20) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    执行语义搜索，返回结果列表和统计摘要。
    """
    if not _initialized or _sentence_transformer_model is None or _chroma_collection is None or _sqlite_conn is None:
        return [], {"error": "搜索服务未初始化或组件失败。"}

    start_t = time.time()
    query_embedding = _sentence_transformer_model.encode(query, convert_to_tensor=False)
    chroma_results = _chroma_collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_n)

    ids_found, distances = chroma_results['ids'][0], chroma_results['distances'][0]
    if not ids_found: return [], get_stats_summary([])

    placeholders = ','.join('?' for _ in ids_found)
    sql_query = f"SELECT rowid, title, authors, abstract, conference, year FROM papers_fts WHERE rowid IN ({placeholders})"
    cursor = _sqlite_conn.cursor()
    raw_sqlite_results = {str(r[0]): r[1:] for r in cursor.execute(sql_query, ids_found).fetchall()}

    final_results = []
    for i, paper_id in enumerate(ids_found):
        details = raw_sqlite_results.get(paper_id)
        if details:
            final_results.append({
                "title": details[0],
                "authors": details[1],
                "abstract": details[2],
                "conference": details[3],
                "year": details[4],
                "similarity": 1 - distances[i]
            })
    end_t = time.time()

    stats = get_stats_summary(final_results)
    stats['message'] = f"语义搜索完成 (耗时: {end_t - start_t:.4f} 秒, 找到 {len(final_results)} 篇)。"
    return final_results, stats


# --- 辅助功能 (与CLI和Web UI共享) ---

def get_stats_summary(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    生成查询结果的统计摘要。
    """
    total_found = len(results)

    conf_year_counter = Counter([(p.get('conference', 'N/A'), p.get('year', 'N/A')) for p in results])
    distribution = {f"{conf} {year}": count for (conf, year), count in conf_year_counter.most_common()}

    return {"total_found": total_found, "distribution": distribution}


def format_papers_for_prompt(papers: List[Dict[str, Any]]) -> str:
    """将论文列表格式化为清晰的字符串，作为AI的上下文。"""
    context = ""
    for i, paper in enumerate(papers, 1):
        context += f"[论文 {i}]\n"
        context += f"标题: {paper.get('title', 'N/A')}\n"
        context += f"作者: {paper.get('authors', 'N/A')}\n"
        context += f"摘要: {paper.get('abstract', 'N/A')}\n\n"
    return context


def save_results_to_markdown(results: List[Dict[str, Any]], query: str) -> str:
    """将搜索结果保存为Markdown文件。"""
    if not results: return "没有搜索结果可保存。"

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# 搜索查询: \"{query}\"\n\n**共找到 {len(results)} 条相关结果**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title, authors, abstract = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('abstract',
                                                                                                         'N/A')
            conf, year = paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            similarity_str = f"- **语义相似度**: {paper['similarity']:.2f}\n" if 'similarity' in paper else ""
            f.write(
                f"### {idx}. {title}\n\n- **作者**: {authors}\n- **会议/年份**: {conf} {year}\n{similarity_str}\n**摘要:**\n> {abstract}\n\n---\n\n")

    return f"结果已保存到: {filename.resolve()}"


# --- AI响应生成器 (服务模块的核心逻辑) ---
def generate_ai_response(chat_history: List[Dict[str, str]], search_results_context: List[Dict[str, Any]]) -> str:
    """
    根据搜索结果上下文和聊天历史生成AI响应。
    chat_history: 仅包含用户消息和AI响应，不包含系统消息和初始背景。
    search_results_context: 原始的论文结果列表。
    """
    global _ai_enabled, _zhipu_ai_client  # 确保可以访问全局变量

    if not _ai_enabled or _zhipu_ai_client is None:
        return "[!] 错误: AI对话功能未启用或智谱AI客户端初始化失败，请检查您的ZHIPUAI_API_KEY。"
    if not search_results_context:
        return "[!] 没有可供AI对话的搜索结果上下文。"

    context_papers = search_results_context[:AI_CONTEXT_PAPERS]
    formatted_context = format_papers_for_prompt(context_papers)

    full_messages = [
        {"role": "system",
         "content": "你是一个专业的AI学术研究助手。请根据下面提供的论文摘要信息，精准、深入地回答用户的问题。你的回答必须严格基于提供的材料，不要编造信息。"},
        {"role": "user", "content": f"这是我为你提供的背景知识，请仔细阅读：\n\n{formatted_context}"},
        {"role": "assistant", "content": "好的，我已经理解了这几篇论文的核心内容。请问您想了解什么？"}
    ]
    full_messages.extend(chat_history)

    try:
        response_generator = _zhipu_ai_client.chat.completions.create(
            model="glm-4.5-flash",
            messages=full_messages,
            stream=True,
            temperature=0.7,
        )

        full_response_content = ""
        for chunk in response_generator:
            delta_content = chunk.choices[0].delta.content
            if delta_content:
                full_response_content += delta_content
        return full_response_content
    except Exception as e:
        return f"[!] 调用AI时出错: {e}"


# 模块加载时自动初始化组件 (确保在任何函数被调用前完成)
initialize_components()

==================== End of: src\search\search_service.py ====================



==================== Start of: src\search\search_ui.py ====================

# FILE: src/search/search_ai_assistant.py (v7.3 - AI Module Separated)

import sqlite3
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pathlib import Path
import textwrap
import time
import sys
import re
import torch
import math
from datetime import datetime
from collections import Counter
import os
from dotenv import load_dotenv

# --- 【核心修改】: 导入新的AI服务模块 ---
from src.ai.glm_chat_service import start_ai_chat_session, print_colored, Colors

# ------------------------------------

# --- 配置 (大部分不变，但不再需要AI_CONTEXT_PAPERS和ZHIPUAI_API_KEY的全局导入) ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
SEARCH_RESULTS_DIR = PROJECT_ROOT / "search_results"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"
RESULTS_PER_PAGE = 10


# --- 【删除】: 不再需要在此文件中加载ZHIPUAI_API_KEY ---
# ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY")
# --------------------------------------------------------------------------

# --- 颜色和打印函数 (这里只需保留print_banner，print_colored和Colors从ai_chat_service导入) ---
# class Colors: ... (这部分可以直接删除，因为它现在是从ai_chat_service导入的)
# def print_colored(text, color): ... (这部分可以直接删除)

def print_banner():
    banner_text = "--- PubCrawler v7.3: AI Research Assistant (Module Refactored) ---"
    print_colored(banner_text, Colors.HEADER)
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    print_colored(f"[*] 结果将保存至: {SEARCH_RESULTS_DIR.resolve()}", Colors.UNDERLINE)


# --- 文件保存、统计、分页模块 (无变化，但注意print_colored现在是导入的) ---
def save_results_to_markdown(results, query, session_dir):
    if not results: return []
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# 搜索查询: \"{query}\"\n\n**共找到 {len(results)} 条相关结果**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title, authors, abstract = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('abstract',
                                                                                                         'N/A')
            conf, year = paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            similarity_str = f"- **语义相似度**: {paper['similarity']:.2f}\n" if 'similarity' in paper else ""
            f.write(
                f"### {idx}. {title}\n\n- **作者**: {authors}\n- **会议/年份**: {conf} {year}\n{similarity_str}\n**摘要:**\n> {abstract}\n\n---\n\n")
    print_colored(f"\n[✔] 结果已成功保存到 Markdown 文件!", Colors.OKGREEN)
    print_colored(f"      -> {filename.resolve()}", Colors.UNDERLINE)


def print_stats_summary(results):
    if not results: return
    total_found = len(results)
    print_colored("\n--- 查询结果统计 ---", Colors.HEADER)
    print(f"总计找到 {Colors.BOLD}{total_found}{Colors.ENDC} 篇相关论文。")
    conf_year_counter = Counter([(p.get('conference', 'N/A'), p.get('year', 'N/A')) for p in results])
    if conf_year_counter:
        print("分布情况:")
        for (conf, year), count in conf_year_counter.most_common():
            print(f"  - {conf} {year}: {count} 篇")
    print_colored("--------------------", Colors.HEADER)


def interactive_pagination(results, query, session_dir):
    num_results = len(results)
    if num_results == 0: return
    print_stats_summary(results)
    total_pages, current_page = math.ceil(num_results / RESULTS_PER_PAGE), 1
    while True:
        start_idx, end_idx = (current_page - 1) * RESULTS_PER_PAGE, current_page * RESULTS_PER_PAGE
        page_results = results[start_idx:end_idx]
        print_colored(f"\n--- 结果预览 (第 {current_page}/{total_pages} 页) ---", Colors.HEADER)
        for i, paper in enumerate(page_results, start=start_idx + 1):
            title, authors, conf, year = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('conference',
                                                                                                           'N/A'), paper.get(
                'year', 'N/A')
            display_line = f"  {Colors.OKCYAN}{conf} {year}{Colors.ENDC} | 作者: {textwrap.shorten(authors, 70)}"
            if 'similarity' in paper: display_line = f"  {Colors.OKGREEN}相似度: {paper['similarity']:.2f}{Colors.ENDC} |" + display_line
            print(f"\n{Colors.BOLD}[{i}]{Colors.ENDC} {title}\n{display_line}")
        if current_page >= total_pages: print("\n--- 已是最后一页 ---"); break
        try:
            choice = input(
                f"\n按 {Colors.BOLD}[Enter]{Colors.ENDC} 下一页, '{Colors.BOLD}s{Colors.ENDC}' 保存, '{Colors.BOLD}ai{Colors.ENDC}' 对结果提问, '{Colors.BOLD}q{Colors.ENDC}' 返回: ").lower()
            if choice == 'q': return
            if choice == 's': break
            if choice == 'ai':
                start_ai_chat_session(results)  # <-- 调用新的AI服务函数
                print_colored("\n[i] AI对话结束，返回结果列表。", Colors.OKBLUE)
                continue
            current_page += 1
        except KeyboardInterrupt:
            return
    if input(f"\n是否将这 {num_results} 条结果全部保存到 Markdown? (y/n, 默认y): ").lower() != 'n':
        save_results_to_markdown(results, query, session_dir)


# --- 搜索核心 (无变化) ---
def keyword_search(conn, raw_query):
    COLUMN_MAP = {'author': 'authors', 'title': 'title', 'abstract': 'abstract'}
    parsed_query_parts = []
    pattern = re.compile(r'(\b\w+):(?:"([^"]*)"|(\S+))')
    remaining_query = raw_query
    for match in list(pattern.finditer(raw_query)):
        full_match_text = match.group(0)
        field_alias = match.group(1).lower()
        value = match.group(2) if match.group(2) is not None else match.group(3)
        if field_alias in COLUMN_MAP:
            db_column = COLUMN_MAP[field_alias]
            safe_value = value.replace('"', '""')
            if ' ' in value or not value.isalnum():
                parsed_query_parts.append(f'{db_column}:"{safe_value}"')
            else:
                parsed_query_parts.append(f'{db_column}:{safe_value}')
            remaining_query = re.sub(re.escape(full_match_text), '', remaining_query, 1)
    general_terms = re.findall(r'"[^"]*"|\S+', remaining_query.strip())
    for term in general_terms:
        safe_term = term.replace('"', '""')
        if term.startswith('"') and term.endswith('"'):
            parsed_query_parts.append(f'{safe_term}')
        else:
            parsed_query_parts.append(safe_term)
    final_fts_query = ' AND '.join(filter(None, parsed_query_parts))
    if not final_fts_query:
        print_colored("[!] 关键词搜索查询为空或解析失败，无法执行。", Colors.WARNING);
        return []
    print(f"[*] 正在执行关键词搜索 (FTS5 Query: '{final_fts_query}')...")
    try:
        cursor = conn.execute(
            "SELECT title, authors, abstract, conference, year FROM papers_fts WHERE papers_fts MATCH ? ORDER BY rank",
            (final_fts_query,))
        results = [{"title": r[0], "authors": r[1], "abstract": r[2], "conference": r[3], "year": r[4]} for r in
                   cursor.fetchall()]
        return results
    except sqlite3.OperationalError as e:
        print_colored(f"[!] 关键词搜索失败: {e}", Colors.FAIL)
        print_colored(f"    原始查询: '{raw_query}'", Colors.WARNING)
        print_colored(f"    生成的FTS5查询: '{final_fts_query}'", Colors.WARNING)
        print_colored("    提示: FTS5查询语法严格，请检查 'AND/OR/NOT', 短语引号或字段名是否有误。", Colors.WARNING);
        return []


def semantic_search(conn, collection, model, query, top_n=20):
    print(f"[*] 正在执行语义搜索: '{query}'...")
    start_t = time.time()
    query_embedding = model.encode(query, convert_to_tensor=False)
    chroma_results = collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_n)
    ids_found, distances = chroma_results['ids'][0], chroma_results['distances'][0]
    if not ids_found: return []
    placeholders = ','.join('?' for _ in ids_found)
    sql_query = f"SELECT rowid, title, authors, abstract, conference, year FROM papers_fts WHERE rowid IN ({placeholders})"
    cursor = conn.cursor()
    raw_sqlite_results = {str(r[0]): r[1:] for r in cursor.execute(sql_query, ids_found).fetchall()}
    final_results = []
    for i, paper_id in enumerate(ids_found):
        details = raw_sqlite_results.get(paper_id)
        if details:
            final_results.append(
                {"title": details[0], "authors": details[1], "abstract": details[2], "conference": details[3],
                 "year": details[4], "similarity": 1 - distances[i]})
    end_t = time.time()
    print(f"[✔] 耗时 {end_t - start_t:.4f} 秒，找到并组合了 {len(final_results)} 个结果。")
    return final_results


# --- 主程序 (需要进行一些调整，因为print_colored和Colors现在从外部导入) ---
def main():
    # 之前这里是检查ZHIPUAI_API_KEY，现在由ai_chat_service.py内部检查

    try:
        conn = sqlite3.connect(f"file:{DB_PATH}?mode=ro", uri=True)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = SentenceTransformer(MODEL_NAME, device=device)
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH, settings=Settings(anonymized_telemetry=False))
        collection = client.get_collection(name=COLLECTION_NAME)
    except Exception as e:
        print_colored(f"\n[!] 无法初始化模块: {e}", Colors.FAIL); return

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    print_banner()
    print_colored("\n--- 搜索语法 (AI已集成 & FTS5已修正) ---", Colors.OKBLUE)
    print("  - `transformer author:vaswani`   (关键词 + 作者字段搜索)")
    print("  - `title:\"vision transformer\"`     (精确标题搜索)")
    print("  - `\"large language model\" AND efficient` (短语和关键词组合)")
    print(f"  - `{Colors.BOLD}sem:{Colors.ENDC} efficiency of few-shot learning` (语义搜索！)")

    # last_results 不再需要在此处存储，因为 ai_chat_session 直接接收结果

    while True:
        try:
            q = input(f"\n🔍 {Colors.BOLD}请输入查询{Colors.ENDC} (或 'exit' 退出): ").strip()
            if not q: continue
            if q.lower() == 'exit': break

            results = []
            if q.lower().startswith('sem:'):
                semantic_query = q[4:].strip()
                if semantic_query: results = semantic_search(conn, collection, model, semantic_query)
            else:
                results = keyword_search(conn, q)

            # last_results = results # 不再需要赋值给 last_results
            interactive_pagination(results, q, session_dir)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"发生未知错误: {e}", Colors.FAIL)

    conn.close()
    print("\n再见！")


if __name__ == "__main__":
    main()

==================== End of: src\search\search_ui.py ====================



==================== Start of: src\search\__init__.py ====================

# FILE: src/search/__init__.py
# Makes 'search' a Python package.

==================== End of: src\search\__init__.py ====================



==================== Start of: src\test\logo.py ====================

import time

# --- Logo 定义 ---
# 我们在这里定义5个版本的Logo
# 使用 \033[xxm ANSI 转义序列来设置颜色
# \033[0m 是重置所有颜色和样式
# \033[1m 是加粗
# \033[9_m 是亮色 (91=亮红, 92=亮绿, 93=亮黄, 94=亮蓝, 95=亮洋红, 96=亮青色, 97=亮白)

v1_name = "V1: 科技-数据流 (亮绿色)"
v1_art = """
\033[92m
>> > P U B C R A W L E R >
[#]=======================[#]
>> > 1011010100101011010 >
\033[0m
"""

v2_name = "V2: 爬虫-蜘蛛网 (亮白/亮红)"
v2_art = """
\033[97m
      /  \\
     /    \\
\033[91m \ \ \033[97m( PubCrawler )\033[91m / /
\033[97m  \ \ \  / / /  / /
   \ \/ /  \ \/ /
    \/ /    \/ /
\033[0m
"""

v3_name = "V3: 现代-结构块 (亮洋红/亮黄)"
v3_art = """
\033[95m
[P] [U] [B]
 \   |   /
\033[93m  [C]-[R]-[A]
\033[95m   /  |    \\
\033[93m [W]-[L]-[E]-[R]
\033[0m
"""

v4_name = "V4: 简洁-电路板 (亮蓝色/亮白)"
v4_art = """
\033[94m
|--[P]--|--[U]--|--[B]--|
|        |       |
|__      |__     |__
   |        |       |
|--[C]--|--[R]--|--[A]--|
|        |       |
|__      |__     |__
   |        |       |
|--[W]--|--[L]--|--[E]--|--[R]--|
\033[97m
... data ... crawl ... analyze ...
\033[0m
"""

v5_name = "V5: 大气-信息框 (亮白/亮青色)"
v5_art = """
\033[97m
+--------------------------+
|                          |
|  \033[96mP U B C R A W L E R\033[97m     |
|      \033[97m...initializing...  |
+--------------------------+
\033[0m
"""

# --- 存储和打印 ---

logos_to_display = [
    (v1_name, v1_art),
    (v2_name, v2_art),
    (v3_name, v3_art),
    (v4_name, v4_art),
    (v5_name, v5_art)
]

def display_logos():
    """
    依次打印所有Logo，带暂停。
    """
    print("正在为您展示5款 'PubCrawler' 科技感Logo设计...")
    print("（请确保您的控制台支持ANSI颜色代码以获得最佳效果）")
    time.sleep(2)

    for name, art in logos_to_display:
        print("\n" * 4)  # 打印几个换行符来分隔
        # \033[1m 是加粗
        print(f"\033[1m--- {name} ---\033[0m")
        print(art)
        print(f"\033[1m--- End of {name.split(':')[0]} ---\033[0m")
        time.sleep(2.5) # 暂停2.5秒

    print("\n" * 4)
    print("全部版本展示完毕！")
    print("您可以从上面的5个版本中选择一个，将对应的字符串（例如 v1_art）复制到您的项目代码中。")

# --- 主程序执行 ---
if __name__ == "__main__":
    display_logos()

==================== End of: src\test\logo.py ====================



==================== Start of: src\test\test_acl.py ====================

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ==============================================================================
# --- 实验配置 ---
# ==============================================================================

# 1. 设置用于测试的线程数列表。脚本将为列表中的每个值运行一次测试。
#    可以根据需要调整这个列表，例如增加 40, 56 等。
THREADS_TO_TEST = [4, 8, 12, 16, 24, 32, 48, 64]

# 2. 选择一个固定的年份进行测试，以保证每次测试的工作量一致。
#    建议选择一个论文数量较多的年份，如 2024 或 2025。
YEAR_FOR_TESTING = 2024

# 3. 设置用于测试的论文数量。数量不宜过少（无法体现差距），也不宜过多（测试时间太长）。
#    100-200 是一个比较理想的范围。
PAPERS_FOR_TESTING = 150

# ==============================================================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
ACL_BASE_URL_PATTERN = "https://aclanthology.org/volumes/{year}.acl-long/"


def get_paper_links_for_workload(year: int, limit: int):
    """
    获取一个固定的工作负载（论文链接列表）用于所有测试。
    """
    target_url = ACL_BASE_URL_PATTERN.format(year=year)
    print(f"[*] 准备实验环境: 正在从 {target_url} 获取论文列表...")

    try:
        response = requests.get(target_url, headers=HEADERS, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'lxml')
        link_tags = soup.select('p.d-sm-flex strong a.align-middle')
        paper_links = [urljoin(target_url, tag['href']) for tag in link_tags if f'{year}.acl-long.0' not in tag['href']]

        actual_found = len(paper_links)
        print(f"[*] 找到了 {actual_found} 篇有效论文。")

        # 智能限制：确保我们有足够的数据，但又不超过实际数量
        actual_limit = min(limit, actual_found)
        if actual_limit < limit:
            print(f"[*] [警告] 期望测试 {limit} 篇，但只找到 {actual_found} 篇。将以 {actual_limit} 篇为准。")

        print(f"[*] 实验工作负载已确定: {actual_limit} 篇论文。")
        return paper_links[:actual_limit]
    except requests.RequestException as e:
        print(f"[!] [错误] 准备工作负载失败: {e}")
        return None


def scrape_single_paper_details(url: str):
    """爬取单个详情页的核心函数。在测试中，我们只关心它是否成功完成。"""
    try:
        # 使用更长的超时，因为并发时网络可能会拥堵
        response = requests.get(url, headers=HEADERS, timeout=25)
        response.raise_for_status()
        # 这里我们不需要解析，只需要确保请求成功返回即可模拟真实耗时
        return True
    except Exception:
        return False


def run_single_test(worker_count: int, urls_to_crawl: list):
    """
    使用指定的线程数，对给定的URL列表执行一次完整的爬取测试。
    """
    print("\n" + "-" * 60)
    print(f"🧪 正在测试: {worker_count} 个并发线程...")

    start_time = time.time()

    with ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(scrape_single_paper_details, url) for url in urls_to_crawl]

        # 使用tqdm来可视化进度
        for _ in tqdm(as_completed(futures), total=len(urls_to_crawl), desc=f"   - 进度 ({worker_count}线程)"):
            pass

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"   -> 完成! 耗时: {elapsed_time:.2f} 秒")
    return elapsed_time


def main():
    """主函数，调度所有测试并生成最终报告。"""
    print("=" * 60)
    print("      并发线程数性能优化器 for ACL Crawler")
    print("=" * 60)

    # 1. 准备一个固定的、用于所有测试的工作负载
    workload_urls = get_paper_links_for_workload(YEAR_FOR_TESTING, PAPERS_FOR_TESTING)
    if not workload_urls:
        print("[!] 无法继续测试，因为未能获取到论文列表。")
        return

    # 2. 循环执行测试
    experiment_results = []
    for num_threads in THREADS_TO_TEST:
        duration = run_single_test(num_threads, workload_urls)
        experiment_results.append({
            "threads": num_threads,
            "time": duration
        })
        # 在每次测试间歇2秒，避免对服务器造成连续冲击
        time.sleep(2)

    # 3. 分析结果并生成报告
    if not experiment_results:
        print("[!] 没有完成任何测试。")
        return

    print("\n\n" + "#" * 60)
    print("📊            最终性能测试报告")
    print(f"            (测试负载: {len(workload_urls)} 篇论文)")
    print("#" * 60)
    print(f"{'线程数':<10} | {'总耗时 (秒)':<15} | {'每秒爬取论文数':<20}")
    print("-" * 60)

    best_result = None
    best_performance = 0

    for res in experiment_results:
        threads = res['threads']
        total_time = res['time']

        if total_time > 0:
            papers_per_second = len(workload_urls) / total_time
            print(f"{threads:<10} | {total_time:<15.2f} | {papers_per_second:<20.2f}")

            if papers_per_second > best_performance:
                best_performance = papers_per_second
                best_result = res
        else:
            print(f"{threads:<10} | {total_time:<15.2f} | {'N/A'}")

    print("-" * 60)

    # 4. 给出最终建议
    if best_result:
        optimal_threads = best_result['threads']
        print("\n🏆 结论:")
        print(f"根据本次在您当前网络环境下的实测结果：")
        print(f"当线程数设置为 **{optimal_threads}** 时，爬取效率最高，达到了每秒 **{best_performance:.2f}** 篇论文。")
        print(f"建议您在 PubCrawler 的 YAML 配置文件中将 ACL 和 CVF 任务的 `max_workers` 设置为 **{optimal_threads}**。")
    else:
        print("\n[!] 未能确定最佳线程数。")

    print("#" * 60)


if __name__ == "__main__":
    main()

==================== End of: src\test\test_acl.py ====================



==================== Start of: src\utils\console_logger.py ====================

# FILE: src/utils/console_logger.py (Banner Updated to Tech Pattern)

import logging
import sys

# 尝试导入 colorama，如果失败则优雅降级
try:
    import colorama
    from colorama import Fore, Style, Back

    colorama.init(autoreset=True)

    # 定义颜色常量
    COLORS = {
        'DEBUG': Style.DIM + Fore.WHITE,
        'INFO': Style.NORMAL + Fore.WHITE,
        'WARNING': Style.BRIGHT + Fore.YELLOW,
        'ERROR': Style.BRIGHT + Fore.RED,
        'CRITICAL': Style.BRIGHT + Back.RED + Fore.WHITE,
        'RESET': Style.RESET_ALL,

        # 自定义颜色，用于特殊高亮
        'BANNER_BLUE': Style.BRIGHT + Fore.BLUE,
        'BANNER_CYAN': Style.BRIGHT + Fore.CYAN,
        'BANNER_GREEN': Style.BRIGHT + Fore.GREEN,
        'BANNER_WHITE': Style.BRIGHT + Fore.WHITE,
        'PHASE': Style.BRIGHT + Fore.BLUE,
        'TASK_START': Style.BRIGHT + Fore.MAGENTA,
        'SUCCESS': Style.BRIGHT + Fore.GREEN,
        'STEP': Style.DIM + Fore.WHITE,  # <-- 我们会用这个
    }

    IS_COLORAMA_AVAILABLE = True

except ImportError:
    # 如果没有安装 colorama，则所有颜色代码都为空字符串
    COLORS = {key: '' for key in
              ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL', 'RESET', 'BANNER_BLUE', 'BANNER_CYAN', 'BANNER_GREEN',
               'BANNER_WHITE', 'PHASE', 'TASK_START', 'SUCCESS',
               'STEP']}
    IS_COLORAMA_AVAILABLE = False


class ColoredFormatter(logging.Formatter):
    """
    一个自定义的日志格式化器，用于在控制台输出中添加颜色。
    """

    def __init__(self, fmt, datefmt=None, style='%'):
        super().__init__(fmt, datefmt, style)

    def format(self, record):
        # 获取原始的日志消息
        log_message = super().format(record)

        if IS_COLORAMA_AVAILABLE:
            # 根据日志级别应用不同的颜色
            level_color = COLORS.get(record.levelname, COLORS['INFO'])
            return f"{level_color}{log_message}{COLORS['RESET']}"
        else:
            return log_message


def print_banner():
    """打印项目启动的 ASCII Art 横幅 (科技感图案版)。"""

    # -------------------【修改点在这里】-------------------
    # 按用户要求，改为科技感图案，放弃大字母块
    # V7: 科技-网络节点 (亮蓝色/亮白色)

    # 我们使用 f-string 来嵌入颜色代码
    banner_art = f"""
{COLORS['BANNER_BLUE']}
        .--.
       / .. \\
    --(  PC  )--  {COLORS['BANNER_WHITE']}PubCrawler{COLORS['RESET']}
       \\ .. /    {COLORS['STEP']}[Initializing...]{COLORS['RESET']}
        '--'
{COLORS['RESET']}
"""

    # -------------------【修改结束】-------------------

    if IS_COLORAMA_AVAILABLE:
        print(banner_art)  # 直接打印包含颜色的 f-string

    else:
        # 如果 colorama 不可用，打印一个手动去除颜色代码的无色版本
        no_color_art = r"""
        .--.
       / .. \
    --(  PC  )--  PubCrawler
       \ .. /    [Initializing...]
        '--'
"""
        print(no_color_art)

==================== End of: src\utils\console_logger.py ====================



==================== Start of: src\utils\downloader.py ====================

# FILE: src/utils/downloader.py (Tqdm Removed Version)

import requests
import re
from pathlib import Path

from src.crawlers.config import get_logger

logger = get_logger(__name__)

def download_single_pdf(paper: dict, pdf_dir: Path):
    """
    Downloads a single PDF file. This function is now designed to be called within a loop
    controlled by an external tqdm instance.
    """
    pdf_url = paper.get('pdf_url')
    title = paper.get('title', 'untitled')

    if not pdf_url:
        logger.warning(f"    -> Skipping download (no PDF URL): {title[:50]}...")
        return False

    sanitized_title = re.sub(r'[\\/*?:"<>|]', "", title).replace('\n', ' ').replace('\r', '')
    filename = (sanitized_title[:150] + ".pdf")
    filepath = pdf_dir / filename

    if filepath.exists():
        return True # Skip if already exists

    try:
        response = requests.get(pdf_url, stream=True, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except requests.exceptions.RequestException as e:
        logger.error(f"    [✖ ERROR] Failed to download {pdf_url}. Reason: {e}")
        if filepath.exists(): filepath.unlink() # Clean up failed download
        return False
    except Exception as e:
        logger.error(f"    [✖ ERROR] An unexpected error occurred for {pdf_url}. Reason: {e}")
        if filepath.exists(): filepath.unlink()
        return False

==================== End of: src\utils\downloader.py ====================



==================== Start of: src\utils\formatter.py ====================

# FILE: src/utils/formatter.py

import pandas as pd
from pathlib import Path
from datetime import datetime


def save_as_markdown(papers: list, task_name: str, output_dir: Path, wordcloud_path: str = None):
    """Saves a list of paper dictionaries as a formatted Markdown file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = output_dir / f"{task_name}_report_{timestamp}.md"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# {task_name} Papers ({timestamp})\n\n")
        f.write(f"Total papers found matching criteria: **{len(papers)}**\n\n")

        if wordcloud_path:
            f.write(f"## Trend Word Cloud\n\n")
            # --- 修复点: 确保路径在Markdown中是正确的相对路径 ---
            f.write(f"![Word Cloud](./{Path(wordcloud_path).name})\n\n")

        f.write("---\n\n")

        for i, paper in enumerate(papers, 1):
            title = paper.get('title', 'N/A').replace('\n', ' ')

            # --- 修复点: 健壮地处理作者字段，无论是字符串还是列表 ---
            authors_data = paper.get('authors', 'N/A')
            if isinstance(authors_data, list):
                authors = ", ".join(authors_data)
            else:
                authors = str(authors_data)  # 确保是字符串
            authors = authors.replace('\n', ' ')

            abstract = paper.get('abstract', 'N/A').replace('\n', ' ')
            pdf_url = paper.get('pdf_url', '#')

            f.write(f"### {i}. {title}\n\n")
            f.write(f"**Authors:** *{authors}*\n\n")

            if pdf_url and pdf_url != '#':
                f.write(f"**[PDF Link]({pdf_url})**\n\n")

            f.write(f"**Abstract:**\n")
            f.write(f"> {abstract}\n\n")
            f.write("---\n\n")

    print(f"Successfully saved Markdown report to {filename}")


def save_as_summary_txt(papers: list, task_name: str, output_dir: Path):
    """Saves a list of paper dictionaries as a formatted TXT file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = output_dir / f"{task_name}_summary_{timestamp}.txt"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"--- {task_name} Summary ({timestamp}) ---\n")
        f.write(f"Total papers found: {len(papers)}\n")
        f.write("=" * 40 + "\n\n")

        for i, paper in enumerate(papers, 1):
            title = paper.get('title', 'N/A').replace('\n', ' ')

            authors_data = paper.get('authors', 'N/A')
            if isinstance(authors_data, list):
                authors = ", ".join(authors_data)
            else:
                authors = str(authors_data)
            authors = authors.replace('\n', ' ')

            abstract = paper.get('abstract', 'N/A').replace('\n', ' ')
            pdf_url = paper.get('pdf_url', 'N/A')

            f.write(f"[{i}] Title: {title}\n")
            f.write(f"    Authors: {authors}\n")
            f.write(f"    PDF URL: {pdf_url}\n")
            f.write(f"    Abstract: {abstract}\n\n")

    print(f"Successfully saved TXT summary to {filename}")


def save_as_csv(papers: list, task_name: str, output_dir: Path):
    """Saves a list of paper dictionaries as a CSV file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y%m%d")
    filename = output_dir / f"{task_name}_data_{timestamp}.csv"

    # --- 修复点: 在转换为DataFrame之前，确保所有列表都变成字符串 ---
    processed_papers = []
    for paper in papers:
        new_paper = paper.copy()
        for key, value in new_paper.items():
            if isinstance(value, list):
                new_paper[key] = ", ".join(map(str, value))
        processed_papers.append(new_paper)

    df = pd.DataFrame(processed_papers)

    cols = ['title', 'authors', 'abstract', 'pdf_url', 'keywords', 'source_url']
    df_cols = [c for c in cols if c in df.columns] + [c for c in df.columns if c not in cols]
    df = df[df_cols]

    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"Successfully saved CSV data to {filename}")

==================== End of: src\utils\formatter.py ====================



==================== Start of: src\utils\network_utils.py ====================

# FILE: src/utils/network_utils.py

import requests
import time
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# 获取一个简单的日志记录器，或者你可以从主配置中传递一个
logger = logging.getLogger(__name__)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_session_with_retries(
    retries=5,
    backoff_factor=1,
    status_forcelist=(500, 502, 503, 504),
    session=None,
):
    """
    创建一个带有重试机制的 requests Session 对象。
    这对于处理临时的网络错误或服务器不稳定非常有效。
    """
    session = session or requests.Session()
    retry_strategy = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session


def robust_get(url: str, timeout: int = 30, retries: int = 5, backoff_factor: float = 1.0):
    """
    一个健壮的 GET 请求函数，集成了重试和更长的超时。
    :param url: 要请求的 URL
    :param timeout: 单次请求的超时时间（秒）
    :param retries: 最大重试次数
    :param backoff_factor: 重试的退避因子 (e.g., 1s, 2s, 4s...)
    :return: requests.Response 对象或 None
    """
    session = get_session_with_retries(retries=retries, backoff_factor=backoff_factor)
    try:
        response = session.get(url, headers=HEADERS, timeout=timeout)
        response.raise_for_status()  # 如果状态码是 4xx 或 5xx，则抛出异常
        return response
    except requests.exceptions.RequestException as e:
        # 使用 logger.error 而不是 print，以便记录到日志文件
        logger.error(f"    [✖ NETWORK ERROR] 请求失败，已达到最大重试次数 for URL: {url}. Error: {e}")
        return None

==================== End of: src\utils\network_utils.py ====================



==================== Start of: src\utils\tqdm_logger.py ====================

# FILE: src/utils/tqdm_logger.py

import logging
from tqdm import tqdm

class TqdmLoggingHandler(logging.Handler):
    """
    一个自定义的日志处理器，它能将日志消息通过 tqdm.write() 输出，
    从而避免与 tqdm 进度条的显示发生冲突。
    """
    def __init__(self, level=logging.NOTSET):
        super().__init__(level)

    def emit(self, record):
        try:
            msg = self.format(record)
            # 使用 tqdm.write 来打印消息，它会自动处理换行，且不会干扰进度条
            tqdm.write(msg)
            self.flush()
        except Exception:
            self.handleError(record)

==================== End of: src\utils\tqdm_logger.py ====================

