

==================== Start of: app.py ====================

# FILE: app.py (Gradio Web UI for PubCrawler - v1.2.3 - Final Fixes)

import gradio as gr
import sys
import textwrap
from typing import List, Dict, Any, Tuple

# --- ä» search_service å¯¼å…¥æ‰€æœ‰åŠŸèƒ½å’Œé…ç½® ---
from src.search.search_service import (
    initialize_components,
    keyword_search,
    semantic_search,
    save_results_to_markdown,
    generate_ai_response,
    _sqlite_conn,
    _initialized,
    ZHIPUAI_API_KEY,
    SEARCH_RESULTS_DIR
)

# Gradioå¯åŠ¨æ—¶è°ƒç”¨åˆå§‹åŒ–å‡½æ•° (ç¡®ä¿åœ¨ä»»ä½•å‡½æ•°è¢«Gradioè°ƒç”¨å‰å®Œæˆ)
initialize_components()

# --- Gradio UI æ ¸å¿ƒé€»è¾‘ ---

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨å½“å‰æœç´¢ç»“æœï¼Œä»¥ä¾¿AIå’Œä¿å­˜åŠŸèƒ½è®¿é—®
current_search_results: List[Dict[str, Any]] = []
current_query_string: str = ""


def perform_search_and_reset_chat(query_input: str) -> Tuple[
    gr.Dataframe, str, str, gr.Column, gr.Accordion, gr.Button]:
    """
    åœ¨Gradio UIä¸­æ‰§è¡Œæœç´¢å¹¶æ›´æ–°UIç»„ä»¶ã€‚
    """
    global current_search_results, current_query_string
    current_query_string = query_input
    current_search_results = []

    results: List[Dict[str, Any]] = []
    stats: Dict[str, Any] = {"total_found": 0, "distribution": {}, "message": "æœç´¢æœªæ‰§è¡Œã€‚"}

    if not _initialized:
        error_msg = "åç«¯æœåŠ¡æœªæˆåŠŸåˆå§‹åŒ–ã€‚"
        return gr.Dataframe(value=[]), error_msg, "åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•æœç´¢ã€‚", gr.Column(visible=False), gr.Accordion(
            open=False), gr.Button(interactive=False)

    if query_input.lower().startswith('sem:'):
        semantic_query = query_input[4:].strip()
        if semantic_query:
            results, stats = semantic_search(semantic_query)
        else:
            stats['message'] = "è¯­ä¹‰æœç´¢æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©ºã€‚"
    else:
        results, stats = keyword_search(query_input)

    current_search_results = results

    # æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯
    stats_markdown = f"**æ€»è®¡æ‰¾åˆ° {stats['total_found']} ç¯‡ç›¸å…³è®ºæ–‡ã€‚**\n\n"
    if stats['distribution']:
        stats_markdown += "**åˆ†å¸ƒæƒ…å†µ:**\n"
        for conf_year, count in stats['distribution'].items():
            stats_markdown += f"- {conf_year}: {count} ç¯‡\n"
    else:
        stats_markdown += "æ— ç»“æœåˆ†å¸ƒä¿¡æ¯ã€‚\n"

    # æ ¼å¼åŒ–ç»“æœä¸ºGradioè¡¨æ ¼
    table_data = []
    for paper in results:
        authors = textwrap.shorten(paper.get('authors', 'N/A'), width=50, placeholder="...")
        title = textwrap.shorten(paper.get('title', 'N/A'), width=80, placeholder="...")
        similarity = f"{paper['similarity']:.2f}" if 'similarity' in paper and paper[
            'similarity'] is not None else "N/A"
        table_data.append([title, authors, paper.get('conference', 'N/A'), paper.get('year', 'N/A'), similarity])

    # æ ¹æ®æ˜¯å¦æœ‰ç»“æœå’ŒAPI Keyæ¥å†³å®šAIæŒ‰é’®æ˜¯å¦å¯ç”¨
    ai_button_interactive = bool(results and ZHIPUAI_API_KEY)

    return (gr.Dataframe(value=table_data, headers=["æ ‡é¢˜", "ä½œè€…", "ä¼šè®®", "å¹´ä»½", "ç›¸ä¼¼åº¦"]),
            stats.get('message', "æœç´¢å®Œæˆã€‚"),
            stats_markdown,
            gr.Column(visible=False),
            gr.Accordion(open=False),
            gr.Button(interactive=ai_button_interactive))


def save_current_results_gradio() -> str:
    """
    Gradio UIä¸­ä¿å­˜å½“å‰æœç´¢ç»“æœçš„å›è°ƒå‡½æ•°ã€‚
    """
    global current_search_results, current_query_string
    if not current_search_results:
        return "æ²¡æœ‰æœç´¢ç»“æœå¯ä¿å­˜ã€‚"
    return save_results_to_markdown(current_search_results, current_query_string)


# --- ã€é‡è¦ä¿®æ”¹ã€‘: AI å¯¹è¯å‡½æ•°é€‚é… `type="messages"` ---
def handle_chat_interaction(user_message: str, chat_history: List[Dict[str, str]]):
    """
    å¤„ç†ç”¨æˆ·çš„èŠå¤©è¾“å…¥ï¼Œè°ƒç”¨AIæœåŠ¡ï¼Œå¹¶è¿”å›å“åº”ã€‚
    ç°åœ¨çš„ chat_history æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œä¾‹å¦‚: [{"role": "user", "content": "ä½ å¥½"}]
    """
    global current_search_results
    if not current_search_results:
        chat_history.append({"role": "user", "content": user_message})
        chat_history.append({"role": "assistant", "content": "é”™è¯¯ï¼šæ²¡æœ‰å¯ä¾›å¯¹è¯çš„æœç´¢ç»“æœã€‚"})
        return chat_history

    chat_history.append({"role": "user", "content": user_message})

    # generate_ai_response å‡½æ•°æœ¬èº«å°±éœ€è¦è¿™ç§æ ¼å¼ï¼Œæ‰€ä»¥ç°åœ¨æ— éœ€è½¬æ¢
    ai_response = generate_ai_response(
        chat_history=chat_history,
        search_results_context=current_search_results
    )

    chat_history.append({"role": "assistant", "content": ai_response})
    return chat_history


def clear_chat():
    """æ¸…ç©ºèŠå¤©è®°å½•"""
    return [], ""


# --- Gradio UI å¸ƒå±€ ---
with gr.Blocks(title="PubCrawler AI Assistant") as demo:
    gr.Markdown(
        """
        # ğŸ“š PubCrawler AI å­¦æœ¯åŠ©æ‰‹
        æ¬¢è¿ä½¿ç”¨ PubCrawlerï¼åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥æœç´¢å­¦æœ¯è®ºæ–‡ï¼ŒæŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶å°†ç»“æœä¿å­˜æˆ–ä¸AIè¿›è¡Œå¯¹è¯ã€‚

        ---

        ### æœç´¢è¯­æ³•:
        - `å…³é”®è¯` æˆ– `çŸ­è¯­` (ä¾‹å¦‚: `transformer`, `"large language model"`)
        - `å­—æ®µæœç´¢`: `author:vaswani`, `title:"vision transformer"`, `abstract:diffusion`
        - `é€»è¾‘ç»„åˆ`: `transformer AND author:vaswani`, `"large language model" OR efficient`
        - `è¯­ä¹‰æœç´¢`: åœ¨æŸ¥è¯¢å‰åŠ ä¸Š `sem:` (ä¾‹å¦‚: `sem: efficiency of few-shot learning`)
        """
    )

    with gr.Row():
        query_input = gr.Textbox(
            label="è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢",
            placeholder="ä¾‹å¦‚: transformer author:vaswani æˆ– sem: efficiency of few-shot learning",
            scale=4
        )
        search_button = gr.Button("æœç´¢", variant="primary", scale=1)

    status_output = gr.Textbox(label="çŠ¶æ€/æ¶ˆæ¯", interactive=False)

    with gr.Row():
        stats_markdown_output = gr.Markdown(
            value="--- æŸ¥è¯¢ç»“æœç»Ÿè®¡ --- \næ€»è®¡æ‰¾åˆ° 0 ç¯‡ç›¸å…³è®ºæ–‡ã€‚\næ— ç»“æœåˆ†å¸ƒä¿¡æ¯ã€‚",
            label="ç»“æœç»Ÿè®¡"
        )

    results_dataframe = gr.Dataframe(
        headers=["æ ‡é¢˜", "ä½œè€…", "ä¼šè®®", "å¹´ä»½", "ç›¸ä¼¼åº¦"],
        col_count=(5, "fixed"),
        interactive=False,
        label="æœç´¢ç»“æœ"
    )

    with gr.Row():
        save_button = gr.Button("ä¿å­˜å½“å‰ç»“æœåˆ° Markdown")
        start_chat_button = gr.Button("ä¸AIå¯¹è¯ (éœ€å…ˆæœç´¢)", interactive=False)

    with gr.Accordion("ğŸ¤– AI å¯¹è¯çª—å£", open=False) as chat_accordion:
        with gr.Column(visible=True) as chat_interface_column:
            # ã€é‡è¦ä¿®æ”¹ã€‘: ä¿®å¤ UserWarning
            chatbot = gr.Chatbot(label="ä¸AIçš„å¯¹è¯", type="messages")
            chat_input = gr.Textbox(label="ä½ çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šè¯·æ€»ç»“ä¸€ä¸‹è¿™äº›è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚")
            with gr.Row():
                chat_submit_btn = gr.Button("å‘é€", variant="primary")
                chat_clear_btn = gr.Button("æ¸…é™¤å¯¹è¯")

    # --- ç»‘å®šäº‹ä»¶ ---
    search_button.click(
        fn=perform_search_and_reset_chat,
        inputs=query_input,
        outputs=[results_dataframe, status_output, stats_markdown_output, chat_interface_column, chat_accordion,
                 start_chat_button]
    )

    query_input.submit(
        fn=perform_search_and_reset_chat,
        inputs=query_input,
        outputs=[results_dataframe, status_output, stats_markdown_output, chat_interface_column, chat_accordion,
                 start_chat_button]
    )

    save_button.click(
        fn=save_current_results_gradio,
        inputs=[],
        outputs=status_output
    )

    start_chat_button.click(
        fn=lambda: (gr.Accordion(open=True), gr.Column(visible=True)),
        inputs=None,
        outputs=[chat_accordion, chat_interface_column]
    )

    chat_submit_btn.click(
        fn=handle_chat_interaction,
        inputs=[chat_input, chatbot],
        outputs=[chatbot]
    ).then(lambda: "", inputs=None, outputs=chat_input)

    chat_input.submit(
        fn=handle_chat_interaction,
        inputs=[chat_input, chatbot],
        outputs=[chatbot]
    ).then(lambda: "", inputs=None, outputs=chat_input)

    chat_clear_btn.click(
        fn=clear_chat,
        inputs=None,
        outputs=[chatbot, chat_input]
    )

# è¿è¡ŒGradioåº”ç”¨
if __name__ == "__main__":
    if not _initialized:
        print(f"æ— æ³•å¯åŠ¨Gradioåº”ç”¨ï¼Œåç«¯åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        sys.exit(1)
    else:
        SEARCH_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        # ã€é‡è¦ä¿®æ”¹ã€‘: æ·»åŠ  inbrowser=True
        demo.launch(share=True, inbrowser=True)


==================== End of: app.py ====================



==================== Start of: getallcode.py ====================

import os

# --- é…ç½® ---

# 1. æŒ‡å®šè¦åŒ…å«çš„æ–‡ä»¶åç¼€å
TARGET_EXTENSIONS = ['.py', '.html', '.css', '.yaml']

# 2. æŒ‡å®šè¾“å‡ºçš„èšåˆæ–‡ä»¶å
OUTPUT_FILENAME = 'combined_files.txt'

# 3. æŒ‡å®šè¦æ’é™¤çš„ç›®å½•å
EXCLUDED_DIRS = ['.git', '__pycache__', 'node_modules', '.vscode', '.venv']


# --- è„šæœ¬ ---

def combine_files():
    """
    éå†å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•åŠå­ç›®å½•,å°†æŒ‡å®šåç¼€çš„æ–‡ä»¶å†…å®¹åˆå¹¶åˆ°ä¸€ä¸ªtxtæ–‡ä»¶ä¸­ã€‚
    """

    # è·å–æ­¤è„šæœ¬æ‰€åœ¨çš„ç›®å½•
    # __file__ æ˜¯ Python çš„ä¸€ä¸ªå†…ç½®å˜é‡ï¼Œè¡¨ç¤ºå½“å‰æ‰§è¡Œçš„è„šæœ¬æ–‡ä»¶çš„è·¯å¾„
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        # å¦‚æœåœ¨ REPL æˆ– notebook ä¸­è¿è¡Œï¼Œ__file__ å¯èƒ½æœªå®šä¹‰
        script_dir = os.getcwd()
        print(f"è­¦å‘Š: æ— æ³•è·å–è„šæœ¬è·¯å¾„, ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•: {script_dir}")

    print(f"å¼€å§‹åœ¨ {script_dir} ä¸­æœç´¢æ–‡ä»¶...")
    print(f"å°†æ’é™¤ä»¥ä¸‹ç›®å½•: {', '.join(EXCLUDED_DIRS)}")

    found_files_count = 0

    # 'w' æ¨¡å¼ä¼šè¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ã€‚ç¡®ä¿æ¯æ¬¡è¿è¡Œéƒ½æ˜¯ä¸€ä¸ªå…¨æ–°çš„èšåˆæ–‡ä»¶ã€‚
    # ä½¿ç”¨ utf-8 ç¼–ç å¤„ç†å„ç§æ–‡ä»¶å†…å®¹
    try:
        with open(os.path.join(script_dir, OUTPUT_FILENAME), 'w', encoding='utf-8') as outfile:

            # os.walk ä¼šé€’å½’éå†ç›®å½•
            # root: å½“å‰ç›®å½•è·¯å¾„
            # dirs: å½“å‰ç›®å½•ä¸‹çš„å­ç›®å½•åˆ—è¡¨
            # files: å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨
            for root, dirs, files in os.walk(script_dir):

                # *** ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œ ***
                # é€šè¿‡ä¿®æ”¹ dirs åˆ—è¡¨ (dirs[:]) æ¥é˜»æ­¢ os.walk è¿›ä¸€æ­¥éå†è¿™äº›ç›®å½•
                dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]

                for filename in files:
                    # æ£€æŸ¥æ–‡ä»¶åç¼€æ˜¯å¦åœ¨æˆ‘ä»¬çš„ç›®æ ‡åˆ—è¡¨ä¸­
                    if any(filename.endswith(ext) for ext in TARGET_EXTENSIONS):

                        file_path = os.path.join(root, filename)

                        # è·å–ç›¸å¯¹è·¯å¾„ï¼Œä»¥ä¾¿åœ¨è¾“å‡ºæ–‡ä»¶ä¸­æ›´æ¸…æ™°åœ°æ˜¾ç¤º
                        relative_path = os.path.relpath(file_path, script_dir)

                        # æ’é™¤è¾“å‡ºæ–‡ä»¶æœ¬èº«ï¼Œé˜²æ­¢å®ƒæŠŠè‡ªå·±ä¹ŸåŒ…å«è¿›å»
                        if relative_path == OUTPUT_FILENAME:
                            continue

                        print(f"  æ­£åœ¨æ·»åŠ : {relative_path}")
                        found_files_count += 1

                        # å†™å…¥æ–‡ä»¶åˆ†éš”ç¬¦å’Œè·¯å¾„
                        outfile.write(f"\n\n{'=' * 20} Start of: {relative_path} {'=' * 20}\n\n")

                        try:
                            # ä»¥åªè¯» ('r') æ¨¡å¼æ‰“å¼€æºæ–‡ä»¶
                            # ä½¿ç”¨ errors='ignore' æ¥è·³è¿‡æ— æ³•è§£ç çš„å­—ç¬¦
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                                content = infile.read()
                                outfile.write(content)

                        except Exception as e:
                            # å¦‚æœè¯»å–å¤±è´¥ï¼ˆä¾‹å¦‚æƒé™é—®é¢˜ï¼‰ï¼Œåˆ™è®°å½•é”™è¯¯
                            outfile.write(f"--- æ— æ³•è¯»å–æ–‡ä»¶: {e} ---\n")
                            print(f"  [é”™è¯¯] æ— æ³•è¯»å– {relative_path}: {e}")

                        # å†™å…¥æ–‡ä»¶ç»“æŸç¬¦
                        outfile.write(f"\n\n{'=' * 20} End of: {relative_path} {'=' * 20}\n\n")

        print(f"\nå®Œæˆï¼æˆåŠŸèšåˆ {found_files_count} ä¸ªæ–‡ä»¶ã€‚")
        print(f"è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜ä¸º: {os.path.join(script_dir, OUTPUT_FILENAME)}")

    except IOError as e:
        print(f"åˆ›å»ºè¾“å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# --- æ‰§è¡Œ ---
if __name__ == "__main__":
    combine_files()

==================== End of: getallcode.py ====================



==================== Start of: streamlit_app.py ====================

# FILE: streamlit_app.py
# æ”¾ç½®äºé¡¹ç›®æ ¹ç›®å½•ï¼Œä¸ 'src' å’Œ 'app.py' åŒçº§ã€‚
# è¿è¡Œ: streamlit run streamlit_app.py

import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import logging
import re
import math
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import traceback # ç”¨äºæ›´è¯¦ç»†çš„é”™è¯¯æ•è·

# -----------------------------------------------------------------
# 1. å¯¼å…¥é¡¹ç›®æ¨¡å— (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
try:
    from src.search.search_service import (
        initialize_components, keyword_search, semantic_search,
        generate_ai_response, get_stats_summary, _initialized,
        ZHIPUAI_API_KEY, SEARCH_RESULTS_DIR
    )
    from src.crawlers.config import METADATA_OUTPUT_DIR, TRENDS_OUTPUT_DIR
except ImportError as e:
    st.error(f"å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥ï¼é”™è¯¯: {e}\n\nè¯·ç¡®ä¿æ»¡è¶³æ‰€æœ‰ä¾èµ–å’Œæ–‡ä»¶ç»“æ„è¦æ±‚ã€‚")
    st.stop()

# -----------------------------------------------------------------
# 2. åº”ç”¨çº§å¸¸é‡ (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
STREAMLIT_AI_CONTEXT_PAPERS = 20
RESULTS_PER_PAGE = 25
ANALYSIS_TOP_N = 50 # è¶‹åŠ¿åˆ†æå›¾è¡¨é»˜è®¤æ˜¾ç¤º Top N ä¸»é¢˜

# -----------------------------------------------------------------
# 3. Streamlit é¡µé¢é…ç½®ä¸åç«¯åˆå§‹åŒ– (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
st.set_page_config(page_title="PubCrawler Pro ğŸš€", layout="wide", initial_sidebar_state="expanded")

@st.cache_resource
def load_backend_components():
    print("--- [Streamlit] æ­£åœ¨åˆå§‹åŒ– PubCrawler åç«¯æœåŠ¡... ---")
    if not _initialized:
        try: initialize_components()
        except Exception as e:
            logging.error(f"åç«¯åˆå§‹åŒ–å¤±è´¥: {e}"); return False
    if not _initialized: print("--- [Streamlit] åç«¯åˆå§‹åŒ–å¤±è´¥! ---"); return False
    print("--- [Streamlit] åç«¯æœåŠ¡å‡†å¤‡å°±ç»ªã€‚ ---")
    return True

backend_ready = load_backend_components()
if not backend_ready:
    st.error("åç«¯æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼è¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚"); st.stop()

# -----------------------------------------------------------------
# 4. Streamlit é¡µé¢çŠ¶æ€ç®¡ç† (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
if "chat_history" not in st.session_state: st.session_state.chat_history: List[Dict[str, str]] = []
if "current_search_results" not in st.session_state: st.session_state.current_search_results: List[Dict[str, Any]] = []
if "current_filtered_results" not in st.session_state: st.session_state.current_filtered_results: List[Dict[str, Any]] = []
if "current_query" not in st.session_state: st.session_state.current_query: str = ""
if "current_page" not in st.session_state: st.session_state.current_page: int = 1

# -----------------------------------------------------------------
# 5. è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------
def find_analysis_files():
    """
    æ‰«æ output/ ç›®å½•æŸ¥æ‰¾åˆ†æ CSV æ–‡ä»¶ï¼Œæ— ç¼“å­˜ã€‚
    v1.6: åŒºåˆ† CSV ç±»å‹ (raw_data, summary_table, trends)ã€‚
    è¿”å›:
        analysis_data (dict): æŒ‰ ä¼šè®®/å¹´ä»½ ç»„ç»‡çš„ {"csvs": [{"path": Path, "type": str}]} å­—å…¸ã€‚
        all_conferences (list): æ‰¾åˆ°çš„æ‰€æœ‰ä¼šè®®åç§°åˆ—è¡¨ã€‚
        all_years (list): æ‰¾åˆ°çš„æ‰€æœ‰å¹´ä»½åˆ—è¡¨ã€‚
    """
    print("--- [Streamlit] æ­£åœ¨æ‰«æåˆ†ææ–‡ä»¶ (v1.6 - æ— ç¼“å­˜)... ---")
    scan_dirs = {"metadata": METADATA_OUTPUT_DIR, "trends": TRENDS_OUTPUT_DIR}
    analysis_data = {}
    all_conferences = set(); all_years = set()
    for dir_type, base_path in scan_dirs.items():
        if not base_path.exists(): continue
        csv_files = []
        if dir_type == "metadata":
            # æŸ¥æ‰¾ analysis/ ç›®å½•ä¸‹çš„ CSV (é€šå¸¸æ˜¯ summary table)
            csv_files.extend(list(base_path.rglob("analysis/*.csv")))
            # æŸ¥æ‰¾ metadata/conf/year/ ç›®å½•ä¸‹çš„ CSV (é€šå¸¸æ˜¯ raw data)
            csv_files.extend(list(base_path.rglob("*_data_*.csv")))
        elif dir_type == "trends":
            csv_files.extend(list(base_path.rglob("*.csv"))) # é€šå¸¸æ˜¯ trends æ•°æ®

        for f in csv_files:
            try:
                conf, year = None, None
                csv_type = "unknown" # é»˜è®¤ç±»å‹

                # è¯†åˆ«æ–‡ä»¶ç±»å‹å’Œä½ç½®
                if dir_type == "metadata":
                    if f.parent.name == "analysis":
                        year = f.parent.parent.name
                        conf = f.parent.parent.parent.name
                        # å¯å‘å¼åˆ¤æ–­æ˜¯å¦ä¸º summary_table
                        if "summary_table" in f.name or "4_summary_table" in f.name:
                             csv_type = "summary_table"
                        else:
                             csv_type = "analysis_other" # å…¶ä»–åˆ†ææ–‡ä»¶
                    elif "_data_" in f.name:
                        year = f.parent.name
                        conf = f.parent.parent.name
                        csv_type = "raw_data"
                elif dir_type == "trends":
                    conf = f.parent.name
                    year = "Cross-Year"
                    csv_type = "trends"

                # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
                if conf and year:
                    all_conferences.add(conf); all_years.add(year)
                    if conf not in analysis_data: analysis_data[conf] = {}
                    if year not in analysis_data[conf]: analysis_data[conf][year] = {"csvs": []}
                    # æ£€æŸ¥æ˜¯å¦é‡å¤æ·»åŠ  (è·¯å¾„ç›¸åŒ)
                    if not any(item["path"] == f for item in analysis_data[conf][year]["csvs"]):
                        analysis_data[conf][year]["csvs"].append({"path": f, "type": csv_type})
            except Exception as scan_e: print(f"æ‰«ææ–‡ä»¶ {f} æ—¶å‡ºé”™: {scan_e}")

    print(f"--- [Streamlit] æ‰«æå®Œæˆï¼Œæ‰¾åˆ°ä¼šè®®: {list(all_conferences)}, å¹´ä»½: {list(all_years)} ---")
    # å¯¹å¹´ä»½æ’åºï¼Œ"Cross-Year" å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€å•æŒ‰å­—ç¬¦ä¸²æ’åº
    return analysis_data, sorted(list(all_conferences)), sorted(list(all_years), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)


def save_results_to_markdown_fixed(results: List[Dict[str, Any]], query: str) -> str:
    """ (v1.3) ä¿å­˜ Markdownï¼ŒåŒ…å«æ‘˜è¦å’Œ PDF é“¾æ¥ã€‚"""
    # ... (å†…éƒ¨é€»è¾‘ä¿æŒä¸å˜) ...
    if not results: return "æ²¡æœ‰æœç´¢ç»“æœå¯ä¿å­˜ã€‚"
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# æœç´¢æŸ¥è¯¢: \"{query}\"\n\n**å…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title = paper.get('title', 'N/A'); authors = paper.get('authors', 'N/A')
            abstract = paper.get('abstract', 'N/A'); conf = paper.get('conference', 'N/A')
            year = paper.get('year', 'N/A'); pdf_url = paper.get('pdf_url', '#')
            f.write(f"### {idx}. {title}\n\n"); f.write(f"- **ä½œè€…**: {authors}\n")
            f.write(f"- **ä¼šè®®/å¹´ä»½**: {conf} {year}\n")
            if 'similarity' in paper: f.write(f"- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {paper['similarity']:.3f}\n")
            if pdf_url and pdf_url != '#':
                pdf_display_name = pdf_url.split('/')[-1] if '/' in pdf_url else "é“¾æ¥"
                f.write(f"- **PDF é“¾æ¥**: [{pdf_display_name}]({pdf_url})\n")
            f.write(f"\n**æ‘˜è¦:**\n> {abstract}\n\n---\n\n")
    return str(filename.resolve())


def get_numeric_columns(df): return df.select_dtypes(include=['number']).columns.tolist()
def get_categorical_columns(df): return df.select_dtypes(include=['object', 'category']).columns.tolist()

# -----------------------------------------------------------------
# 6. é¡µé¢æ¸²æŸ“å‡½æ•°
# -----------------------------------------------------------------

def render_search_and_chat_page():
    """ æ¸²æŸ“ "AI åŠ©æ‰‹ & æœç´¢" é¡µé¢ (v1.5 é€»è¾‘ï¼Œæ‘˜è¦éƒ¨åˆ†é«˜äº®) """
    # ... (å¤§éƒ¨åˆ†é€»è¾‘ä¸å˜ï¼Œä»…ä¿®æ”¹æ‘˜è¦æ˜¾ç¤ºéƒ¨åˆ†) ...
    st.header("ğŸ” PubCrawler Pro: AI åŠ©æ‰‹ & æœç´¢", divider="rainbow")
    _, conf_list, year_list = find_analysis_files()
    search_query = st.text_input("æœç´¢æœ¬åœ°å­¦æœ¯çŸ¥è¯†åº“...", key="search_input", placeholder="è¾“å…¥å…³é”®è¯æˆ– 'sem:' å‰ç¼€è¿›è¡Œè¯­ä¹‰æœç´¢", help="å…³é”®è¯æœç´¢: `transformer author:vaswani` | è¯­ä¹‰æœç´¢: `sem: few-shot learning efficiency`")
    col_f1, col_f2 = st.columns(2)
    with col_f1: selected_conferences = st.multiselect("ç­›é€‰ä¼šè®®", options=conf_list, key="filter_conf")
    with col_f2: selected_years = st.multiselect("ç­›é€‰å¹´ä»½", options=year_list, key="filter_year")
    is_new_search = (st.session_state.search_input != st.session_state.current_query)
    if is_new_search:
        with st.spinner(f"æ­£åœ¨æœç´¢: {st.session_state.search_input}..."):
            results, stats = [], {}; query = st.session_state.search_input.strip()
            if query.lower().startswith('sem:'):
                query_text = query[4:].strip();
                if query_text: results, stats = semantic_search(query_text)
            elif query: results, stats = keyword_search(query)
            st.session_state.current_search_results = results
            st.session_state.current_query = st.session_state.search_input
            st.session_state.chat_history = []; st.session_state.current_page = 1
            st.toast(stats.get('message', 'æœç´¢å®Œæˆ!'))
    if st.session_state.current_search_results:
        temp_filtered_results = st.session_state.current_search_results
        if selected_conferences: temp_filtered_results = [p for p in temp_filtered_results if p.get('conference') in selected_conferences]
        if selected_years: temp_filtered_results = [p for p in temp_filtered_results if str(p.get('year')) in selected_years]
        st.session_state.current_filtered_results = temp_filtered_results
    else: st.session_state.current_filtered_results = []
    if is_new_search: st.session_state.current_page = 1
    col_results, col_chat = st.columns([0.6, 0.4])
    with col_results:
        results_to_display = st.session_state.current_filtered_results
        st.subheader(f"æœç´¢ç»“æœ (ç­›é€‰å: {len(results_to_display)} ç¯‡ / åŸå§‹: {len(st.session_state.current_search_results)} ç¯‡)")
        if results_to_display:
            with st.container(border=True, height=300):
                stats = get_stats_summary(results_to_display); c1, c2 = st.columns(2)
                c1.metric("ç­›é€‰åæ‰¾åˆ°", f"{stats['total_found']} ç¯‡")
                if c2.button("ğŸ“¥ ä¿å­˜å½“å‰ *ç­›é€‰å* çš„ç»“æœåˆ° Markdown", use_container_width=True):
                    with st.spinner("æ­£åœ¨ä¿å­˜..."):
                        save_path = save_results_to_markdown_fixed(results_to_display, st.session_state.current_query)
                        st.success(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
                st.write("**ä¼šè®®/å¹´ä»½åˆ†å¸ƒ (ç­›é€‰å):**"); st.dataframe(pd.DataFrame(stats['distribution'].items(), columns=['æ¥æº', 'è®ºæ–‡æ•°']), use_container_width=True, hide_index=True)
            st.divider()
            total_items = len(results_to_display); total_pages = math.ceil(total_items / RESULTS_PER_PAGE)
            if st.session_state.current_page > total_pages: st.session_state.current_page = max(1, total_pages)
            page_display_text = f"ç¬¬ {st.session_state.current_page} / {total_pages} é¡µ ({total_items} æ¡)" if total_pages > 0 else "æ— ç»“æœ"
            col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
            with col_page1:
                 if st.button("ä¸Šä¸€é¡µ", disabled=st.session_state.current_page <= 1, use_container_width=True): st.session_state.current_page -= 1; st.rerun()
            with col_page2: st.markdown(f"<div style='text-align: center; margin-top: 8px;'>{page_display_text}</div>", unsafe_allow_html=True)
            with col_page3:
                 if st.button("ä¸‹ä¸€é¡µ", disabled=st.session_state.current_page >= total_pages, use_container_width=True): st.session_state.current_page += 1; st.rerun()
            start_idx = (st.session_state.current_page - 1) * RESULTS_PER_PAGE; end_idx = start_idx + RESULTS_PER_PAGE
            paginated_results = results_to_display[start_idx:end_idx]
            for i, paper in enumerate(paginated_results, start=start_idx + 1):
                with st.expander(f"**{i}. {paper.get('title', 'N/A')}**"):
                    if 'similarity' in paper: st.markdown(f"**è¯­ä¹‰ç›¸ä¼¼åº¦**: `{paper['similarity']:.3f}`")
                    st.markdown(f"**ä½œè€…**: *{paper.get('authors', 'N/A')}*"); st.markdown(f"**ä¼šè®®/å¹´ä»½**: {paper.get('conference', 'N/A')} {paper.get('year', 'N/A')}")
                    # ---ã€v1.6 æ‘˜è¦é«˜äº®ã€‘---
                    abstract_text = paper.get('abstract', None) # Use None to distinguish missing key vs empty string
                    if abstract_text is None or abstract_text == '' or abstract_text == 'N/A' or pd.isna(abstract_text):
                        st.markdown(f"**æ‘˜è¦**: <span style='color:orange; font-style: italic;'>[æ‘˜è¦ä¿¡æ¯ç¼ºå¤±æˆ–ä¸ºç©º]</span>", unsafe_allow_html=True)
                    else:
                        st.markdown(f"**æ‘˜è¦**: \n> {abstract_text}")

                    pdf_url = paper.get('pdf_url', '#');
                    if pdf_url and pdf_url != '#': st.link_button("ğŸ”— æ‰“å¼€ PDF é“¾æ¥", pdf_url)
        elif st.session_state.current_query: st.info("æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼ˆæˆ–è¢«ç­›é€‰æ¡ä»¶è¿‡æ»¤ï¼‰ã€‚")
        else: st.info("è¯·è¾“å…¥æŸ¥è¯¢ä»¥å¼€å§‹æœç´¢ã€‚")
    # ... (èŠå¤©éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜) ...
    with col_chat:
        st.subheader(f"ğŸ¤– AI å¯¹è¯åŠ©æ‰‹"); st.info(f"AI å°†åŸºäºä¸Šæ–‡æœç´¢åˆ°çš„ **Top {STREAMLIT_AI_CONTEXT_PAPERS}** ç¯‡è®ºæ–‡è¿›è¡Œå›ç­”ã€‚")
        chat_container = st.container(height=500, border=True)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]): st.markdown(message["content"])
        if not ZHIPUAI_API_KEY: st.error("æœªé…ç½® ZHIPUAI_API_KEY!"); chat_disabled = True
        elif not st.session_state.current_filtered_results: st.info("è¯·å…ˆæœç´¢å¹¶ç¡®ä¿æœ‰ç»“æœå†å¯¹è¯ã€‚"); chat_disabled = True
        else: chat_disabled = False
        if prompt := st.chat_input("åŸºäºæœç´¢ç»“æœæé—®...", disabled=chat_disabled, key="chat_input"):
            st.session_state.chat_history.append({"role": "user", "content": prompt}); st.rerun()
        if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
            with chat_container:
                for message in st.session_state.chat_history: # Re-render history
                    with st.chat_message(message["role"]): st.markdown(message["content"])
                with st.chat_message("assistant"): # Request AI
                    with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                        response = generate_ai_response(st.session_state.chat_history, st.session_state.current_filtered_results[:STREAMLIT_AI_CONTEXT_PAPERS])
                        st.markdown(response)
            st.session_state.chat_history.append({"role": "assistant", "content": response}); st.rerun() # Persist AI response
        if st.session_state.chat_history and not chat_disabled:
            if st.button("æ¸…é™¤å¯¹è¯å†å²", use_container_width=True): st.session_state.chat_history = []; st.rerun()

def render_analysis_dashboard_page():
    """
    æ¸²æŸ“ "è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜" é¡µé¢ (v1.6 - æ™ºèƒ½ CSV å¤„ç†å’Œæ ¸å¿ƒå›¾è¡¨)
    """
    st.header("ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜ (æ ¸å¿ƒå›¾è¡¨)", divider="rainbow")
    st.info(
        "é€‰æ‹©ä¼šè®®ã€å¹´ä»½å’Œå…·ä½“çš„ CSV åˆ†ææ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆæ ¸å¿ƒè¶‹åŠ¿å›¾è¡¨æˆ–å±•ç¤ºåŸå§‹æ•°æ®ã€‚"
    )

    analysis_data, all_conferences, _ = find_analysis_files()

    if not analysis_data:
        st.warning("æœªæ‰¾åˆ°ä»»ä½•åˆ†ææ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ `run_crawler.py`ã€‚")
        st.stop()

    # --- ç”¨æˆ·é€‰æ‹© ---
    selected_conf = st.selectbox("1. é€‰æ‹©ä¼šè®®", options=sorted(analysis_data.keys()))
    if not selected_conf: st.stop()

    conf_data = analysis_data[selected_conf]
    sorted_years = sorted(conf_data.keys(), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)
    selected_year = st.selectbox(f"2. é€‰æ‹© {selected_conf} çš„å¹´ä»½æˆ–è·¨å¹´æ•°æ®", options=sorted_years)
    if not selected_year: st.stop()

    files_info = conf_data[selected_year]

    if not files_info["csvs"]:
        st.warning(f"æœªæ‰¾åˆ° {selected_conf} {selected_year} çš„ CSV æ•°æ®æ–‡ä»¶ã€‚")
        st.stop()

    # ---ã€v1.6 æ–°å¢ã€‘è®©ç”¨æˆ·é€‰æ‹©è¦åˆ†æçš„ CSV æ–‡ä»¶ ---
    # åˆ›å»ºæ˜¾ç¤ºæ ‡ç­¾ï¼ŒåŒ…å«ç±»å‹ä¿¡æ¯
    csv_options = {f"{item['path'].name} ({item['type']})": item for item in files_info["csvs"]}
    selected_csv_label = st.selectbox("3. é€‰æ‹©è¦åˆ†æçš„ CSV æ–‡ä»¶", options=csv_options.keys())
    if not selected_csv_label: st.stop()

    selected_csv_info = csv_options[selected_csv_label]
    csv_path = selected_csv_info["path"]
    csv_type = selected_csv_info["type"]

    st.markdown(f"#### æ­£åœ¨åˆ†æ: `{csv_path.name}` (ç±»å‹: `{csv_type}`)")

    # --- åŠ è½½å¹¶å¤„ç† CSV ---
    try:
        df = pd.read_csv(csv_path)

        if df.empty:
            st.warning("CSV æ–‡ä»¶ä¸ºç©ºã€‚"); st.stop()

        # --- ã€æ ¸å¿ƒ v1.6ã€‘æ ¹æ® CSV ç±»å‹å†³å®šå±•ç¤ºå†…å®¹ ---
        st.markdown("---")

        # <<< --- A. å¦‚æœæ˜¯åˆ†ææ±‡æ€»æ–‡ä»¶ (summary_table) --- >>>
        if csv_type == "summary_table":
            st.subheader("ğŸ“ˆ æ ¸å¿ƒåˆ†æå›¾è¡¨")

            # å›¾è¡¨ 1: ä¸»é¢˜çƒ­åº¦ (æŒ‰è®ºæ–‡æ•°)
            required_cols_heat = ['Topic_Name', 'paper_count']
            if all(col in df.columns for col in required_cols_heat):
                st.markdown(f"##### 1. ä¸»é¢˜çƒ­åº¦æ’å (Top {ANALYSIS_TOP_N} è®ºæ–‡æ•°)")
                df_sorted_count = df.sort_values(by='paper_count', ascending=False).head(ANALYSIS_TOP_N)
                fig_bar_count = px.bar(df_sorted_count, x='paper_count', y='Topic_Name', orientation='h',
                                 title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} çƒ­é—¨ä¸»é¢˜ (è®ºæ–‡æ•°é‡)',
                                 labels={'paper_count': 'è®ºæ–‡æ•°é‡', 'Topic_Name': 'ä¸»é¢˜'},
                                 text_auto=True, height=max(600, len(df_sorted_count)*20))
                fig_bar_count.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_bar_count, use_container_width=True)
            else:
                st.caption(f"æ— æ³•ç”Ÿæˆä¸»é¢˜çƒ­åº¦å›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_cols_heat}")

            # å›¾è¡¨ 2: ä¸»é¢˜è´¨é‡ (æŒ‰å¹³å‡åˆ†)
            required_cols_quality = ['Topic_Name', 'avg_rating']
            if all(col in df.columns for col in required_cols_quality) and df['avg_rating'].notna().any():
                st.markdown(f"##### 2. ä¸»é¢˜è´¨é‡æ’å (Top {ANALYSIS_TOP_N} å¹³å‡å®¡ç¨¿åˆ†)")
                df_sorted_rating = df.dropna(subset=['avg_rating']).sort_values(by='avg_rating', ascending=False).head(ANALYSIS_TOP_N)
                if not df_sorted_rating.empty:
                    fig_bar_rating = px.bar(df_sorted_rating, x='avg_rating', y='Topic_Name', orientation='h',
                                     title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜ (å¹³å‡å®¡ç¨¿åˆ†)',
                                     labels={'avg_rating': 'å¹³å‡å®¡ç¨¿åˆ†', 'Topic_Name': 'ä¸»é¢˜'},
                                     text_auto='.2f', height=max(600, len(df_sorted_rating)*20))
                    fig_bar_rating.update_layout(yaxis={'categoryorder':'total ascending'})
                    st.plotly_chart(fig_bar_rating, use_container_width=True)
                else: st.caption("æœªèƒ½ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼šç­›é€‰/æ’åºåæ•°æ®ä¸ºç©ºã€‚")
            elif not all(col in df.columns for col in required_cols_quality):
                st.caption(f"æ— æ³•ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_cols_quality}")
            else: st.caption("æ— æ³•ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼š'avg_rating' åˆ—æ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")

            # å›¾è¡¨ 3: å†³ç­–æ„æˆ (æŒ‰æ¥æ”¶ç‡)
            decision_cols = ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']
            present_decision_cols = [col for col in decision_cols if col in df.columns]
            required_cols_decision = ['Topic_Name', 'acceptance_rate'] + present_decision_cols
            if present_decision_cols and all(col in df.columns for col in ['Topic_Name', 'acceptance_rate']):
                st.markdown(f"##### 3. ä¸»é¢˜æ¥æ”¶æ„æˆ (Top {ANALYSIS_TOP_N} æŒ‰æ¥æ”¶ç‡æ’åº)")
                df_sorted_accept = df.dropna(subset=['acceptance_rate']).sort_values(by='acceptance_rate', ascending=False).head(ANALYSIS_TOP_N)
                if not df_sorted_accept.empty:
                    df_plot = df_sorted_accept.set_index('Topic_Name')[present_decision_cols]
                    # æ£€æŸ¥æ˜¯å¦æœ‰éé›¶æ•°æ®è¡Œ
                    df_plot = df_plot.loc[df_plot.sum(axis=1) > 0] # ç§»é™¤å…¨é›¶è¡Œé¿å…é™¤é›¶é”™è¯¯
                    if not df_plot.empty:
                        df_plot_normalized = df_plot.apply(lambda x: x / x.sum(), axis=1) # å½’ä¸€åŒ–
                        fig_stack = px.bar(df_plot_normalized, y=df_plot_normalized.index, x=df_plot_normalized.columns,
                                           orientation='h', title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜å†³ç­–æ„æˆ (æŒ‰æ¥æ”¶ç‡æ’åº)',
                                           labels={'value': 'è®ºæ–‡æ¯”ä¾‹', 'variable': 'å†³ç­–ç±»å‹', 'y': 'ä¸»é¢˜'},
                                           height=max(600, len(df_plot_normalized)*25), text_auto='.1%')
                        fig_stack.update_layout(yaxis={'categoryorder':'total ascending'}, xaxis_tickformat=".0%", legend_title_text='å†³ç­–ç±»å‹')
                        fig_stack.update_traces(hovertemplate='<b>%{y}</b><br>%{variable}: %{x:.1%}<extra></extra>') # æ”¹è¿›æ‚¬åœ
                        st.plotly_chart(fig_stack, use_container_width=True)
                    else:
                        st.caption("æœªèƒ½ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šç­›é€‰æ‰å…¨é›¶æ•°æ®åä¸ºç©ºã€‚")

                else: st.caption("æœªèƒ½ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šæŒ‰æ¥å—ç‡ç­›é€‰/æ’åºåæ•°æ®ä¸ºç©ºã€‚")
            elif not present_decision_cols: st.caption(f"æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å†³ç­–åˆ— (å¦‚ Oral, Poster ç­‰)ã€‚")
            else: st.caption(f"æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘ 'Topic_Name' æˆ– 'acceptance_rate' åˆ—ã€‚")

        # <<< --- B. å¦‚æœæ˜¯è·¨å¹´è¶‹åŠ¿æ–‡ä»¶ (trends) --- >>>
        elif csv_type == "trends":
            st.subheader("ğŸ“ˆ è·¨å¹´ä»½è¶‹åŠ¿å›¾")
            # å‡è®¾ trends CSV çš„ç»“æ„ï¼šindex=å¹´ä»½, columns=ä¸»é¢˜åç§°, values=è®ºæ–‡æ•°
            try:
                # ç¡®ä¿å¹´ä»½æ˜¯ç´¢å¼•
                if 'year' in df.columns: df = df.set_index('year')
                elif df.index.name != 'year': # å°è¯•å‡è®¾ç¬¬ä¸€åˆ—æ˜¯å¹´ä»½ç´¢å¼•
                     potential_year_col = df.columns[0]
                     if pd.api.types.is_numeric_dtype(df[potential_year_col]):
                          df = df.set_index(potential_year_col)
                          df.index.name = 'year'

                if df.index.name == 'year' and len(df.columns) > 0:
                    # é€‰æ‹© Top N ä¸»é¢˜è¿›è¡Œå±•ç¤º
                    top_topics = df.sum().nlargest(15).index # æ˜¾ç¤º Top 15 è¶‹åŠ¿
                    df_top = df[top_topics]
                    fig_line = px.line(df_top, x=df_top.index, y=df_top.columns, markers=True,
                                       title=f'{selected_conf} - Top 15 ä¸»é¢˜å†å¹´è®ºæ–‡æ•°è¶‹åŠ¿',
                                       labels={'year': 'å¹´ä»½', 'value': 'è®ºæ–‡æ•°é‡', 'variable': 'ä¸»é¢˜'})
                    fig_line.update_layout(xaxis_type='category') # ç¡®ä¿å¹´ä»½æŒ‰ç±»åˆ«æ˜¾ç¤º
                    st.plotly_chart(fig_line, use_container_width=True)
                else:
                    st.warning(f"æ— æ³•ä¸º `{csv_path.name}` ç”Ÿæˆè¶‹åŠ¿æŠ˜çº¿å›¾ã€‚è¯·ç¡®ä¿ CSV æ ¼å¼æ­£ç¡®ï¼ˆä¾‹å¦‚ï¼Œå¹´ä»½ä½œä¸ºç´¢å¼•æˆ–åä¸º 'year' çš„åˆ—ï¼Œå…¶ä»–åˆ—ä¸ºä¸»é¢˜åç§°ï¼‰ã€‚")

            except Exception as trend_e:
                st.error(f"ç”Ÿæˆè·¨å¹´è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {trend_e}")
                st.error(traceback.format_exc())

        # <<< --- C. å¦‚æœæ˜¯åŸå§‹æ•°æ®æ–‡ä»¶ (raw_data) æˆ–å…¶ä»– --- >>>
        else:
            st.subheader("ğŸ“„ åŸå§‹æ•°æ®é¢„è§ˆ")
            st.info("æ£€æµ‹åˆ°è¿™æ˜¯ä¸€ä¸ªåŸå§‹æ•°æ®æ–‡ä»¶æˆ–æ— æ³•è¯†åˆ«çš„åˆ†ææ–‡ä»¶ï¼Œä»…æä¾›æ•°æ®è¡¨æ ¼é¢„è§ˆã€‚")
            st.dataframe(df, height=500, use_container_width=True)


        # --- æ˜¾ç¤º/ä¸‹è½½åŸå§‹æ•°æ®è¡¨æ ¼ (é€‚ç”¨äºæ‰€æœ‰ç±»å‹) ---
        st.markdown("---")
        with st.expander("æŸ¥çœ‹/ä¸‹è½½å½“å‰ CSV æ•°æ®è¡¨æ ¼"):
            st.dataframe(df, height=300, use_container_width=True)
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½æ•°æ® {csv_path.name}",
                data=df.to_csv(index=False).encode('utf-8-sig'),
                file_name=csv_path.name,
                mime='text/csv',
                key=f"download_{csv_path.stem}" # æ·»åŠ å”¯ä¸€ key
            )

    except pd.errors.EmptyDataError:
        st.warning(f"æ–‡ä»¶ {csv_path.name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
    except Exception as e:
        st.error(f"å¤„ç† CSV æ–‡ä»¶ {csv_path.name} æ—¶å¤±è´¥: {e}")
        st.error(traceback.format_exc()) # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯

# -----------------------------------------------------------------
# 7. ä¸»åº”ç”¨é€»è¾‘ï¼šä¾§è¾¹æ å¯¼èˆª (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
st.sidebar.title("PubCrawler Pro ğŸš€"); st.sidebar.caption("v1.6 - Smart CSV Analysis")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½é¡µé¢", ["ğŸ¤– AI åŠ©æ‰‹ & æœç´¢", "ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜"], key="page_selection")
st.sidebar.divider()
analysis_data_sidebar, conf_list_sidebar, _ = find_analysis_files()
if conf_list_sidebar:
    with st.sidebar.expander("ç›®å‰å·²ç´¢å¼•çš„ä¼šè®®", expanded=True):
        st.dataframe(conf_list_sidebar, use_container_width=True, hide_index=True, column_config={"value": "ä¼šè®®åç§°"})
if page == "ğŸ¤– AI åŠ©æ‰‹ & æœç´¢": render_search_and_chat_page()
elif page == "ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜": render_analysis_dashboard_page()



==================== End of: streamlit_app.py ====================



==================== Start of: configs\tasks.yaml ====================

# FILE: configs/tasks.yaml

# ==============================================================================
# PubCrawler Task Configuration v11.0 (Refactored)
# ==============================================================================

# ------------------------------------------------------------------------------
# 1. DATA SOURCE DEFINITIONS ("The Encyclopedia")
# ------------------------------------------------------------------------------
source_definitions:
  # OpenReview å®šä¹‰
  openreview:
    ICLR: { venue_id: "ICLR.cc/YYYY/Conference", api_v1_years: [2019, 2020, 2021, 2022, 2023] }
    NeurIPS: { venue_id: "NeurIPS.cc/YYYY/Conference", api_v1_years: [2019, 2020, 2021, 2022] }

  # HTML å®šä¹‰
  html_cvf:
    CVPR: "https://openaccess.thecvf.com/CVPRYYYY?day=all"
    ICCV: "https://openaccess.thecvf.com/ICCVYYYY?day=all"
  html_pmlr:
    ICML: "https://proceedings.mlr.press/"
  html_acl:
    ACL: "https://aclanthology.org/volumes/YYYY.acl-long/"
    EMNLP: "https://aclanthology.org/volumes/YYYY.emnlp-main/"
    NAACL: { pattern_map: { 2019: "2019.naacl-main", 2021: "2021.naacl-main", 2022: "2022.naacl-main", 2024: "2024.naacl-long" } }

  # å…¶ä»–å®šä¹‰
  selenium:
    AAAI: "https://aaai.org/aaai-publications/aaai-conference-proceedings/"
    KDD: "https://dl.acm.org/conference/kdd/proceedings"
  arxiv:
    API: "http://export.arxiv.org/api/query?"


# ------------------------------------------------------------------------------
# 2. TASKS TO EXECUTE ("The Battle Plan")
# ------------------------------------------------------------------------------
tasks:

  # === TPAMI: æµ‹è¯•æœ€æ–°çš„ 5 ç¯‡ Early Access è®ºæ–‡ ===
#  - name: 'TPAMI_Latest_Test'
#    conference: 'TPAMI'
#    year: 2025
#    source_type: 'tpami'
#    enabled: true
#    punumber: '34' #


  # === ICLR: 2022 - 2026 ===

#  - name: 'ICLR_2026'
#    conference: 'ICLR'
#    year: 2026
#    source_type: 'iclr'
#    enabled: true
#    fetch_reviews: false
#    download_pdfs: false


  - name: 'ICLR_2025'
    conference: 'ICLR'
    year: 2025
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2024'
    conference: 'ICLR'
    year: 2024
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2023'
    conference: 'ICLR'
    year: 2023
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'ICLR_2022'
    conference: 'ICLR'
    year: 2022
    source_type: 'iclr'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  # === NeurIPS: 2022 - 2026 ===

  - name: 'NeurIPS_2026'
    conference: 'NeurIPS'
    year: 2026
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2025'
    conference: 'NeurIPS'
    year: 2025
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2024'
    conference: 'NeurIPS'
    year: 2024
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2023'
    conference: 'NeurIPS'
    year: 2023
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  - name: 'NeurIPS_2022'
    conference: 'NeurIPS'
    year: 2022
    source_type: 'neurips'
    enabled: true
    fetch_reviews: false
    download_pdfs: false


  # === ICML: 2022 - 2026 ===

  - name: 'ICML_2026'
    conference: 'ICML'
    year: 2026
    source_type: 'icml'
    enabled: true
    download_pdfs: false


  - name: 'ICML_2025'
    conference: 'ICML'
    year: 2025
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v237/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2024'
    conference: 'ICML'
    year: 2024
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v235/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2023'
    conference: 'ICML'
    year: 2023
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v202/"
    enabled: true
    download_pdfs: false


  - name: 'ICML_2022'
    conference: 'ICML'
    year: 2022
    source_type: 'icml'
    url_override: "https://proceedings.mlr.press/v162/"
    enabled: true
    download_pdfs: false
    
  # === ACL: 2022 - 2026 ===
  # å¯¹äº ACL, EMNLP, NAACL ç­‰ç½‘ç«™ï¼Œç”±äºå…¶ç»“æ„ç‰¹æ®Šï¼Œéœ€è¦é€ä¸€è®¿é—®è®ºæ–‡è¯¦æƒ…é¡µï¼Œ
  # é€Ÿåº¦è¾ƒæ…¢ã€‚æˆ‘ä»¬ä¸ºæ­¤ç±»ä»»åŠ¡å¼•å…¥äº†å¹¶å‘çˆ¬å–ä¼˜åŒ–ã€‚
#
#  - name: 'ACL_2026'
#    conference: 'ACL'
#    year: 2026
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- å¹¶å‘ä¼˜åŒ–å‚æ•° ---
#    # å¹¶å‘çº¿ç¨‹æ•° (æ¨è 8-32)ã€‚ä»…å¯¹ ACL, CVF ç­‰éœ€è¦é€é¡µæŠ“å–çš„çˆ¬è™«æœ‰æ•ˆã€‚
#    max_workers: 24
#    # è¯¥ä»»åŠ¡æœ€å¤šçˆ¬å–çš„è®ºæ–‡æ•°é‡ä¸Šé™ã€‚è®¾ç½®ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼Œçˆ¬å–æ‰€æœ‰æ‰¾åˆ°çš„è®ºæ–‡ã€‚
#    max_papers_limit: 100
#
#
#  - name: 'ACL_2025'
#    conference: 'ACL'
#    year: 2025
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- å¹¶å‘ä¼˜åŒ–å‚æ•° ---
#    max_workers: 24
#    max_papers_limit: 100 # 0 è¡¨ç¤ºä¸é™åˆ¶
#
#
#  - name: 'ACL_2024'
#    conference: 'ACL'
#    year: 2024
#    source_type: 'acl'
#    url_override: "https://aclanthology.org/volumes/2024.acl-long/"
#    enabled: true
#    download_pdfs: false
#    # --- å¹¶å‘ä¼˜åŒ–å‚æ•° ---
#    max_workers: 24
#    # ç¤ºä¾‹ï¼šå¦‚æœåªæƒ³çˆ¬å–å‰500ç¯‡ï¼Œå¯ä»¥è®¾ç½®ä¸º 500
#    max_papers_limit: 500
#
#
#  - name: 'ACL_2023'
#    conference: 'ACL'
#    year: 2023
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- å¹¶å‘ä¼˜åŒ–å‚æ•° ---
#    max_workers: 24
#    max_papers_limit: 100
#
#
#  - name: 'ACL_2022'
#    conference: 'ACL'
#    year: 2022
#    source_type: 'acl'
#    enabled: true
#    download_pdfs: false
#    # --- å¹¶å‘ä¼˜åŒ–å‚æ•° ---
#    max_workers: 24
#    max_papers_limit: 100
#
#
#  # === CVF (CVPR, ICCV): 2022 - 2026 ===
#  # CVF æ——ä¸‹ä¼šè®®ç½‘ç«™ä¸ ACL ç»“æ„ç±»ä¼¼ï¼ŒåŒæ ·å—ç›Šäºå¹¶å‘çˆ¬å–ä¼˜åŒ–ã€‚
#
#  - name: 'CVPR_2026'
#    conference: 'CVPR'
#    year: 2026
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2025'
#    conference: 'CVPR'
#    year: 2025
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2024'
#    conference: 'CVPR'
#    year: 2024
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2023'
#    conference: 'CVPR'
#    year: 2023
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'CVPR_2022'
#    conference: 'CVPR'
#    year: 2022
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'ICCV_2025' # ICCV ä¸ºå¥‡æ•°å¹´ä¸¾åŠ
#    conference: 'ICCV'
#    year: 2025
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100
#
#  - name: 'ICCV_2023' # ICCV ä¸ºå¥‡æ•°å¹´ä¸¾åŠ
#    conference: 'ICCV'
#    year: 2023
#    source_type: 'cvf'
#    enabled: true
#    download_pdfs: false
#    max_workers: 24
#    max_papers_limit: 100


==================== End of: configs\tasks.yaml ====================



==================== Start of: configs\trends.yaml ====================

# FILE: configs/trends.yaml ("Flagship Edition")

# å®šä¹‰AIç ”ç©¶é¢†åŸŸåŠå…¶å­æ–¹å‘çš„å…³é”®è¯ã€‚è¿™æ˜¯ä¸€ä¸ªå…¨é¢ã€å±‚çº§åŒ–ã€ä¸æ—¶ä¿±è¿›çš„çŸ¥è¯†åº“ã€‚
# å…³é”®è¯ä¸åŒºåˆ†å¤§å°å†™ï¼Œå¹¶ç»è¿‡ä¼˜åŒ–ä»¥æé«˜åŒ¹é…å‡†ç¡®ç‡ã€‚

# --- å¤§è¯­è¨€æ¨¡å‹ä¸åŸºç¡€æ¨¡å‹ (LLMs & Foundation Models) ---
"LLMs & Foundation Models":
  keywords: ["language model", "foundation model", "llm", "large model"]
  sub_fields:
    "LLM Alignment & RLHF/DPO": ["alignment", "rlhf", "dpo", "instruction tuning", "human feedback", "constitutional ai", "preference optimization"]
    "LLM Evaluation": ["llm evaluation", "benchmark", "hallucination", "llm robustness", "truthfulness"]
    "LLM Reasoning & Planning": ["reasoning", "chain-of-thought", "tree-of-thought", "self-consistency", "planning"]
    "LLM-Based Agents": ["llm agent", "tool use", "toolformer", "react"]
    "Parameter-Efficient Fine-tuning (PEFT)": ["parameter-efficient", "peft", "lora", "qlora", "adapter tuning", "soft prompts"]
    "Retrieval-Augmented Generation (RAG)": ["retrieval-augmented", "rag", "in-context learning", "knowledge retrieval"]
    "Mixture of Experts (MoE)": ["mixture of experts", "moe", "sparse model"]
    "State Space Models (Mamba)": ["state space model", "ssm", "mamba", "s4"]
    "World Models": ["world model", "generative world model", "learning world models"]

# --- å¤šæ¨¡æ€ AI (Multimodal AI) ---
"Multimodal AI":
  keywords: ["multimodal", "multi-modal", "cross-modal"]
  sub_fields:
    "Visual-Language Models (VLM)": ["visual-language", "vlm", "multi-modal llm", "vision-language", "llava", "gpt-4v"]
    "Text-to-Image Generation": ["text-to-image", "dall-e", "stable diffusion", "midjourney", "image generation"]
    "Video Generation & Editing": ["video generation", "video editing", "text-to-video", "sora", "video synthesis"]
    "Speech & Audio Generation": ["speech synthesis", "text-to-speech", "tts", "audio generation", "voice conversion"]
    "General Multimodality": ["audio-visual", "text-video", "image-audio", "speech recognition"] # æ•æ‰éVLMçš„å¤šæ¨¡æ€ç»„åˆ

# --- è®¡ç®—æœºè§†è§‰ (CV) ---
"Computer Vision":
  keywords: ["image", "vision", "visual", "cnn", "convolutional", "scene"]
  sub_fields:
    "Diffusion Models & Generative Theory": ["diffusion model", "denoising diffusion", "score-based", "generative model"]
    "3D Vision & Gaussian Splatting": ["3d vision", "gaussian splatting", "nerf", "neural radiance", "reconstruction", "point cloud", "view synthesis"]
    "Object Detection & Segmentation": ["object detection", "segmentation", "yolo", "mask r-cnn", "instance segmentation", "panoptic"]
    "Video Understanding": ["video understanding", "action recognition", "video classification", "temporal understanding"]
    "Image Restoration": ["image restoration", "super-resolution", "denoising", "deblurring"]
    "Visual Transformers (ViT)": ["vision transformer", "vit", "visual transformer"]
    "Self-Supervised Learning (CV)": ["self-supervised", "contrastive learning", "simclr", "moco", "byol", "masked image modeling"]

# --- è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ---
# Note: å¾ˆå¤šNLPä»»åŠ¡æ­£è¢«LLMs subsumeï¼Œè¿™é‡Œä¿ç•™æ›´ç»å…¸çš„æˆ–éLLM-centricçš„ä»»åŠ¡
"Natural Language Processing":
  keywords: ["natural language", "nlp", "text", "corpus", "linguistic"]
  sub_fields:
    "Code Generation": ["code generation", "text-to-code", "program synthesis", "alphacode"]
    "Machine Translation": ["machine translation", "nmt", "cross-lingual"]
    "Information Extraction": ["information extraction", "named entity recognition", "ner", "relation extraction"]
    "Summarization": ["summarization", "text summarization", "abstractive", "extractive"]

# --- å¼ºåŒ–å­¦ä¹  (RL) ---
"Reinforcement Learning":
  keywords: ["reinforcement learning", "rl", "q-learning", "reward", "policy", "markov decision"]
  sub_fields:
    "Reinforcement Learning (Algorithms)": ["actor-critic", "a2c", "a3c", "policy gradient", "sac", "ppo", "td3"]
    "Offline & Imitation Learning": ["offline rl", "imitation learning", "behavioral cloning", "inverse rl"]
    "Multi-Agent RL (MARL)": ["multi-agent rl", "marl", "cooperative", "competitive"]
    "Human Motion Generation": ["motion generation", "humanoid", "locomotion", "character animation"]

# --- æœºå™¨å­¦ä¹ æ ¸å¿ƒ (Core ML) ---
"Core Machine Learning":
  keywords: ["learning", "model", "network", "algorithm", "theory"]
  sub_fields:
    "Federated Learning (FL)": ["federated learning", "fl", "decentralized learning"]
    "Continual Learning": ["continual learning", "lifelong learning", "catastrophic forgetting"]
    "Transfer Learning": ["transfer learning", "domain adaptation", "fine-tuning"]
    "Meta-Learning": ["meta-learning", "learning to learn", "few-shot learning", "maml"]
    "Self-Supervised Learning (General)": ["self-supervised", "ssl", "contrastive learning"] # For non-CV applications
    "Graph Neural Networks (GNN)": ["graph neural network", "gnn", "graph representation", "message passing"]
    "Transformers & Attention": ["transformer", "attention mechanism", "self-attention"] # General, non-visual
    "Causal Discovery & Inference": ["causal discovery", "causal inference", "structural causal model", "scm", "treatment effect"]
    "Optimization Algorithms": ["optimization", "sgd", "adam", "gradient descent", "convergence", "second-order"]
    "Bayesian Methods": ["bayesian", "gaussian process", "variational inference", "probabilistic model"]
    "Quantization & Pruning": ["quantization", "pruning", "model compression", "8-bit", "4-bit", "int8", "binarization"]

# --- AIä¼¦ç†ã€å®‰å…¨ä¸å¯è§£é‡Šæ€§ (Trustworthy AI) ---
"Trustworthy AI":
  keywords: ["trustworthy", "responsible", "ethical"]
  sub_fields:
    "Adversarial Robustness & Attacks": ["adversarial attack", "adversarial robustness", "defense", "adversarial example"]
    "Differential Privacy (DP)": ["differential privacy", "dp-sgd", "privacy-preserving", "private ml"]
    "AI Fairness & Bias": ["fairness", "bias", "algorithmic fairness", "group fairness", "debiasing"]
    "Model Interpretability (XAI)": ["interpretability", "explainable ai", "xai", "shap", "lime", "feature attribution"]
    "LLM Safety & Jailbreaking": ["llm safety", "jailbreaking", "red teaming", "model guardrails"] # LLM-specific safety

# --- AI for Science & Society ---
"AI for Science & Society":
  keywords: ["ai for", "applications", "applied ai"]
  sub_fields:
    "AI for Drug/Molecule Science": ["drug discovery", "molecule generation", "protein folding", "alphafold", "computational biology"]
    "AI for Healthcare": ["healthcare", "medical image", "ecg", "eeg", "patient data", "clinical notes", "radiology"]
    "AI for Weather & Climate": ["weather forecasting", "climate modeling", "physics-informed", "pinn"]
    "Robotics": ["robotics", "robot learning", "manipulation", "control", "embodied ai"]
    "Recommender Systems": ["recommender system", "collaborative filtering", "recommendation"]
    "AI for Chip Design (EDA)": ["chip design", "eda", "electronic design automation", "placement", "routing"]
    "Time Series Forecasting": ["time series", "forecasting", "temporal data", "sequential data"]

# --- æœªæ¥å¯æ‰©å±•çš„â€œç¬¬ä¸‰å±‚çº§â€ç»“æ„ç¤ºä¾‹ (ä»£ç æš‚ä¸æ”¯æŒ) ---
# "Example with Sub-Sub-Fields":
#  keywords: ["example"]
#  sub_fields:
#    "Generative Vision":
#      keywords: ["generative vision"]
#      sub_sub_fields:
#        "GANs": ["gan", "generative adversarial"]
#        "Diffusion Models": ["diffusion", "ddpm"]
#        "VAEs": ["variational autoencoder", "vae"]
#        "Autoregressive Models": ["pixelcnn", "imagen"]

==================== End of: configs\trends.yaml ====================



==================== Start of: src\__init__.py ====================

# FILE: src/__init__.py

# This file makes the 'src' directory a Python package.

# END OF FILE: src/__init__.py

==================== End of: src\__init__.py ====================



==================== Start of: src\ai\glm_chat_service.py ====================

# FILE: src/ai/glm_chat_service.py (CLI AI Interaction Layer - v1.1)

import sys
from typing import List, Dict, Any

# --- ä» search_service å¯¼å…¥å¿…è¦çš„AIç›¸å…³åŠŸèƒ½å’Œé…ç½® ---
from src.search.search_service import (
    generate_ai_response,
    AI_CONTEXT_PAPERS,
    ZHIPUAI_API_KEY,
    Colors,  # é¢œè‰²å®šä¹‰
    _ai_enabled  # æ£€æŸ¥AIæ˜¯å¦å·²åˆå§‹åŒ–æˆåŠŸ
)


# --- å®šä¹‰CLIä¸“å±çš„ print_colored å‡½æ•° ---
# ç¡®ä¿åœ¨AIå¯¹è¯å¾ªç¯ä¸­èƒ½å¤Ÿæ­£ç¡®æ‰“å°å½©è‰²æ–‡æœ¬
def print_colored(text, color, end='\n'):
    if sys.stdout.isatty():
        print(f"{color}{text}{Colors.ENDC}", end=end)
    else:
        print(text, end=end)


def start_ai_chat_session(search_results: List[Dict[str, Any]]):
    """
    å¯åŠ¨ä¸€ä¸ªAIå¯¹è¯ä¼šè¯ï¼Œå¤„ç†CLIä¸­çš„å¤šè½®äº¤äº’ã€‚
    Args:
        search_results (list): å½“å‰æœç´¢ç»“æœçš„è®ºæ–‡åˆ—è¡¨ã€‚
    """
    if not ZHIPUAI_API_KEY:
        print_colored("[!] é”™è¯¯: æœªæ‰¾åˆ° ZHIPUAI_API_KEYã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ã€‚", Colors.FAIL)
        return
    if not _ai_enabled:
        print_colored("[!] é”™è¯¯: AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨å¯¹è¯ã€‚", Colors.FAIL)
        return
    if not search_results:
        print_colored("[!] æ²¡æœ‰å¯ä¾›å¯¹è¯çš„æœç´¢ç»“æœï¼Œè¯·å…ˆæ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢ã€‚", Colors.WARNING)
        return

    print_colored("\n--- ğŸ¤– AI å¯¹è¯æ¨¡å¼ ---", Colors.HEADER)
    print_colored(f"æˆ‘å·²ç»é˜…è¯»äº†ä¸æ‚¨æŸ¥è¯¢æœ€ç›¸å…³çš„ {AI_CONTEXT_PAPERS} ç¯‡è®ºæ–‡ï¼Œè¯·å°±è¿™äº›è®ºæ–‡å‘æˆ‘æé—®ã€‚", Colors.OKCYAN)
    print_colored("è¾“å…¥ 'exit' æˆ– 'quit' ç»“æŸå¯¹è¯ã€‚", Colors.OKCYAN)

    messages = []

    initial_assistant_message = "å¥½çš„ï¼Œæˆ‘å·²ç»ç†è§£äº†è¿™å‡ ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚è¯·é—®æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ"
    print(f"\nAIåŠ©æ‰‹ > {initial_assistant_message}")
    messages.append({"role": "assistant", "content": initial_assistant_message})

    while True:
        try:
            user_question = input(f"\n{Colors.BOLD}æ‚¨çš„é—®é¢˜æ˜¯?{Colors.ENDC} > ").strip()
            if not user_question: continue
            if user_question.lower() in ['exit', 'quit']: break

            messages.append({"role": "user", "content": user_question})

            print_colored("ğŸ¤– GLM-4.5 æ­£åœ¨æ€è€ƒ...", Colors.OKCYAN, end="", flush=True)

            ai_response_content = generate_ai_response(
                chat_history=messages,
                search_results_context=search_results
            )

            print("\r" + " " * 30 + "\r", end="")  # æ¸…é™¤ "æ€è€ƒä¸­..." æç¤º

            if ai_response_content.startswith("[!]"):
                print_colored(f"\nAIåŠ©æ‰‹ > {ai_response_content}", Colors.FAIL)
            else:
                print_colored(f"\nAIåŠ©æ‰‹ > {ai_response_content}")
                messages.append({"role": "assistant", "content": ai_response_content})

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"\n[!] è°ƒç”¨AIæ—¶å‡ºé”™: {e}", Colors.FAIL)
            break

==================== End of: src\ai\glm_chat_service.py ====================



==================== Start of: src\ai\__init__.py ====================

# FILE: src/ai/__init__.py
# Makes 'ai' a Python package.

==================== End of: src\ai\__init__.py ====================



==================== Start of: src\analysis\analyzer.py ====================

# FILE: src/analysis/analyzer.py

import re
import nltk
from wordcloud import WordCloud
from collections import Counter
from pathlib import Path

# --- NLTK Data Check ---
try:
    from nltk.corpus import stopwords

    STOPWORDS = set(stopwords.words('english'))
except LookupError:
    # This block executes if the stopwords data is not found.
    # We guide the user to download it manually for reliability.
    print("-" * 80)
    print("!!! NLTK DATA NOT FOUND !!!")
    print("Required 'stopwords' data package is missing.")
    print("Please run the following command in your terminal once to download it:")
    print("\n    python -m nltk.downloader stopwords\n")
    print("-" * 80)
    # Exit gracefully instead of attempting a download, which can be unreliable.
    exit(1)

# Add custom stopwords relevant to academic papers
CUSTOM_STOPWORDS = {
    'abstract', 'paper', 'introduction', 'method', 'methods', 'results', 'conclusion',
    'propose', 'proposed', 'present', 'presents', 'show', 'demonstrate', 'model', 'models',
    'state', 'art', 'state-of-the-art', 'sota', 'approach', 'novel', 'work', 'based',
    'data', 'dataset', 'datasets', 'training', 'learning', 'network', 'networks',
    'performance', 'task', 'tasks', 'key', 'using', 'use', 'et', 'al', 'figure',
    'table', 'results', 'analysis', 'system', 'systems', 'research', 'deep', 'large',
    'also', 'however', 'framework', 'well', 'effective', 'efficient'
}
ALL_STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)


def clean_text(text: str) -> list:
    """Cleans and tokenizes text, removing stopwords and non-alphanumeric characters."""
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    tokens = text.split()
    return [word for word in tokens if word.isalpha() and word not in ALL_STOPWORDS and len(word) > 2]


def generate_wordcloud_from_papers(papers: list, output_path: Path) -> bool:
    """
    Generates and saves a word cloud image from the titles and abstracts of papers.
    Returns True if successful, False otherwise.
    """
    if not papers:
        return False

    # Combine all titles and abstracts into a single string
    full_text = " ".join([p.get('title', '') + " " + p.get('abstract', '') for p in papers])

    if not full_text.strip():
        print("Warning: No text available to generate word cloud.")
        return False

    word_tokens = clean_text(full_text)

    if not word_tokens:
        print("Warning: No valid words left after cleaning to generate word cloud.")
        return False

    word_freq = Counter(word_tokens)

    try:
        wc = WordCloud(width=1200, height=600, background_color="white", collocations=False).generate_from_frequencies(
            word_freq)
        wc.to_file(str(output_path))
        print(f"Word cloud generated and saved to {output_path}")
        return True
    except Exception as e:
        print(f"Error generating word cloud: {e}")
        return False

# END OF FILE: src/analysis/analyzer.py

==================== End of: src\analysis\analyzer.py ====================



==================== Start of: src\analysis\trends.py ====================

# FILE: src/analysis/trends.py

import yaml
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import matplotlib.ticker as mtick

from src.crawlers.config import ROOT_DIR, get_logger

logger = get_logger(__name__)
TREND_CONFIG_FILE = ROOT_DIR / "configs" / "trends.yaml"
sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['figure.dpi'] = 300


def _load_trend_config():
    if not TREND_CONFIG_FILE.exists():
        logger.error(f"Trend config file not found: {TREND_CONFIG_FILE}")
        return None
    with open(TREND_CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def _classify_paper_subfields(paper: dict, trend_config: dict) -> list:
    text = str(paper.get('title', '')) + ' ' + str(paper.get('abstract', ''))
    if not text.strip(): return []
    text = text.lower()
    matched = set()
    for field, data in trend_config.items():
        if 'sub_fields' not in data: continue
        for sub_field, keywords in data.get('sub_fields', {}).items():
            if not isinstance(keywords, list): continue
            keyword_pattern = r'\b(' + '|'.join(re.escape(k) for k in keywords) + r')\b'
            if re.search(keyword_pattern, text, re.IGNORECASE):
                matched.add(sub_field)
    return list(matched)


def _create_analysis_df(df: pd.DataFrame, trend_config: dict) -> pd.DataFrame:
    df['sub_fields'] = df.apply(lambda row: _classify_paper_subfields(row, trend_config), axis=1)
    df_exploded = df.explode('sub_fields').dropna(subset=['sub_fields'])
    if df_exploded.empty:
        return pd.DataFrame()

    stats = df_exploded.groupby('sub_fields').size().reset_index(name='paper_count')

    if 'avg_rating' in df_exploded.columns and not df_exploded['avg_rating'].isnull().all():
        avg_ratings = df_exploded.groupby('sub_fields')['avg_rating'].mean().reset_index()
        stats = pd.merge(stats, avg_ratings, on='sub_fields', how='left')

    analysis_df = stats

    if 'decision' in df_exploded.columns:
        decisions = df_exploded.groupby(['sub_fields', 'decision']).size().unstack(fill_value=0)
        analysis_df = pd.merge(analysis_df, decisions, on='sub_fields', how='left').fillna(0)

        for dtype in ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']:
            if dtype not in analysis_df.columns:
                analysis_df[dtype] = 0

        accepted = analysis_df.get('Oral', 0) + analysis_df.get('Spotlight', 0) + analysis_df.get('Poster', 0)
        total_decision = accepted + analysis_df.get('Reject', 0)
        analysis_df['acceptance_rate'] = (accepted / total_decision.where(total_decision != 0, np.nan)).fillna(0)

    analysis_df.rename(columns={'sub_fields': 'Topic_Name'}, inplace=True)
    return analysis_df


def _plot_topic_ranking(df, metric, title, path, top_n=40):
    if metric not in df.columns:
        logger.warning(f"Metric '{metric}' not in DataFrame. Skipping plot: {title}")
        return
    df_sorted = df.dropna(subset=[metric]).sort_values(by=metric, ascending=False).head(top_n)
    if df_sorted.empty: return

    # --- æ ¸å¿ƒä¿®å¤ç‚¹: ä¸ºè¿™ä¸ªå‡½æ•°ä¹Ÿæ·»åŠ æœ€å¤§é«˜åº¦é™åˆ¶ ---
    height = min(30, max(10, len(df_sorted) * 0.4))

    plt.figure(figsize=(16, height))
    palette = 'viridis' if metric == 'paper_count' else 'plasma_r'
    sns.barplot(x=metric, y='Topic_Name', data=df_sorted, hue='Topic_Name', palette=palette, legend=False)
    plt.title(title, fontsize=22, pad=20)
    plt.xlabel(metric.replace('_', ' ').title(), fontsize=16)
    plt.ylabel('Topic Name', fontsize=16)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(path)
    plt.close()


def _plot_decision_breakdown(df, title, path, top_n=40):
    if 'acceptance_rate' not in df.columns:
        logger.warning(f"Acceptance rate not available. Skipping plot: {title}")
        return
    df_sorted = df.sort_values(by='acceptance_rate', ascending=False).head(top_n)
    if df_sorted.empty: return
    cols = ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']
    plot_data = df_sorted.set_index('Topic_Name')[[c for c in cols if c in df_sorted.columns]]
    plot_norm = plot_data.div(plot_data.sum(axis=1), axis=0)

    # --- æ ¸å¿ƒä¿®å¤ç‚¹: ç¡®ä¿è¿™ä¸ªå‡½æ•°ä¹Ÿä¿ç•™äº†æœ€å¤§é«˜åº¦é™åˆ¶ ---
    height = min(30, max(12, len(plot_norm) * 0.5))

    fig, ax = plt.subplots(figsize=(20, height))
    plot_norm.plot(kind='barh', stacked=True, colormap='viridis', width=0.85, ax=ax)
    count_map = df_sorted.set_index('Topic_Name')['paper_count']
    for i, name in enumerate(plot_norm.index):
        ax.text(1.01, i, f"n={count_map.get(name, 0)}", va='center', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=24, pad=40)
    ax.set_xlabel('Proportion of Papers', fontsize=16)
    ax.set_ylabel('Topic Name (Sorted by Acceptance Rate)', fontsize=16)
    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))
    ax.set_xlim(0, 1)
    ax.invert_yaxis()
    ax.legend(title='Decision Type', loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=5, frameon=False)
    plt.tight_layout(rect=[0, 0, 0.95, 1])
    plt.savefig(path)
    plt.close()


def _save_summary_table(df, title, path_base, top_n=65):
    if 'acceptance_rate' not in df.columns:
        logger.warning(f"Acceptance rate not available. Skipping summary table: {title}")
        return
    df_sorted = df.sort_values(by='acceptance_rate', ascending=False).head(top_n)
    if df_sorted.empty: return
    cols = ['Topic_Name', 'paper_count', 'avg_rating', 'acceptance_rate', 'Oral', 'Spotlight', 'Poster', 'Reject',
            'N/A']
    final_table = df_sorted[[c for c in cols if c in df_sorted.columns]]
    final_table.to_csv(f"{path_base}.csv", index=False, encoding='utf-8-sig')
    styler = final_table.style.format({'avg_rating': '{:.2f}', 'acceptance_rate': '{:.2%}'}) \
        .bar(subset=['paper_count'], color='#6495ED', align='zero') \
        .bar(subset=['avg_rating'], color='#FFA07A', align='mean') \
        .background_gradient(subset=['acceptance_rate'], cmap='summer_r') \
        .set_caption(title) \
        .set_table_styles([{'selector': 'th, td', 'props': [('text-align', 'center')]}])
    with open(f"{path_base}.html", 'w', encoding='utf-8') as f:
        f.write(styler.to_html())


def _plot_cross_year_trends(df, title, path):
    df_exploded = df.explode('sub_fields').dropna(subset=['sub_fields'])
    if df_exploded.empty or df_exploded['year'].nunique() < 2:
        logger.warning(f"Skipping cross-year trend plot for '{title}': requires data from at least 2 years.")
        return
    pivot = df_exploded.groupby(['year', 'sub_fields']).size().unstack(fill_value=0)
    top_sub_fields = pivot.sum().nlargest(12).index
    pivot = pivot[top_sub_fields]
    pivot_percent = pivot.div(pivot.sum(axis=1), axis=0) * 100
    pivot_percent.sort_index(inplace=True)
    plt.figure(figsize=(16, 9))
    plt.stackplot(pivot_percent.index, pivot_percent.T.values, labels=pivot_percent.columns, alpha=0.8)
    plt.title(title, fontsize=22, weight='bold')
    plt.xlabel('Year', fontsize=16)
    plt.ylabel('Percentage of Papers (%)', fontsize=16)
    plt.xticks(pivot_percent.index.astype(int))
    plt.legend(title='Top Sub-Fields', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout(rect=[0, 0, 0.82, 1])
    plt.savefig(path)
    plt.close()


def run_single_task_analysis(papers: list, task_name: str, output_dir: Path):
    trend_config = _load_trend_config()
    if not trend_config or not papers: return

    df = pd.DataFrame(papers)
    analysis_df = _create_analysis_df(df, trend_config)
    if analysis_df.empty:
        logger.warning(f"No topics matched for {task_name}, skipping analysis plots.")
        return

    _plot_topic_ranking(analysis_df, 'paper_count', f"Topic Hotness at {task_name}", output_dir / "1_topic_hotness.png")

    has_review_data = 'avg_rating' in analysis_df.columns and 'acceptance_rate' in analysis_df.columns
    if has_review_data:
        _plot_topic_ranking(analysis_df, 'avg_rating', f"Topic Quality at {task_name}",
                            output_dir / "2_topic_quality.png")
        _plot_decision_breakdown(analysis_df, f"Decision Breakdown at {task_name}",
                                 output_dir / "3_decision_breakdown.png")
        _save_summary_table(analysis_df, f"Summary Table for {task_name}", output_dir / "4_summary_table")
    else:
        # è¿™ä¸ªæ—¥å¿—åªä¼šåœ¨æ²¡æœ‰å®¡ç¨¿æ•°æ®çš„ä»»åŠ¡ä¸­æ‰“å°ï¼Œæ˜¯æ­£å¸¸çš„
        logger.info(f"Skipping review-based analysis for {task_name}: missing review data.")

    logger.info(f"Single-task analysis for {task_name} completed.")


def run_cross_year_analysis(papers: list, conference_name: str, output_dir: Path):
    trend_config = _load_trend_config()
    if not trend_config or not papers: return

    df = pd.DataFrame(papers)
    if 'year' not in df.columns or df['year'].isnull().all():
        logger.warning(f"Skipping cross-year analysis for {conference_name}: 'year' column not found or is empty.")
        return

    df['sub_fields'] = df.apply(lambda row: _classify_paper_subfields(row, trend_config), axis=1)

    _plot_cross_year_trends(
        df,
        f"Sub-Field Trends at {conference_name} Over Time",
        output_dir / f"trends_{conference_name}.png"
    )
    logger.info(f"Cross-year analysis for {conference_name} completed.")

==================== End of: src\analysis\trends.py ====================



==================== Start of: src\analysis\__init__.py ====================

# FILE: src/analysis/__init__.py

# This file makes the 'analysis' directory a Python package.

# END OF FILE: src/analysis/__init__.py


==================== End of: src\analysis\__init__.py ====================



==================== Start of: src\api\main.py ====================

# FILE: src/api/main.py (FastAPI Backend for PubCrawler)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import sys

# --- ä» search_service å¯¼å…¥æ ¸å¿ƒé€»è¾‘å’Œåˆå§‹åŒ–å‡½æ•° ---
from src.search.search_service import (
    keyword_search,
    semantic_search,
    get_stats_summary,
    generate_ai_response,
    _initialize_search_components,
    _sqlite_conn  # ç”¨äºFastAPIå…³é—­æ—¶å…³é—­è¿æ¥
)
from src.search.search_service import ZHIPUAI_API_KEY  # å¯¼å…¥API Keyï¼Œç”¨äºæ£€æŸ¥AIå¯ç”¨æ€§

# --- FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="PubCrawler AI Assistant API",
    description="ä¸ºAIå­¦æœ¯ç ”ç©¶åŠ©æ‰‹æä¾›æœç´¢ã€ç»Ÿè®¡å’ŒAIå¯¹è¯åŠŸèƒ½ã€‚",
    version="1.0.0",
)


# --- Pydantic æ¨¡å‹ç”¨äºè¯·æ±‚ä½“å’Œå“åº” ---
class SearchQuery(BaseModel):
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä»¥ 'sem:' å¼€å¤´è¡¨ç¤ºè¯­ä¹‰æœç´¢ï¼Œå¦åˆ™ä¸ºå…³é”®è¯æœç´¢ã€‚")
    top_n: int = Field(20, description="è¯­ä¹‰æœç´¢æ—¶è¿”å›çš„æœ€ç›¸å…³è®ºæ–‡æ•°é‡ã€‚", ge=1, le=100)


class SearchResultPaper(BaseModel):
    title: str
    authors: str
    abstract: str
    conference: str
    year: str
    similarity: Optional[float] = None  # è¯­ä¹‰æœç´¢ç»“æœå¯èƒ½åŒ…å«ç›¸ä¼¼åº¦


class SearchStats(BaseModel):
    total_found: int
    distribution: Dict[str, int]


class SearchResponse(BaseModel):
    results: List[SearchResultPaper]
    stats: SearchStats
    message: str = "æœç´¢æˆåŠŸã€‚"


class AIChatMessage(BaseModel):
    role: str
    content: str


class AIChatRequest(BaseModel):
    chat_history: List[AIChatMessage] = Field(..., description="ä¹‹å‰çš„èŠå¤©å†å²ï¼Œä¸åŒ…å«å½“å‰ç”¨æˆ·æ¶ˆæ¯ã€‚")
    current_message: str = Field(..., description="ç”¨æˆ·å½“å‰çš„æœ€æ–°æ¶ˆæ¯ã€‚")
    search_results_context: List[SearchResultPaper] = Field(..., description="æä¾›ç»™AIä½œä¸ºä¸Šä¸‹æ–‡çš„æœç´¢ç»“æœè®ºæ–‡åˆ—è¡¨ã€‚")


class AIChatResponse(BaseModel):
    response: str
    message: str = "AIå“åº”æˆåŠŸã€‚"


# --- ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ (å¯åŠ¨æ—¶åˆå§‹åŒ–ç»„ä»¶ï¼Œå…³é—­æ—¶æ¸…ç†èµ„æº) ---
@app.on_event("startup")
async def startup_event():
    print("[*] FastAPIåº”ç”¨å¯åŠ¨ä¸­ï¼Œæ­£åœ¨åˆå§‹åŒ–æœç´¢ç»„ä»¶...")
    try:
        _initialize_search_components()
        print("[âœ”] æœç´¢ç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")
    except Exception as e:
        print(f"[âœ–] æœç´¢ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
        # è¿™é‡Œå¯ä»¥é€‰æ‹©ç»ˆæ­¢åº”ç”¨å¯åŠ¨ï¼Œæˆ–è€…åœ¨åç»­APIè°ƒç”¨ä¸­å¤„ç†é”™è¯¯


@app.on_event("shutdown")
async def shutdown_event():
    print("[*] FastAPIåº”ç”¨å…³é—­ä¸­ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
    if _sqlite_conn:
        _sqlite_conn.close()
        print("[âœ”] SQLiteè¿æ¥å·²å…³é—­ã€‚")


# --- API è·¯ç”± ---
@app.post("/search", response_model=SearchResponse)
async def perform_search(search_query: SearchQuery):
    """
    æ‰§è¡Œå…³é”®è¯æˆ–è¯­ä¹‰æœç´¢ï¼Œå¹¶è¿”å›è®ºæ–‡åˆ—è¡¨åŠç»Ÿè®¡ä¿¡æ¯ã€‚
    - ä»¥ 'sem:' å¼€å¤´çš„æŸ¥è¯¢å­—ç¬¦ä¸²å°†è§¦å‘è¯­ä¹‰æœç´¢ã€‚
    - å…¶ä»–æŸ¥è¯¢å­—ç¬¦ä¸²å°†è§¦å‘å…³é”®è¯æœç´¢ã€‚
    """
    query_text = search_query.query.strip()

    if query_text.lower().startswith('sem:'):
        actual_query = query_text[4:].strip()
        if not actual_query:
            raise HTTPException(status_code=400, detail="è¯­ä¹‰æœç´¢æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©ºã€‚")
        results = semantic_search(actual_query, top_n=search_query.top_n)
    else:
        results = keyword_search(query_text)

    if not results:
        return SearchResponse(results=[], stats={"total_found": 0, "distribution": {}}, message="æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")

    stats_summary = get_stats_summary(results)
    return SearchResponse(results=results, stats=stats_summary, message="æœç´¢æˆåŠŸã€‚")


@app.post("/chat", response_model=AIChatResponse)
async def chat_with_ai(chat_request: AIChatRequest):
    """
    ä¸AIåŠ©æ‰‹è¿›è¡Œå¯¹è¯ï¼Œæä¾›èŠå¤©å†å²å’Œæœç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ã€‚
    """
    if not ZHIPUAI_API_KEY:
        raise HTTPException(status_code=503, detail="æ™ºè°±AI API Keyæœªé…ç½®ï¼ŒAIæœåŠ¡ä¸å¯ç”¨ã€‚")

    if not chat_request.search_results_context:
        raise HTTPException(status_code=400, detail="æœªæä¾›æœç´¢ç»“æœä¸Šä¸‹æ–‡ï¼ŒAIæ— æ³•è¿›è¡Œå¯¹è¯ã€‚")

    # æ„é€ å®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·èƒŒæ™¯æ¶ˆæ¯ï¼Œç„¶åä¼ å…¥AIæœåŠ¡
    # generate_ai_response å‡½æ•°ä¼šå¤„ç†ç³»ç»Ÿæ¶ˆæ¯å’ŒèƒŒæ™¯çŸ¥è¯†çš„æ³¨å…¥

    # chat_history åŒ…å«ä¹‹å‰çš„å¯¹è¯ï¼Œcurrent_message æ˜¯ç”¨æˆ·æœ€æ–°è¾“å…¥
    full_chat_history_for_service = chat_request.chat_history + [
        {"role": "user", "content": chat_request.current_message}]

    ai_response_text = generate_ai_response(
        chat_history=full_chat_history_for_service,
        search_results_context=[p.dict() for p in chat_request.search_results_context]  # ç¡®ä¿ä¼ é€’çš„æ˜¯å­—å…¸åˆ—è¡¨
    )

    if ai_response_text.startswith("[!]"):
        raise HTTPException(status_code=500, detail=ai_response_text)

    return AIChatResponse(response=ai_response_text)

==================== End of: src\api\main.py ====================



==================== Start of: src\api\__init__.py ====================

# FILE: src/api/__init__.py
# Makes 'api' a Python package.

==================== End of: src\api\__init__.py ====================



==================== Start of: src\crawlers\config.py ====================

# FILE: src/config.py (Structured Logging)

import logging
from pathlib import Path

# å¯¼å…¥ Tqdm æ—¥å¿—å¤„ç†å™¨å’Œå½©è‰²æ ¼å¼åŒ–å™¨
from src.utils.tqdm_logger import TqdmLoggingHandler
from src.utils.console_logger import ColoredFormatter, COLORS

# --- Project Structure ---
# å¤šåŠ ä¸€ä¸ª .parent å°±èƒ½æ­£ç¡®åœ°å›åˆ°é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent.parent

OUTPUT_DIR = ROOT_DIR / "output"
LOG_DIR = ROOT_DIR / "logs" # <-- ç°åœ¨è¿™ä¸ª logs è·¯å¾„ä¹Ÿæ­£ç¡®äº†
CONFIG_FILE = ROOT_DIR / "configs" / "tasks.yaml" # <-- ç°åœ¨è¿™ä¸ª configs è·¯å¾„ä¹Ÿæ­£ç¡®äº†

METADATA_OUTPUT_DIR = OUTPUT_DIR / "metadata"
PDF_DOWNLOAD_DIR = OUTPUT_DIR / "pdfs"
TRENDS_OUTPUT_DIR = OUTPUT_DIR / "trends"

# --- Create Directories ---
# è¿™éƒ¨åˆ†ä»£ç ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†
OUTPUT_DIR.mkdir(exist_ok=True)
LOG_DIR.mkdir(exist_ok=True)


# --- Logging Configuration (æ ¸å¿ƒä¿®æ”¹ç‚¹) ---
def get_logger(name: str, log_file: Path = LOG_DIR / "pubcrawler.log") -> logging.Logger:
    """
    é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ã€‚
    - æ§åˆ¶å°è¾“å‡º: ç®€æ´ã€å½©è‰²ã€ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚
    - æ–‡ä»¶è¾“å‡º: åŒ…å«å®Œæ•´ Traceback çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•ã€‚
    """
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)

        # 1. æ§åˆ¶å°å¤„ç†å™¨ (ä½¿ç”¨ Tqdm å®‰å…¨å¤„ç†å™¨å’Œæ–°çš„ç»“æ„åŒ–å½©è‰²æ ¼å¼)
        tqdm_handler = TqdmLoggingHandler()
        tqdm_handler.setLevel(logging.INFO)
        # --- æ–°çš„ç»“æ„åŒ–æ ¼å¼ ---
        # %(levelname)s ä¼šè¢« ColoredFormatter è½¬æ¢æˆå¸¦é¢œè‰²çš„æ ‡è¯†
        console_format = f"{COLORS['STEP']}[%(levelname)s]{COLORS['RESET']} %(message)s"
        console_formatter = ColoredFormatter(console_format)
        tqdm_handler.setFormatter(console_formatter)

        # 2. æ–‡ä»¶å¤„ç†å™¨ (ä¿æŒä¸å˜ï¼Œç”¨äºè®°å½•å…¨éƒ¨ç»†èŠ‚)
        file_handler = logging.FileHandler(log_file, 'a', encoding='utf-8') # ä½¿ç”¨ 'a' æ¨¡å¼è¿½åŠ æ—¥å¿—
        file_handler.setLevel(logging.INFO)
        file_format = '%(asctime)s - %(name)s - [%(levelname)s] - %(message)s'
        file_formatter = logging.Formatter(file_format)
        file_handler.setFormatter(file_formatter)

        logger.addHandler(tqdm_handler)
        logger.addHandler(file_handler)

        logger.propagate = False # é˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ° root logger

    return logger

==================== End of: src\crawlers\config.py ====================



==================== Start of: src\crawlers\models.py ====================

# FILE: src/models.py

from dataclasses import dataclass, field
from typing import List, Optional


@dataclass
class Paper:
    """
    ä¸€ä¸ªç”¨äºå­˜å‚¨è®ºæ–‡ä¿¡æ¯çš„æ•°æ®ç±»ï¼Œç¡®ä¿æ‰€æœ‰ scraper è¿”å›ç»Ÿä¸€çš„ç»“æ„ã€‚
    """
    id: str
    title: str
    authors: List[str]
    summary: str
    published_date: str
    updated_date: str

    pdf_url: Optional[str] = None
    categories: List[str] = field(default_factory=list)
    primary_category: Optional[str] = None

    # å‘è¡¨ä¿¡æ¯
    journal_ref: Optional[str] = None
    doi: Optional[str] = None

    # é¢å¤–å¤‡æ³¨ï¼Œä¾‹å¦‚é¡¹ç›®ä¸»é¡µ
    comment: Optional[str] = None

    # ä½œè€…åŠå…¶å•ä½çš„è¯¦ç»†ä¿¡æ¯
    author_details: List[str] = field(default_factory=list)

# END OF FILE: src/models.py

==================== End of: src\crawlers\models.py ====================



==================== Start of: src\crawlers\processor.py ====================

# FILE: src/processor.py

import logging
import os
import requests
import zipfile
import re
from typing import Iterator, Dict, Any
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class Processor:
    """
    Processes a stream of paper data to generate output files.
    - A summary.txt file for LLM analysis.
    - A compressed .zip file containing all downloaded PDFs.
    """

    def __init__(self, output_dir: str = 'output', download_pdfs: bool = False):
        self.output_dir = output_dir
        self.download_pdfs = download_pdfs
        self.summary_path = os.path.join(self.output_dir, 'summary.txt')
        self.zip_path = os.path.join(self.output_dir, 'papers.zip')

        # Ensure output directory exists
        os.makedirs(self.output_dir, exist_ok=True)

    def _sanitize_filename(self, title: str) -> str:
        """Creates a safe filename from a paper title."""
        # Remove invalid characters
        sanitized = re.sub(r'[\\/*?:"<>|]', "", title)
        # Truncate to a reasonable length
        return (sanitized[:100] + '.pdf') if len(sanitized) > 100 else (sanitized + '.pdf')

    def _format_summary_entry(self, paper_data: Dict[str, Any]) -> str:
        """Formats a single paper's data into the specified text format."""
        # Safely get all required fields
        title = paper_data.get('title', 'N/A')
        authors = ", ".join(paper_data.get('authors', []))
        conference = paper_data.get('conference', 'N/A')
        year = paper_data.get('year', 'N/A')
        source_url = paper_data.get('source_url', 'N/A')
        pdf_link = paper_data.get('pdf_link', 'N/A')
        abstract = paper_data.get('abstract', 'No abstract available.')
        reviews = paper_data.get('reviews', [])

        # Build the entry string
        entry = []
        entry.append("=" * 80)
        entry.append(f"Title: {title}")
        entry.append(f"Authors: {authors}")
        entry.append(f"Conference: {conference} {year}")
        entry.append(f"Source URL: {source_url}")
        entry.append(f"PDF Link: {pdf_link}")
        entry.append("\n--- Abstract ---")
        entry.append(abstract)

        if reviews:
            entry.append(f"\n--- Reviews ({len(reviews)}) ---")
            for i, review in enumerate(reviews, 1):
                review_title = review.get('title', 'N/A')
                review_comment = review.get('comment', 'No comment.')
                review_decision = review.get('decision', None)
                review_rating = review.get('rating', None)

                entry.append(f"\n[Review {i}]")
                entry.append(f"Title: {review_title}")
                if review_decision:
                    entry.append(f"Decision: {review_decision}")
                if review_rating:
                    entry.append(f"Rating: {review_rating}")
                entry.append(f"Comment: {review_comment}")

        entry.append("=" * 80 + "\n\n")
        return "\n".join(entry)

    def _download_pdf(self, pdf_url: str, filename: str, zip_file: zipfile.ZipFile):
        """Downloads a PDF in streaming fashion and adds it to the zip archive."""
        if not pdf_url:
            logging.warning(f"Skipping download for '{filename}' due to missing URL.")
            return

        temp_pdf_path = os.path.join(self.output_dir, filename)
        try:
            logging.info(f"Downloading: {pdf_url}")
            with requests.get(pdf_url, stream=True, timeout=30, headers=HEADERS) as r:
                r.raise_for_status()
                with open(temp_pdf_path, 'wb') as f:
                    # Download in chunks to keep memory usage low
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)

            # Add the downloaded file to the zip archive
            zip_file.write(temp_pdf_path, arcname=filename)
            logging.info(f"Added to zip: {filename}")

        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to download {pdf_url}: {e}")
        except Exception as e:
            logging.error(f"An error occurred while handling {filename}: {e}")
        finally:
            # Clean up the temporary PDF file
            if os.path.exists(temp_pdf_path):
                os.remove(temp_pdf_path)

    def process_papers(self, papers_iterator: Iterator[Dict[str, Any]], total: int):
        """
        The main processing pipeline. Iterates through papers and writes to files.
        """
        logging.info("Starting paper processing pipeline...")
        logging.info(f"Summary will be saved to: {self.summary_path}")
        if self.download_pdfs:
            logging.info(f"PDFs will be saved to: {self.zip_path}")
        else:
            logging.info("PDF download is disabled.")

        # Clear summary file at the start of a run
        with open(self.summary_path, 'w', encoding='utf-8') as f:
            f.write("--- PubCrawler Summary ---\n\n")

        try:
            with zipfile.ZipFile(self.zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Use tqdm for a nice progress bar
                pbar = tqdm(papers_iterator, total=total, desc="Processing papers")
                for paper_data in pbar:
                    # 1. Format and append to summary.txt
                    summary_entry = self._format_summary_entry(paper_data)
                    with open(self.summary_path, 'a', encoding='utf-8') as f:
                        f.write(summary_entry)

                    # 2. Download PDF if enabled
                    if self.download_pdfs:
                        filename = self._sanitize_filename(paper_data.get('title', 'untitled'))
                        self._download_pdf(paper_data.get('pdf_link'), filename, zipf)

        except Exception as e:
            logging.error(f"A critical error occurred during processing: {e}")

        logging.info("Processing pipeline complete.")

# END OF FILE: src/processor.py

==================== End of: src\crawlers\processor.py ====================



==================== Start of: src\crawlers\run_crawler.py ====================

# FILE: src/main.py (Optimized for Memory)

import time
import yaml
import re
import pandas as pd
from collections import defaultdict
from pathlib import Path
from tqdm import tqdm

# --- å¯¼å…¥æ‰€æœ‰ç‹¬ç«‹çš„ Scraper ---
from src.scrapers.iclr_scraper import IclrScraper
from src.scrapers.neurips_scraper import NeuripsScraper
from src.scrapers.icml_scraper import IcmlScraper
from src.scrapers.acl_scraper import AclScraper
from src.scrapers.arxiv_scraper import ArxivScraper
from src.scrapers.cvf_scraper import CvfScraper
from src.scrapers.aaai_scraper import AaaiScraper
from src.scrapers.kdd_scraper import KddScraper

from src.crawlers.config import get_logger, CONFIG_FILE, METADATA_OUTPUT_DIR, PDF_DOWNLOAD_DIR, TRENDS_OUTPUT_DIR, \
    LOG_DIR
from src.scrapers.tpami_scraper import TpamiScraper
# --- ã€ä¿®æ”¹ç‚¹ã€‘: å¯¼å…¥ save_as_markdown å’Œ generate_wordcloud_from_papers ---
from src.utils.formatter import save_as_csv, save_as_markdown
from src.analysis.analyzer import generate_wordcloud_from_papers
# ----------------------------------------------------------------------
from src.utils.downloader import download_single_pdf
from src.analysis.trends import run_single_task_analysis, run_cross_year_analysis
from src.utils.console_logger import print_banner, COLORS

OPERATION_MODE = "collect_and_analyze"

logger = get_logger(__name__)

# --- Scraper å’Œ Conference å®šä¹‰ (ä¿æŒä¸å˜) ---
SCRAPER_MAPPING = {"iclr": IclrScraper, "neurips": NeuripsScraper, "icml": IcmlScraper, "acl": AclScraper,
                   "cvf": CvfScraper, "aaai": AaaiScraper, "kdd": KddScraper, "arxiv": ArxivScraper,
                   "tpami": TpamiScraper}
CONF_TO_DEF_SOURCE = {'ICLR': 'openreview', 'NeurIPS': 'openreview', 'ICML': 'html_pmlr', 'ACL': 'html_acl',
                      'EMNLP': 'html_acl', 'NAACL': 'html_acl', 'CVPR': 'html_cvf', 'ICCV': 'html_cvf',
                      'AAAI': 'selenium', 'KDD': 'selenium'}

# å®šä¹‰å“ªäº›çˆ¬è™«ç±»å‹æ”¯æŒå¹¶å‘ä¼˜åŒ–ï¼Œä»¥ä¾¿åœ¨ä¸»ç¨‹åºä¸­ç»™å‡ºæç¤º
CONCURRENT_SCRAPER_TYPES = ['acl', 'cvf']


def load_config():
    if not CONFIG_FILE.exists():
        logger.error(f"[âœ– ERROR] Config file not found at {CONFIG_FILE}")
        return None
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def build_task_info(task: dict, source_definitions: dict) -> dict:
    # æ­¤å‡½æ•°é€»è¾‘ä¿æŒä¸å˜
    task_info = task.copy()
    conf, year, source_type = task.get('conference'), task.get('year'), task.get('source_type')
    if source_type in ['arxiv', 'tpami']: return task_info
    if not conf or not year:
        logger.error(f"[âœ– ERROR] Task '{task.get('name')}' is missing 'conference' or 'year'.");
        return None
    def_source_key = CONF_TO_DEF_SOURCE.get(conf)
    if not def_source_key:
        logger.error(f"[âœ– ERROR] No definition source found for conference '{conf}'.");
        return None
    if 'url_override' not in task:
        try:
            definition = source_definitions[def_source_key][conf]
            if isinstance(definition, dict):
                if 'venue_id' in definition:
                    task_info['venue_id'] = definition['venue_id'].replace('YYYY', str(year))
                    task_info['api_version'] = 'v1' if year in definition.get('api_v1_years', []) else 'v2'
                elif 'pattern_map' in definition:
                    base_url = "https://aclanthology.org/"
                    pattern = definition['pattern_map'].get(year)
                    if not pattern:
                        logger.error(f"[âœ– ERROR] No URL pattern defined for {conf} in year {year}");
                        return None
                    task_info['url'] = f"{base_url}{pattern}/"
            else:
                task_info['url'] = definition.replace('YYYY', str(year))
        except KeyError:
            logger.error(f"[âœ– ERROR] No definition for source='{def_source_key}' and conf='{conf}'");
            return None
    else:
        task_info['url'] = task['url_override']
    return task_info


def filter_papers(papers: list, filters: list) -> list:
    # æ­¤å‡½æ•°é€»è¾‘ä¿æŒä¸å˜
    if not filters: return papers
    original_count = len(papers)
    filter_regex = re.compile('|'.join(filters), re.IGNORECASE)
    filtered_papers = [p for p in papers if filter_regex.search(p.get('title', '') + ' ' + p.get('abstract', ''))]
    logger.info(
        f"    {COLORS['STEP']}-> Filtered papers: {original_count} -> {len(filtered_papers)} using filters: {filters}")
    return filtered_papers


def run_tasks_sequentially(tasks_to_run: list, source_definitions: dict, perform_single_analysis: bool) -> list:
    """
    é¡ºåºæ‰§è¡Œæ¯ä¸ªä»»åŠ¡ï¼Œå¹¶åœ¨æ¯ä¸ªä»»åŠ¡å®Œæˆåç«‹å³å¤„ç†å’Œä¿å­˜ç»“æœï¼Œä»¥èŠ‚çœå†…å­˜ã€‚
    è¿”å›æ‰€æœ‰ä»»åŠ¡æ”¶é›†åˆ°çš„è®ºæ–‡æ€»åˆ—è¡¨ï¼Œç”¨äºåç»­çš„è·¨å¹´åˆ†æã€‚
    """
    all_collected_papers = []

    for task in tasks_to_run:
        task_name = task.get('name', f"{task.get('conference')}_{task.get('year')}")
        if not task.get('enabled', False):
            continue

        logger.info(f"{COLORS['TASK_START']}[â–¶] STARTING TASK: {task_name}{COLORS['RESET']}")

        # --- æ–°å¢çš„æç¤ºä¿¡æ¯ ---
        source_type = task.get('source_type')
        if source_type in CONCURRENT_SCRAPER_TYPES:
            max_workers = task.get('max_workers', 1)  # é»˜è®¤ä¸º1ä¿è¯å®‰å…¨
            logger.info(f"    {COLORS['STEP']}[!] æ³¨æ„: æ­¤ä»»åŠ¡ç±»å‹ ({source_type}) éœ€è¦é€ä¸€è®¿é—®è®ºæ–‡è¯¦æƒ…é¡µã€‚")
            logger.info(
                f"    {COLORS['STEP']}    å·²å¯ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹è¿›è¡ŒåŠ é€Ÿã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœè®ºæ–‡æ•°é‡åºå¤§ï¼Œä»å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚")

        scraper_class = SCRAPER_MAPPING.get(source_type)
        if not scraper_class:
            logger.error(
                f"{COLORS['ERROR']}[âœ– FAILURE] No scraper for source: '{task['source_type']}'{COLORS['RESET']}\n");
            continue

        task_info = build_task_info(task, source_definitions)
        if not task_info:
            logger.error(
                f"{COLORS['ERROR']}[âœ– FAILURE] Could not build task info for '{task_name}'.{COLORS['RESET']}\n");
            continue

        try:
            scraper = scraper_class(task_info, logger)
            papers = scraper.scrape()
            papers = filter_papers(papers, task.get('filters', []))

            if papers:
                for paper in papers:
                    paper['year'] = task.get('year')
                    paper['conference'] = task.get('conference')

                logger.info(f"    {COLORS['STEP']}-> Successfully processed {len(papers)} papers for '{task_name}'.")

                logger.info(f"{COLORS['PHASE']}--- Processing & Saving Results for '{task_name}' ---{COLORS['RESET']}")
                conf, year = task.get('conference', 'Misc'), task.get('year', 'Latest')

                metadata_dir = METADATA_OUTPUT_DIR / conf / str(year)
                metadata_dir.mkdir(exist_ok=True, parents=True)

                # --- ã€æ–°å¢åŠŸèƒ½ã€‘: ç”Ÿæˆè¯äº‘å›¾å’ŒMarkdownæŠ¥å‘Š ---
                logger.info(f"    -> Generating word cloud...")
                wordcloud_path = metadata_dir / f"{task_name}_wordcloud.png"
                wordcloud_success = generate_wordcloud_from_papers(papers, wordcloud_path)
                final_wordcloud_path = str(wordcloud_path) if wordcloud_success else None

                logger.info(f"    -> Saving results to Markdown report...")
                save_as_markdown(papers, task_name, metadata_dir, wordcloud_path=final_wordcloud_path)
                # ----------------------------------------------------

                logger.info(f"    -> Saving metadata to {metadata_dir}")
                save_as_csv(papers, task_name, metadata_dir)

                if task.get('download_pdfs', False):
                    logger.info(f"    -> Starting PDF download...")
                    pdf_dir = PDF_DOWNLOAD_DIR / conf / str(year)
                    pdf_dir.mkdir(exist_ok=True, parents=True)
                    pbar_desc = f"    -> Downloading PDFs for {task_name}"
                    for paper in tqdm(papers, desc=pbar_desc, leave=True):
                        download_single_pdf(paper, pdf_dir)

                if perform_single_analysis:
                    analysis_output_dir = metadata_dir / "analysis"
                    analysis_output_dir.mkdir(exist_ok=True)
                    logger.info(f"    -> Running single-task analysis...")
                    run_single_task_analysis(papers, task_name, analysis_output_dir)

                all_collected_papers.extend(papers)
                logger.info(
                    f"{COLORS['SUCCESS']}[âœ” SUCCESS] Task '{task_name}' completed and saved.{COLORS['RESET']}\n")

            else:
                logger.warning(f"[âš  WARNING] No papers found for task: {task_name} (or none matched filters)")
                logger.info(f"{COLORS['WARNING']}[!] Task '{task_name}' finished with no results.{COLORS['RESET']}\n")

        except Exception as e:
            logger.critical(f"ä»»åŠ¡ '{task_name}' é­é‡ä¸¥é‡é”™è¯¯ï¼Œå·²ç»ˆæ­¢ã€‚é”™è¯¯: {e}")
            logger.info(f"è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯å·²è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶: {LOG_DIR / 'pubcrawler.log'}")

        time.sleep(0.5)

    return all_collected_papers


def load_all_data_for_cross_analysis(metadata_dir: Path) -> list:
    """åœ¨ 'analyze' æ¨¡å¼ä¸‹ï¼Œä»ç£ç›˜åŠ è½½æ‰€æœ‰ä¹‹å‰ä¿å­˜çš„ CSV æ–‡ä»¶ã€‚"""
    if not metadata_dir.exists():
        logger.error(f"[âœ– ERROR] Data directory not found: {metadata_dir}.");
        return []

    all_papers = []
    csv_files = list(metadata_dir.rglob("*_data_*.csv"))
    if not csv_files:
        logger.warning("[âš  WARNING] No CSV data files found for cross-year analysis.");
        return []

    logger.info(f"    -> Loading {len(csv_files)} previously collected CSV file(s) from disk...")
    for csv_path in csv_files:
        try:
            df = pd.read_csv(csv_path)
            df.fillna('', inplace=True)
            all_papers.extend(df.to_dict('records'))
        except Exception as e:
            logger.error(f"[âœ– ERROR] Failed to load data from {csv_path}: {e}")
    return all_papers


def main():
    print_banner()
    logger.info("=====================================================================================")
    logger.info(f"Starting PubCrawler in mode: '{OPERATION_MODE}'")
    logger.info("=====================================================================================\n")

    config = load_config()
    if not config: return

    all_papers_for_analysis = []

    if OPERATION_MODE in ["collect", "collect_and_analyze"]:
        logger.info(f"{COLORS['PHASE']}+----------------------------------------------------------+")
        logger.info(f"|    PHASE 1: PAPER COLLECTION & SINGLE-TASK ANALYSIS      |")
        logger.info(f"+----------------------------------------------------------+{COLORS['RESET']}\n")

        all_papers_for_analysis = run_tasks_sequentially(
            config.get('tasks', []),
            config.get('source_definitions', {}),
            perform_single_analysis=True
        )

    if OPERATION_MODE in ["analyze", "collect_and_analyze"]:
        logger.info(f"\n{COLORS['PHASE']}+----------------------------------------------------------+")
        logger.info(f"|          PHASE 2: CROSS-YEAR TREND ANALYSIS              |")
        logger.info(f"+----------------------------------------------------------+{COLORS['RESET']}\n")

        if OPERATION_MODE == "collect_and_analyze" and not all_papers_for_analysis:
            logger.warning("[âš  WARNING] No data was collected in Phase 1 to perform cross-year analysis.")

        elif OPERATION_MODE == "analyze":
            all_papers_for_analysis = load_all_data_for_cross_analysis(METADATA_OUTPUT_DIR)

        if all_papers_for_analysis:
            all_data_by_conf = defaultdict(list)
            for paper in all_papers_for_analysis:
                if paper.get('conference'):
                    all_data_by_conf[paper['conference']].append(paper)

            for conference, papers in all_data_by_conf.items():
                if not papers: continue
                conf_trend_dir = TRENDS_OUTPUT_DIR / conference
                conf_trend_dir.mkdir(exist_ok=True, parents=True)
                logger.info(f"{COLORS['TASK_START']}[â–¶] Analyzing trends for: {conference}{COLORS['RESET']}")
                run_cross_year_analysis(papers, conference, conf_trend_dir)
                logger.info(
                    f"{COLORS['SUCCESS']}[âœ” SUCCESS] Cross-year analysis for '{conference}' completed.{COLORS['RESET']}\n")

    logger.info("=====================================================================================")
    logger.info("PubCrawler run finished successfully.")
    logger.info("=====================================================================================")


if __name__ == "__main__":
    main()

==================== End of: src\crawlers\run_crawler.py ====================



==================== Start of: src\crawlers\__init__.py ====================



==================== End of: src\crawlers\__init__.py ====================



==================== Start of: src\scrapers\aaai_scraper.py ====================

# FILE: src/scrapers/aaai_scraper.py

import time
from typing import List, Dict, Any

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from .base_scraper import BaseScraper


class AaaiScraper(BaseScraper):
    """ä¸“é—¨ç”¨äº AAAI ç½‘ç«™çš„çˆ¬è™« (ä½¿ç”¨ Selenium)ã€‚"""

    def scrape(self) -> List[Dict[str, Any]]:
        url = self.task_info["url"]
        limit = self.task_info.get("limit")

        # AAAI ç‰¹å®šçš„é€‰æ‹©å™¨
        paper_link_selector = 'h5.toc-title > a'

        self.logger.info(f"    -> æ­£åœ¨å¯åŠ¨ Selenium è®¿é—® (AAAI): {url}")
        driver = None
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument(
                'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            driver.get(url)
            self.logger.info("    -> é¡µé¢å·²åŠ è½½. ç­‰å¾… 10 ç§’ä»¥å¤„ç†åŠ¨æ€å†…å®¹...")
            time.sleep(10)

            link_elements = driver.find_elements(By.CSS_SELECTOR, paper_link_selector)
            if not link_elements:
                self.logger.warning(f"    -> Selenium æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡é“¾æ¥ï¼Œä½¿ç”¨çš„é€‰æ‹©å™¨æ˜¯: '{paper_link_selector}'")
                return []

            self.logger.info(f"    -> æ‰¾åˆ°äº† {len(link_elements)} ä¸ªæ½œåœ¨çš„è®ºæ–‡é“¾æ¥ã€‚")
            if limit and len(link_elements) > limit:
                self.logger.info(f"    -> åº”ç”¨é™åˆ¶ï¼šå¤„ç†å‰ {limit} ä¸ªé“¾æ¥ã€‚")
                link_elements = link_elements[:limit]

            papers = []
            for i, link_elem in enumerate(link_elements):
                paper_url = link_elem.get_attribute('href')
                paper_title = link_elem.text
                if paper_url and paper_title:
                    papers.append({
                        'id': f"aaai_{self.task_info['year']}_{i}",
                        'title': paper_title.strip(),
                        'authors': 'N/A (AAAI Selenium)',
                        'abstract': 'N/A (AAAI Selenium)',
                        'pdf_url': None,
                        'source_url': paper_url
                    })
            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] AAAI Selenium æŠ“å–å¤±è´¥ {url}: {e}", exc_info=True)
            return []
        finally:
            if driver:
                driver.quit()

==================== End of: src\scrapers\aaai_scraper.py ====================



==================== Start of: src\scrapers\acl_scraper.py ====================

# FILE: src/scrapers/acl_scraper.py (Concurrent Version)

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get


class AclScraper(BaseScraper):
    """
    ä¸“é—¨ç”¨äº ACL Anthology ç½‘ç«™çš„çˆ¬è™«ã€‚
    æ­¤ç‰ˆæœ¬ç»è¿‡ä¼˜åŒ–ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘è·å–è®ºæ–‡è¯¦æƒ…ï¼Œä»¥å¤§å¹…æé«˜é€Ÿåº¦ã€‚
    """

    def _scrape_details_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å–å¹¶è§£æå•ä¸ª ACL è®ºæ–‡è¯¦æƒ…é¡µã€‚è¿™æ˜¯å°†è¢«å¹¶å‘æ‰§è¡Œçš„æ ¸å¿ƒå·¥ä½œå‡½æ•°ã€‚
        """
        response = robust_get(url, timeout=20)
        if not response:
            self.logger.debug(f"    -> è¯·æ±‚è¯¦æƒ…é¡µå¤±è´¥ (å·²é‡è¯•): {url}")
            return None

        try:
            soup = BeautifulSoup(response.content, 'lxml')

            title_tag = soup.select_one("h2#title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            author_tags = soup.select("p.lead a")
            authors = ", ".join([a.get_text(strip=True) for a in author_tags]) if author_tags else "N/A"

            abstract_tag = soup.select_one("div.acl-abstract > span")
            abstract = abstract_tag.get_text(strip=True) if abstract_tag else "N/A"

            pdf_url_tag = soup.select_one('meta[name="citation_pdf_url"]')
            pdf_url = pdf_url_tag['content'] if pdf_url_tag else None
            if pdf_url and not pdf_url.startswith('http'):
                pdf_url = urljoin(url, pdf_url)

            paper_id = url.strip('/').split('/')[-1]

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': url}
        except Exception as e:
            self.logger.debug(f"    -> è§£æ ACL è¯¦æƒ…é¡µå¤±è´¥ {url}: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]

        # ä»é…ç½®ä¸­è¯»å–å¹¶å‘å‚æ•°ï¼Œå¹¶æä¾›å®‰å…¨çš„é»˜è®¤å€¼
        max_workers = self.task_info.get("max_workers", 1)
        max_papers_limit = self.task_info.get("max_papers_limit", 0)

        # 1. é¦–å…ˆï¼Œè·å–åŒ…å«æ‰€æœ‰è®ºæ–‡é“¾æ¥çš„ç´¢å¼•é¡µ
        self.logger.info(f"    -> æ­£åœ¨æŠ“å– ACL ç´¢å¼•é¡µ: {index_url}")
        response = robust_get(index_url)
        if not response:
            return []

        if response.status_code == 404:
            self.logger.warning(f"    -> é¡µé¢æœªæ‰¾åˆ° (404): {index_url}")
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            link_tags = soup.select('p.d-sm-flex strong a.align-middle')

            detail_urls = [urljoin(index_url, tag['href']) for tag in link_tags if
                           f'{self.task_info["year"]}.acl-long.0' not in tag['href']]
            self.logger.info(f"    -> ç´¢å¼•é¡µè§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(detail_urls)} ç¯‡æœ‰æ•ˆè®ºæ–‡ã€‚")

            # 2. åº”ç”¨æ•°é‡é™åˆ¶
            urls_to_crawl = detail_urls
            if max_papers_limit > 0:
                # æ™ºèƒ½é™åˆ¶ï¼šå–ç”¨æˆ·è®¾ç½®å’Œå®é™…æ•°é‡ä¸­çš„è¾ƒå°å€¼
                actual_limit = min(max_papers_limit, len(detail_urls))
                urls_to_crawl = detail_urls[:actual_limit]
                self.logger.info(f"    -> å·²åº”ç”¨æ•°é‡é™åˆ¶ï¼Œå°†çˆ¬å–å‰ {len(urls_to_crawl)} ç¯‡è®ºæ–‡ã€‚")

            if not urls_to_crawl:
                return []

            # 3. ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘çˆ¬å–
            papers = []
            pbar_desc = f"    -> å¹¶å‘è§£æ {self.task_info.get('conference')} è¯¦æƒ…é¡µ"

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self._scrape_details_page, url): url for url in urls_to_crawl}

                # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
                for future in tqdm(as_completed(future_to_url), total=len(urls_to_crawl), desc=pbar_desc, leave=True):
                    result = future.result()
                    if result:
                        papers.append(result)

            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] è§£æ ACL é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []

==================== End of: src\scrapers\acl_scraper.py ====================



==================== Start of: src\scrapers\arxiv_scraper.py ====================

# FILE: src/scrapers/arxiv_scraper.py

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from typing import List, Dict, Any
import logging

from .base_scraper import BaseScraper


class ArxivScraper(BaseScraper):
    """Scraper for the arXiv API."""
    BASE_URL = 'http://export.arxiv.org/api/query?'

    def __init__(self, task_info: Dict[str, Any], logger: logging.Logger):
        super().__init__(task_info, logger)
        self.search_query = self.task_info.get('search_query', 'cat:cs.AI')
        self.limit = self.task_info.get('limit')
        self.max_results = self.limit if self.limit is not None else self.task_info.get('max_results', 10)
        self.sort_by = self.task_info.get('sort_by', 'submittedDate')
        self.sort_order = self.task_info.get('sort_order', 'descending')

    def _build_url(self) -> str:
        encoded_query = urllib.parse.quote(self.search_query)
        query_params = (f'search_query={encoded_query}&start=0&max_results={self.max_results}&'
                        f'sortBy={self.sort_by}&sortOrder={self.sort_order}')
        return self.BASE_URL + query_params

    def _parse_xml_entry(self, entry: ET.Element, ns: Dict[str, str]) -> Dict[str, Any]:
        def _get_text(element_name: str, namespace: str = 'atom'):
            element = entry.find(f'{namespace}:{element_name}', ns)
            return element.text.strip().replace('\n', ' ') if element is not None and element.text else None

        author_elements = entry.findall('atom:author', ns)
        authors_list = [author.find('atom:name', ns).text for author in author_elements if
                        author.find('atom:name', ns) is not None]

        pdf_url = None
        for link in entry.findall('atom:link', ns):
            if link.attrib.get('title') == 'pdf':
                pdf_url = link.attrib.get('href')
                break

        arxiv_id_url = _get_text('id')
        arxiv_id = arxiv_id_url.split('/abs/')[-1] if arxiv_id_url else "N/A"

        return {"id": arxiv_id, "title": _get_text('title'), "authors": ", ".join(authors_list),
                "abstract": _get_text('summary'), "pdf_url": pdf_url, "source_url": arxiv_id_url}

    def scrape(self) -> List[Dict[str, Any]]:
        full_url = self._build_url()
        self.logger.info(f"    -> Requesting data from arXiv: {self.search_query}")
        papers: List[Dict[str, Any]] = []
        try:
            with urllib.request.urlopen(full_url) as response:
                if response.status != 200:
                    self.logger.error(f"    [âœ– ERROR] HTTP request to arXiv failed with status code: {response.status}")
                    return papers
                xml_data = response.read().decode('utf-8')
                ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}
                root = ET.fromstring(xml_data)
                entries = root.findall('atom:entry', ns)
                for entry in entries:
                    papers.append(self._parse_xml_entry(entry, ns))
                return papers
        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] An unexpected error occurred during arXiv scraping: {e}", exc_info=True)
            return papers

==================== End of: src\scrapers\arxiv_scraper.py ====================



==================== Start of: src\scrapers\base_scraper.py ====================

# FILE: src/scrapers/base_scraper.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any
import logging

class BaseScraper(ABC):
    """
    æ‰€æœ‰æŠ“å–å™¨ç±»çš„æŠ½è±¡åŸºç±»ã€‚
    å®šä¹‰äº†æ‰€æœ‰å…·ä½“æŠ“å–å™¨å¿…é¡»éµå¾ªçš„æ¥å£ã€‚
    """

    def __init__(self, task_info: Dict[str, Any], logger: logging.Logger):
        """
        åˆå§‹åŒ–æŠ“å–å™¨ã€‚

        Args:
            task_info (Dict[str, Any]): ä» tasks.yaml ä¸­è¯»å–å¹¶æ„å»ºçš„ç‰¹å®šä»»åŠ¡é…ç½®ã€‚
            logger (logging.Logger): ä»ä¸»ç¨‹åºä¼ é€’è¿‡æ¥çš„å…±äº«æ—¥å¿—è®°å½•å™¨ã€‚
        """
        self.task_info = task_info
        self.logger = logger

    @abstractmethod
    def scrape(self) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒæŠ“å–çš„æ ¸å¿ƒæ–¹æ³•ã€‚

        æ¯ä¸ªå­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ï¼Œä»¥æ‰§è¡Œå…¶ç‰¹å®šçš„æŠ“å–é€»è¾‘ï¼Œ
        å¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ ‡å‡†å­—å…¸ç»“æ„çš„è®ºæ–‡åˆ—è¡¨ã€‚

        Returns:
            List[Dict[str, Any]]: æŠ“å–åˆ°çš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ã€‚
        """
        raise NotImplementedError("æ¯ä¸ª scraper å­ç±»å¿…é¡»å®ç° scrape æ–¹æ³•ã€‚")

==================== End of: src\scrapers\base_scraper.py ====================



==================== Start of: src\scrapers\cvf_scraper.py ====================

# FILE: src/scrapers/cvf_scraper.py (Concurrent Version)

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get


class CvfScraper(BaseScraper):
    """
    ä¸“é—¨ç”¨äº CVF (CVPR, ICCV) ç½‘ç«™çš„çˆ¬è™«ã€‚
    æ­¤ç‰ˆæœ¬ç»è¿‡ä¼˜åŒ–ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘è·å–è®ºæ–‡è¯¦æƒ…ï¼Œä»¥å¤§å¹…æé«˜é€Ÿåº¦ã€‚
    """

    def _scrape_details_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å–å¹¶è§£æå•ä¸ª CVF è®ºæ–‡è¯¦æƒ…é¡µã€‚è¿™æ˜¯å°†è¢«å¹¶å‘æ‰§è¡Œçš„æ ¸å¿ƒå·¥ä½œå‡½æ•°ã€‚
        """
        response = robust_get(url, timeout=20)
        if not response:
            self.logger.debug(f"    -> è¯·æ±‚è¯¦æƒ…é¡µå¤±è´¥ (å·²é‡è¯•): {url}")
            return None

        try:
            soup = BeautifulSoup(response.content, 'lxml')

            title_tag = soup.select_one("#papertitle")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            author_tags = soup.select("#authors > b > i")
            authors = ", ".join([a.get_text(strip=True) for a in author_tags]) if author_tags else "N/A"

            abstract_tag = soup.select_one("#abstract")
            abstract = abstract_tag.get_text(strip=True) if abstract_tag else "N/A"

            pdf_url_tag = soup.select_one('meta[name="citation_pdf_url"]')
            pdf_url = pdf_url_tag['content'] if pdf_url_tag else None
            if pdf_url and not pdf_url.startswith('http'):
                pdf_url = urljoin(url, pdf_url)

            paper_id = url.strip('/').split('/')[-1].replace('.html', '')

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': url}
        except Exception as e:
            self.logger.debug(f"    -> è§£æ CVF è¯¦æƒ…é¡µå¤±è´¥ {url}: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]
        max_workers = self.task_info.get("max_workers", 1)
        max_papers_limit = self.task_info.get("max_papers_limit", 0)

        self.logger.info(f"    -> æ­£åœ¨æŠ“å– CVF ç´¢å¼•é¡µ: {index_url}")
        response = robust_get(index_url)
        if not response:
            return []

        if response.status_code == 404:
            self.logger.warning(f"    -> é¡µé¢æœªæ‰¾åˆ° (404): {index_url}")
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            link_tags = soup.select('dt.ptitle > a[href$=".html"]')

            detail_urls = [urljoin(index_url, tag['href']) for tag in link_tags]
            self.logger.info(f"    -> ç´¢å¼•é¡µè§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(detail_urls)} ç¯‡è®ºæ–‡ã€‚")

            urls_to_crawl = detail_urls
            if max_papers_limit > 0:
                actual_limit = min(max_papers_limit, len(detail_urls))
                urls_to_crawl = detail_urls[:actual_limit]
                self.logger.info(f"    -> å·²åº”ç”¨æ•°é‡é™åˆ¶ï¼Œå°†çˆ¬å–å‰ {len(urls_to_crawl)} ç¯‡è®ºæ–‡ã€‚")

            if not urls_to_crawl:
                return []

            papers = []
            pbar_desc = f"    -> å¹¶å‘è§£æ {self.task_info.get('conference')} è¯¦æƒ…é¡µ"

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self._scrape_details_page, url): url for url in urls_to_crawl}

                for future in tqdm(as_completed(future_to_url), total=len(urls_to_crawl), desc=pbar_desc, leave=True):
                    result = future.result()
                    if result:
                        papers.append(result)

            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] è§£æ CVF é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []

==================== End of: src\scrapers\cvf_scraper.py ====================



==================== Start of: src\scrapers\iclr_scraper.py ====================

# FILE: src/scrapers/iclr_scraper.py

import openreview
import openreview.api
import re
import numpy as np
from tqdm import tqdm
from itertools import islice
import time
from typing import List, Dict, Any

from .base_scraper import BaseScraper

class IclrScraper(BaseScraper):
    """ä¸“é—¨ç”¨äº ICLR (OpenReview) çš„çˆ¬è™«ã€‚"""

    def _get_v1_notes_with_retry(self, client, venue_id, limit, max_retries=3):
        """ä¸º openreview v1 çš„ get_all_notes æ·»åŠ ç®€å•çš„é‡è¯•é€»è¾‘ã€‚"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"    -> [V1 API] æ­£åœ¨å°è¯•è·å–ç¬”è®° (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
                notes_iterator = client.get_all_notes(content={'venueid': venue_id})
                notes_list = list(islice(notes_iterator, limit)) if limit else list(notes_iterator)
                return notes_list
            except Exception as e:
                self.logger.warning(f"    -> [V1 API] ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # ç­‰å¾…æ—¶é—´é€æ¸å¢åŠ 
                else:
                    self.logger.error(f"    -> [V1 API] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·å–å¤±è´¥ã€‚")
                    raise e # æŠ›å‡ºæœ€ç»ˆçš„å¼‚å¸¸

    def scrape(self) -> List[Dict[str, Any]]:
        api_version = self.task_info.get("api_version", "v2")
        venue_id = self.task_info["venue_id"]
        limit = self.task_info.get("limit")
        fetch_reviews = self.task_info.get("fetch_reviews", False)

        self.logger.info(f"    -> ä½¿ç”¨ OpenReview API v{api_version} for venue: {venue_id}")
        if fetch_reviews:
            self.logger.info("    -> å·²å¯ç”¨å®¡ç¨¿ä¿¡æ¯è·å–ã€‚ç”±äºAPIé€Ÿç‡é™åˆ¶ï¼Œé€Ÿåº¦ä¼šå˜æ…¢ã€‚")

        try:
            notes_list = []
            if api_version == "v1":
                client = openreview.Client(baseurl='https://api.openreview.net')
                # <-- ä½¿ç”¨å¸¦é‡è¯•çš„å‡½æ•°
                notes_list = self._get_v1_notes_with_retry(client, venue_id, limit)
            else:  # API v2
                client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')
                # V2 API é€šå¸¸æ›´ç¨³å®šï¼Œä½†ä¹Ÿå¯ä»¥ä¸ºå…¶æ·»åŠ é‡è¯•
                notes_list = client.get_notes(content={'venueid': venue_id}, limit=limit) if limit else list(
                    client.get_all_notes(content={'venueid': venue_id}))

            if not notes_list:
                return []

            self.logger.info(f"    -> æ‰¾åˆ°äº† {len(notes_list)} ä»½æäº¤è¿›è¡Œå¤„ç†ã€‚")
            papers = []
            client_v2_for_reviews = openreview.api.OpenReviewClient(
                baseurl='https://api2.openreview.net') if fetch_reviews else None

            pbar_desc = f"    -> æ­£åœ¨è§£æ ICLR è®ºæ–‡"
            for note in tqdm(notes_list, desc=pbar_desc, leave=True):
                paper_details = self._parse_note(note)
                if fetch_reviews and client_v2_for_reviews:
                    time.sleep(0.3)
                    review_details = self._fetch_review_details(client_v2_for_reviews, note.id)
                    paper_details.update(review_details)
                papers.append(paper_details)
            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] ICLR OpenReview æŠ“å–å¤±è´¥: {e}", exc_info=True)
            return []

    def _parse_note(self, note: Any) -> Dict[str, Any]:
        """è§£æå•ä¸ª OpenReview note å¯¹è±¡ã€‚"""
        content = note.content
        def get_field_robust(field_name, default_value):
            field_data = content.get(field_name)
            if isinstance(field_data, dict):
                return field_data.get('value', default_value)
            return field_data if field_data is not None else default_value
        return {'id': note.id, 'title': get_field_robust('title', 'N/A'), 'authors': ', '.join(get_field_robust('authors', [])), 'abstract': get_field_robust('abstract', 'N/A'), 'pdf_url': f"https://openreview.net/pdf?id={note.id}", 'source_url': f"https://openreview.net/forum?id={note.id}"}

    def _fetch_review_details(self, client: openreview.api.OpenReviewClient, forum_id: str) -> Dict[str, Any]:
        """è·å–å•ä¸ªè®ºæ–‡çš„å®¡ç¨¿ä¿¡æ¯ã€‚"""
        ratings, decision = [], 'N/A'
        try:
            related_notes = client.get_notes(forum=forum_id)
            for note in related_notes:
                if any(re.search(r'/Decision', inv, re.IGNORECASE) for inv in note.invitations):
                    decision_value = note.content.get('decision', {}).get('value')
                    if decision_value: decision = str(decision_value)
                if any(re.search(r'/Review|/Official_Review', inv, re.IGNORECASE) for inv in note.invitations):
                    rating_val = note.content.get('rating', {}).get('value')
                    if isinstance(rating_val, str):
                        match = re.search(r'^\d+', rating_val)
                        if match: ratings.append(int(match.group(0)))
                    elif isinstance(rating_val, (int, float)):
                        ratings.append(int(rating_val))
        except Exception as e:
            self.logger.debug(f"è·å–å®¡ç¨¿ä¿¡æ¯å¤±è´¥ forum_id={forum_id}: {e}")
        return {'decision': decision, 'avg_rating': round(np.mean(ratings), 2) if ratings else None, 'review_ratings': ratings}

==================== End of: src\scrapers\iclr_scraper.py ====================



==================== Start of: src\scrapers\icml_scraper.py ====================

# FILE: src/scrapers/icml_scraper.py

from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
from typing import List, Dict, Optional, Any
from bs4.element import Tag

from .base_scraper import BaseScraper
from src.utils.network_utils import robust_get  # <-- å¯¼å…¥æ–°çš„å·¥å…·å‡½æ•°


class IcmlScraper(BaseScraper):
    """ä¸“é—¨ç”¨äº ICML (PMLR) ç½‘ç«™çš„çˆ¬è™«ã€‚"""

    def scrape(self) -> List[Dict[str, Any]]:
        index_url = self.task_info["url"]
        limit = self.task_info.get("limit")
        papers = []

        self.logger.info(f"    -> æ­£åœ¨æŠ“å– ICML ç´¢å¼•é¡µ: {index_url}")

        response = robust_get(index_url, timeout=45)  # <-- ä½¿ç”¨ robust_get å¹¶å¢åŠ è¶…æ—¶
        if not response:
            return []

        try:
            soup = BeautifulSoup(response.content, 'lxml')
            paper_containers = soup.select('div.paper')
            self.logger.info(f"    -> æ‰¾åˆ°äº† {len(paper_containers)} ç¯‡è®ºæ–‡ã€‚")

            if limit:
                paper_containers = paper_containers[:limit]
                self.logger.info(f"    -> åº”ç”¨é™åˆ¶ï¼šå¤„ç†å‰ {limit} ç¯‡è®ºæ–‡ã€‚")

            pbar_desc = f"    -> æ­£åœ¨è§£æ {self.task_info.get('conference')} é¡µé¢"
            for paper_div in tqdm(paper_containers, desc=pbar_desc, leave=True):
                paper_data = self._parse_paper_div(paper_div, index_url)
                if paper_data:
                    papers.append(paper_data)

            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] è§£æ ICML é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []

    def _parse_paper_div(self, paper_div: Tag, base_url: str) -> Optional[Dict[str, Any]]:
        """ä»å•ä¸ª <div class="paper"> ä¸­è§£æå‡ºæ‰€æœ‰ä¿¡æ¯ã€‚"""
        try:
            title_tag = paper_div.select_one('p.title')
            title = title_tag.get_text(strip=True) if title_tag else "N/A"

            authors_tag = paper_div.select_one('p.details span.authors')
            authors = authors_tag.get_text(strip=True).replace(';', ', ') if authors_tag else "N/A"

            links_p = paper_div.select_one('p.links')
            if not links_p:
                return None

            source_url_tag = links_p.select_one('a:-soup-contains("abs")')
            source_url = urljoin(base_url, source_url_tag['href']) if source_url_tag else 'N/A'

            pdf_url_tag = links_p.select_one('a:-soup-contains("Download PDF")')
            pdf_url = urljoin(base_url, pdf_url_tag['href']) if pdf_url_tag else 'N/A'

            paper_id = source_url.split('/')[-1].replace('.html', '') if source_url != 'N/A' else title
            abstract = "N/A (æ‘˜è¦éœ€è®¿é—®è¯¦æƒ…é¡µ)"

            return {'id': paper_id, 'title': title, 'authors': authors, 'abstract': abstract, 'pdf_url': pdf_url,
                    'source_url': source_url}
        except Exception as e:
            self.logger.debug(f"    -> ä» ICML å®¹å™¨è§£æå¤±è´¥: {e}")
            return None

==================== End of: src\scrapers\icml_scraper.py ====================



==================== Start of: src\scrapers\kdd_scraper.py ====================

# FILE: src/scrapers/kdd_scraper.py

import time
from typing import List, Dict, Any

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from .base_scraper import BaseScraper


class KddScraper(BaseScraper):
    """ä¸“é—¨ç”¨äº KDD ç½‘ç«™çš„çˆ¬è™« (ä½¿ç”¨ Selenium)ã€‚"""

    def scrape(self) -> List[Dict[str, Any]]:
        url = self.task_info["url"]
        limit = self.task_info.get("limit")

        # KDD ç‰¹å®šçš„é€‰æ‹©å™¨
        paper_link_selector = 'a.item-title'

        self.logger.info(f"    -> æ­£åœ¨å¯åŠ¨ Selenium è®¿é—® (KDD): {url}")
        driver = None
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument(
                'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            driver.get(url)
            self.logger.info("    -> é¡µé¢å·²åŠ è½½. ç­‰å¾… 10 ç§’ä»¥å¤„ç†åŠ¨æ€å†…å®¹...")
            time.sleep(10)

            link_elements = driver.find_elements(By.CSS_SELECTOR, paper_link_selector)
            if not link_elements:
                self.logger.warning(f"    -> Selenium æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡é“¾æ¥ï¼Œä½¿ç”¨çš„é€‰æ‹©å™¨æ˜¯: '{paper_link_selector}'")
                return []

            self.logger.info(f"    -> æ‰¾åˆ°äº† {len(link_elements)} ä¸ªæ½œåœ¨çš„è®ºæ–‡é“¾æ¥ã€‚")
            if limit and len(link_elements) > limit:
                self.logger.info(f"    -> åº”ç”¨é™åˆ¶ï¼šå¤„ç†å‰ {limit} ä¸ªé“¾æ¥ã€‚")
                link_elements = link_elements[:limit]

            papers = []
            for i, link_elem in enumerate(link_elements):
                paper_url = link_elem.get_attribute('href')
                paper_title = link_elem.text
                if paper_url and paper_title:
                    papers.append({
                        'id': f"kdd_{self.task_info['year']}_{i}",
                        'title': paper_title.strip(),
                        'authors': 'N/A (KDD Selenium)',
                        'abstract': 'N/A (KDD Selenium)',
                        'pdf_url': None,
                        'source_url': paper_url
                    })
            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] KDD Selenium æŠ“å–å¤±è´¥ {url}: {e}", exc_info=True)
            return []
        finally:
            if driver:
                driver.quit()

==================== End of: src\scrapers\kdd_scraper.py ====================



==================== Start of: src\scrapers\neurips_scraper.py ====================

# FILE: src/scrapers/neurips_scraper.py

import openreview
import openreview.api
import re
import numpy as np
from tqdm import tqdm
from itertools import islice
import time
from typing import List, Dict, Any

from .base_scraper import BaseScraper


class NeuripsScraper(BaseScraper):
    """ä¸“é—¨ç”¨äº NeurIPS (OpenReview) çš„çˆ¬è™«ã€‚"""

    def scrape(self) -> List[Dict[str, Any]]:
        api_version = self.task_info.get("api_version", "v2")
        venue_id = self.task_info["venue_id"]
        limit = self.task_info.get("limit")
        fetch_reviews = self.task_info.get("fetch_reviews", False)

        self.logger.info(f"    -> ä½¿ç”¨ OpenReview API v{api_version} for venue: {venue_id}")
        if fetch_reviews:
            self.logger.info("    -> å·²å¯ç”¨å®¡ç¨¿ä¿¡æ¯è·å–ã€‚ç”±äºAPIé€Ÿç‡é™åˆ¶ï¼Œé€Ÿåº¦ä¼šå˜æ…¢ã€‚")

        try:
            notes_list = []
            if api_version == "v1":
                client = openreview.Client(baseurl='https://api.openreview.net')
                notes_iterator = client.get_all_notes(content={'venueid': venue_id})
                notes_list = list(islice(notes_iterator, limit)) if limit else list(notes_iterator)
            else:  # API v2
                client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')
                notes_list = client.get_notes(content={'venueid': venue_id}, limit=limit) if limit else list(
                    client.get_all_notes(content={'venueid': venue_id}))

            if not notes_list:
                return []

            self.logger.info(f"    -> æ‰¾åˆ°äº† {len(notes_list)} ä»½æäº¤è¿›è¡Œå¤„ç†ã€‚")
            papers = []
            client_v2_for_reviews = openreview.api.OpenReviewClient(
                baseurl='https://api2.openreview.net') if fetch_reviews else None

            pbar_desc = f"    -> æ­£åœ¨è§£æ NeurIPS è®ºæ–‡"
            for note in tqdm(notes_list, desc=pbar_desc, leave=True):
                paper_details = self._parse_note(note)
                if fetch_reviews and client_v2_for_reviews:
                    time.sleep(0.3)
                    review_details = self._fetch_review_details(client_v2_for_reviews, note.id)
                    paper_details.update(review_details)
                papers.append(paper_details)
            return papers

        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] NeurIPS OpenReview æŠ“å–å¤±è´¥: {e}", exc_info=True)
            return []

    def _parse_note(self, note: Any) -> Dict[str, Any]:
        """è§£æå•ä¸ª OpenReview note å¯¹è±¡ã€‚"""
        content = note.content

        def get_field_robust(field_name, default_value):
            field_data = content.get(field_name)
            if isinstance(field_data, dict):
                return field_data.get('value', default_value)
            return field_data if field_data is not None else default_value

        return {
            'id': note.id,
            'title': get_field_robust('title', 'N/A'),
            'authors': ', '.join(get_field_robust('authors', [])),
            'abstract': get_field_robust('abstract', 'N/A'),
            'pdf_url': f"https://openreview.net/pdf?id={note.id}",
            'source_url': f"https://openreview.net/forum?id={note.id}"
        }

    def _fetch_review_details(self, client: openreview.api.OpenReviewClient, forum_id: str) -> Dict[str, Any]:
        """è·å–å•ä¸ªè®ºæ–‡çš„å®¡ç¨¿ä¿¡æ¯ã€‚"""
        ratings, decision = [], 'N/A'
        try:
            related_notes = client.get_notes(forum=forum_id)
            for note in related_notes:
                if any(re.search(r'/Decision', inv, re.IGNORECASE) for inv in note.invitations):
                    decision_value = note.content.get('decision', {}).get('value')
                    if decision_value: decision = str(decision_value)
                if any(re.search(r'/Review|/Official_Review', inv, re.IGNORECASE) for inv in note.invitations):
                    rating_val = note.content.get('rating', {}).get('value')
                    if isinstance(rating_val, str):
                        match = re.search(r'^\d+', rating_val)
                        if match: ratings.append(int(match.group(0)))
                    elif isinstance(rating_val, (int, float)):
                        ratings.append(int(rating_val))
        except Exception as e:
            self.logger.debug(f"è·å–å®¡ç¨¿ä¿¡æ¯å¤±è´¥ forum_id={forum_id}: {e}")

        return {'decision': decision, 'avg_rating': round(np.mean(ratings), 2) if ratings else None,
                'review_ratings': ratings}

==================== End of: src\scrapers\neurips_scraper.py ====================



==================== Start of: src\scrapers\tpami_scraper.py ====================

# FILE: src/scrapers/tpami_scraper.py (API Version)

import requests
import json
from typing import List, Dict, Any
from tqdm import tqdm
import time

from .base_scraper import BaseScraper


class TpamiScraper(BaseScraper):
    """
    ä¸“é—¨ç”¨äº IEEE TPAMI æœŸåˆŠçš„çˆ¬è™« (ä½¿ç”¨åå° API)ã€‚
    è¿™æ˜¯ä¸€ä¸ªæ›´ç¨³å®šã€æ›´é«˜æ•ˆçš„æ–¹æ¡ˆï¼Œå–ä»£äº† Seleniumã€‚
    """
    BASE_URL = "https://ieeexplore.ieee.org"

    def _get_issue_number(self, punumber: str) -> str:
        """
        ç¬¬ä¸€æ­¥: è°ƒç”¨ metadata API è·å–æœ€æ–°çš„ 'issueNumber'ã€‚
        è¿™ä¸ª issueNumber æ˜¯è·å–è®ºæ–‡åˆ—è¡¨çš„å…³é”®ã€‚
        """
        metadata_url = f"{self.BASE_URL}/rest/publication/home/metadata?pubid={punumber}"
        headers = {
            # å…³é”®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿä»æœŸåˆŠä¸»é¡µå‘èµ·çš„è¯·æ±‚
            'Referer': f'{self.BASE_URL}/xpl/conhome/{punumber}/proceeding',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.logger.info(f"    -> æ­£åœ¨è·å– issue number from: {metadata_url}")
        try:
            response = requests.get(metadata_url, headers=headers, timeout=20)
            response.raise_for_status()
            data = response.json()
            # issueNumber å¯ä»¥æ˜¯ 'Early Access' çš„ IDï¼Œä¹Ÿå¯ä»¥æ˜¯æœ€æ–°ä¸€æœŸçš„ ID
            issue_number = str(data['currentIssue']['issueNumber'])
            self.logger.info(f"    -> æˆåŠŸè·å– issue number: {issue_number}")
            return issue_number
        except Exception as e:
            self.logger.error(f"    [âœ– ERROR] è·å– issue number å¤±è´¥: {e}")
            return None

    def scrape(self) -> List[Dict[str, Any]]:
        punumber = self.task_info.get("punumber")
        if not punumber:
            self.logger.error("    [âœ– ERROR] TPAMI task in YAML must have a 'punumber'. For TPAMI, it's '34'.")
            return []

        limit = self.task_info.get("limit")

        issue_number = self._get_issue_number(punumber)
        if not issue_number:
            return []

        papers = []
        page_number = 1
        total_records = 0
        total_pages = 1  # å…ˆå‡è®¾åªæœ‰ä¸€é¡µ

        self.logger.info("    -> å¼€å§‹é€é¡µè·å–è®ºæ–‡åˆ—è¡¨...")
        pbar = tqdm(total=total_records or limit or 25, desc=f"    -> Scraping TPAMI page {page_number}")

        while True:
            toc_url = f"{self.BASE_URL}/rest/search/pub/{punumber}/issue/{issue_number}/toc"
            payload = {
                "pageNumber": str(page_number),
                "punumber": str(punumber),
                "isnumber": str(issue_number)
            }
            headers = {
                'Referer': f'{self.BASE_URL}/xpl/conhome/{punumber}/proceeding?pageNumber={page_number}',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Content-Type': 'application/json;charset=UTF-8'
            }

            try:
                response = requests.post(toc_url, headers=headers, data=json.dumps(payload), timeout=20)
                response.raise_for_status()
                data = response.json()

                if page_number == 1:
                    total_records = data.get('totalRecords', 0)
                    total_pages = data.get('totalPages', 1)
                    pbar.total = limit if limit and limit < total_records else total_records
                    self.logger.info(f"    -> å…±å‘ç° {total_records} ç¯‡è®ºæ–‡ï¼Œåˆ†å¸ƒåœ¨ {total_pages} é¡µã€‚")

                records = data.get('records', [])
                if not records:
                    self.logger.info("    -> å½“å‰é¡µæ²¡æœ‰æ›´å¤šè®ºæ–‡ï¼ŒæŠ“å–ç»“æŸã€‚")
                    break

                for record in records:
                    papers.append({
                        'id': record.get('articleNumber', ''),
                        'title': record.get('highlightedTitle', 'N/A').replace('<br>', ' '),
                        'authors': ', '.join([author['name'] for author in record.get('authors', [])]),
                        'abstract': record.get('abstract', 'N/A'),
                        'pdf_url': f"è¯·è®¿é—®æºé¡µé¢æŸ¥çœ‹PDFï¼ˆå¯èƒ½éœ€è¦è®¢é˜…ï¼‰",
                        'source_url': self.BASE_URL + record.get('documentLink', ''),
                        'conference': 'TPAMI'
                    })
                    pbar.update(1)
                    if limit and len(papers) >= limit:
                        break

                if (limit and len(papers) >= limit) or page_number >= total_pages:
                    break

                page_number += 1
                pbar.set_description(f"    -> Scraping TPAMI page {page_number}")
                time.sleep(1)  # å‹å¥½è®¿é—®

            except Exception as e:
                self.logger.error(f"    [âœ– ERROR] åœ¨ç¬¬ {page_number} é¡µæŠ“å–å¤±è´¥: {e}")
                break

        pbar.close()
        return papers

==================== End of: src\scrapers\tpami_scraper.py ====================



==================== Start of: src\scrapers\__init__.py ====================

# FILE: src/scrapers/__init__.py

# This file makes the 'scrapers' directory a Python package.

# END OF FILE: src/scrapers/__init__.py

==================== End of: src\scrapers\__init__.py ====================



==================== Start of: src\search\embedder_chroma.py ====================

# FILE: src/search/embedder_multiprocess.py (Final Version with LIMIT and Incremental Update)

import sqlite3
import chromadb
from sentence_transformers import SentenceTransformer, util
from pathlib import Path
import time
import torch
import os

# --- é…ç½® ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"

# --- ã€æ ¸å¿ƒæ§åˆ¶å¼€å…³ã€‘ ---
# è®¾ç½®ä¸ºæ•°å­— (å¦‚ 2000) æ¥å¼€å¯â€œå¿«é€Ÿæµ‹è¯•æ¨¡å¼â€ï¼Œåªå¤„ç†æŒ‡å®šæ•°é‡çš„è®ºæ–‡ã€‚
# è®¾ç½®ä¸º None æ¥å¼€å¯â€œæ™ºèƒ½å¢é‡æ¨¡å¼â€ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰æ–°è®ºæ–‡ã€‚
PAPER_LIMIT = None


# ------------------------

def embed_and_store_parallel():
    if not DB_PATH.exists():
        print(f"[!] é”™è¯¯: SQLiteæ•°æ®åº“æ–‡ä»¶ {DB_PATH} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ indexer.pyã€‚")
        return

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"[*] 1. åˆå§‹åŒ–æ¨¡å‹ '{MODEL_NAME}' (è®¾å¤‡: {device})...")
    model = SentenceTransformer(MODEL_NAME, device=device)
    print("[âœ”] æ¨¡å‹åŠ è½½æˆåŠŸã€‚")

    print(f"[*] 2. è¿æ¥å¹¶è®¾ç½®ChromaDB (è·¯å¾„: {CHROMA_DB_PATH})...")
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    collection = client.get_or_create_collection(name=COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    print(f"[âœ”] ChromaDBé›†åˆ '{COLLECTION_NAME}' å‡†å¤‡å°±ç»ªã€‚")

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    papers_to_process = []

    # --- ã€æ ¸å¿ƒé€»è¾‘ï¼šæ¨¡å¼é€‰æ‹©ã€‘ ---
    if PAPER_LIMIT:
        # æ¨¡å¼ä¸€ï¼šå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        print(f"[*] [å¿«é€Ÿæµ‹è¯•æ¨¡å¼] å·²å¯ç”¨ï¼Œå°†å¼ºåˆ¶å¤„ç†æ•°æ®åº“ä¸­çš„å‰ {PAPER_LIMIT} ç¯‡è®ºæ–‡ã€‚")
        cursor.execute(f"SELECT rowid, title, abstract, conference, year, source_file FROM papers_fts LIMIT ?",
                       (PAPER_LIMIT,))
        papers_to_process = cursor.fetchall()

    else:
        # æ¨¡å¼äºŒï¼šæ™ºèƒ½å¢é‡æ¨¡å¼
        print(f"[*] [æ™ºèƒ½å¢é‡æ¨¡å¼] å¯åŠ¨ï¼Œå¼€å§‹è®¡ç®—éœ€è¦æ›´æ–°çš„è®ºæ–‡...")

        # 1. ä»ChromaDBè·å–å·²å­˜åœ¨çš„ID
        existing_ids_in_chroma = set(collection.get(include=[])['ids'])
        print(f"    -> ChromaDBä¸­å·²å­˜åœ¨ {len(existing_ids_in_chroma)} ä¸ªå‘é‡ã€‚")

        # 2. ä»SQLiteè·å–æ‰€æœ‰ID
        cursor.execute("SELECT rowid FROM papers_fts")
        all_ids_in_sqlite = {str(row[0]) for row in cursor.fetchall()}
        print(f"    -> SQLiteä¸­æ€»å…±æœ‰ {len(all_ids_in_sqlite)} ç¯‡è®ºæ–‡ã€‚")

        # 3. è®¡ç®—å·®é›†ï¼Œå¾—åˆ°éœ€è¦å¤„ç†çš„æ–°ID
        new_paper_ids = list(all_ids_in_sqlite - existing_ids_in_chroma)

        if not new_paper_ids:
            print("\n[âœ”] æ•°æ®åº“å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚ä»»åŠ¡ç»“æŸã€‚")
            conn.close()
            return

        print(f"[âœ”] å‘ç° {len(new_paper_ids)} ç¯‡æ–°è®ºæ–‡éœ€è¦å¤„ç†ã€‚")

        # 4. åªä»SQLiteä¸­è·å–è¿™äº›æ–°è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
        placeholders = ','.join('?' for _ in new_paper_ids)
        query = f"SELECT rowid, title, abstract, conference, year, source_file FROM papers_fts WHERE rowid IN ({placeholders})"
        cursor.execute(query, new_paper_ids)
        papers_to_process = cursor.fetchall()

    conn.close()

    if not papers_to_process:
        print("[!] æœ¬æ¬¡è¿è¡Œæ²¡æœ‰éœ€è¦å¤„ç†çš„è®ºæ–‡ã€‚ä»»åŠ¡ç»“æŸã€‚")
        return

    print(f"[âœ”] æœ¬æ¬¡å…±éœ€å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡ã€‚")

    worker_processes = 2  # å›ºå®šä½¿ç”¨2ä¸ªè¿›ç¨‹
    print(f"[*] å°†ä½¿ç”¨ {worker_processes} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚")

    print("[*] 4. å¯åŠ¨å¤šè¿›ç¨‹æ± ...")
    pool = model.start_multi_process_pool(target_devices=[device] * worker_processes)
    print("[âœ”] è¿›ç¨‹æ± å·²å¯åŠ¨ã€‚")

    print("[*] 5. å¼€å§‹å¹¶è¡Œç”Ÿæˆå‘é‡...")
    start_time = time.time()

    documents_to_embed = [f"{p[1]}. {p[2]}" for p in papers_to_process]
    embeddings = model.encode_multi_process(documents_to_embed, pool, batch_size=64)

    end_time_encoding = time.time()
    print(f"[âœ”] å‘é‡ç”Ÿæˆå®Œæ¯•! è€—æ—¶: {end_time_encoding - start_time:.2f} ç§’ã€‚")

    model.stop_multi_process_pool(pool)
    print("[âœ”] è¿›ç¨‹æ± å·²å…³é—­ã€‚")

    print("[*] 6. å¼€å§‹å°†å‘é‡æ‰¹é‡å­˜å…¥ChromaDB...")
    start_time_storing = time.time()

    ids = [str(p[0]) for p in papers_to_process]
    metadatas = [{"title": p[1], "conference": p[3], "year": p[4], "source_file": p[5]} for p in papers_to_process]

    # ChromaDBçš„addæ–¹æ³•æ˜¯å¹‚ç­‰çš„ï¼Œå¦‚æœIDå·²å­˜åœ¨ï¼Œå®ƒä¼šæ›´æ–°å†…å®¹ã€‚å¯¹äºæˆ‘ä»¬çš„åœºæ™¯ï¼Œç”¨upsertæ›´è¯­ä¹‰åŒ–ã€‚
    db_batch_size = 1024
    for i in range(0, len(papers_to_process), db_batch_size):
        collection.upsert(
            ids=ids[i:i + db_batch_size],
            embeddings=embeddings[i:i + db_batch_size].tolist(),
            metadatas=metadatas[i:i + db_batch_size],
            documents=documents_to_embed[i:i + db_batch_size]
        )

    end_time_storing = time.time()
    print(f"[âœ”] æ•°æ®å­˜å‚¨/æ›´æ–°å®Œæ¯•! è€—æ—¶: {end_time_storing - start_time_storing:.2f} ç§’ã€‚")

    print("\n" + "=" * 50)
    print(f"[âœ”] æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print(f"    - æœ¬æ¬¡å¤„ç†è®ºæ–‡: {len(papers_to_process)} ç¯‡")
    print(f"    - å‘é‡æ•°æ®åº“ä¸­çš„æ¡ç›®æ€»æ•°: {collection.count()}")
    print(f"    - æ€»è€—æ—¶: {end_time_storing - start_time:.2f} ç§’")
    print("=" * 50)


if __name__ == "__main__":
    try:
        import torch
    except ImportError:
        print("é”™è¯¯: PyTorch æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'uv pip install torch'")
        exit()

    embed_and_store_parallel()

==================== End of: src\search\embedder_chroma.py ====================



==================== Start of: src\search\indexer.py ====================

# FILE: src/search/indexer.py

import sqlite3
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import time

# --- é…ç½® ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
METADATA_DIR = PROJECT_ROOT / "output" / "metadata"
# æ–°å¢ï¼šå®šä¹‰ç»Ÿä¸€çš„æ•°æ®åº“å­˜æ”¾ç›®å½•
DB_DIR = PROJECT_ROOT / "database"

# ä¿®æ”¹ï¼šè®© DB_PATH æŒ‡å‘æ–°ç›®å½•ä¸­çš„æ–‡ä»¶
DB_PATH = DB_DIR / "papers.db"
# å®šä¹‰éœ€è¦çš„åˆ—ï¼Œä¸æ•°æ®åº“è¡¨ç»“æ„å¯¹åº”
REQUIRED_COLUMNS = ['title', 'authors', 'abstract', 'conference', 'year', 'pdf_url', 'source_file']


def create_fts_table(conn):
    """åˆ›å»ºæ”¯æŒå…¨æ–‡æœç´¢çš„ FTS5 è™šæ‹Ÿè¡¨"""
    cursor = conn.cursor()
    # å¦‚æœè¡¨å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼Œç¡®ä¿æ¯æ¬¡é‡å»ºç´¢å¼•éƒ½æ˜¯æœ€æ–°çš„
    cursor.execute("DROP TABLE IF EXISTS papers_fts")
    # åˆ›å»º FTS5 è¡¨ã€‚è¿™é‡Œå®šä¹‰äº†æˆ‘ä»¬æƒ³å¯¹å…¶è¿›è¡Œå…¨æ–‡æœç´¢çš„æ‰€æœ‰å­—æ®µã€‚
    cursor.execute("""
        CREATE VIRTUAL TABLE papers_fts USING fts5(
            title,
            authors,
            abstract,
            conference UNINDEXED,  -- UNINDEXED è¡¨ç¤ºè¿™ä¸ªå­—æ®µå­˜å‚¨ä½†ä¸å»ºç«‹å…¨æ–‡ç´¢å¼•(èŠ‚çœç©ºé—´)ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦å…¨æ–‡æœå®ƒ
            year UNINDEXED,
            pdf_url UNINDEXED,
            source_file UNINDEXED,
            tokenize='porter'      -- ä½¿ç”¨ porter åˆ†è¯å™¨ï¼Œæ”¯æŒè‹±æ–‡è¯å¹²æå–(ä¾‹å¦‚æœ searching èƒ½åŒ¹é… search)
        )
    """)
    conn.commit()


def index_csv_files():
    print(f"[*] å¼€å§‹æ„å»ºç´¢å¼•...")
    print(f"    - æ•°æ®æºç›®å½•: {METADATA_DIR}")
    print(f"    - æ•°æ®åº“è·¯å¾„: {DB_PATH}")

    csv_files = list(METADATA_DIR.rglob("*_data_*.csv"))
    if not csv_files:
        print("[!] é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œçˆ¬è™«é‡‡é›†æ•°æ®ã€‚")
        return

    conn = sqlite3.connect(str(DB_PATH))
    create_fts_table(conn)

    total_files = len(csv_files)
    total_papers = 0
    start_time = time.time()

    print(f"[*] å‘ç° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    for i, csv_path in enumerate(csv_files, 1):
        try:
            # ä½¿ç”¨ chunksize åˆ†å—è¯»å–ï¼Œæ ¸å¿ƒå†…å­˜ä¼˜åŒ–ç‚¹ï¼
            # æ¯æ¬¡åªè¯» 5000 è¡Œåˆ°å†…å­˜ï¼Œå¤„ç†å®Œå°±é‡Šæ”¾ï¼Œç»ä¸çˆ†å†…å­˜ã€‚
            chunk_iterator = pd.read_csv(csv_path, chunksize=5000, dtype=str)

            for chunk_df in chunk_iterator:
                if chunk_df.empty: continue

                # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
                chunk_df = chunk_df.fillna('')
                if 'source_file' not in chunk_df.columns:
                    chunk_df['source_file'] = csv_path.name

                # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
                for col in REQUIRED_COLUMNS:
                    if col not in chunk_df.columns:
                        chunk_df[col] = ''

                # é€‰å–å¹¶æ’åºç‰¹å®šçš„åˆ—ä»¥åŒ¹é…æ•°æ®åº“ç»“æ„
                data_to_insert = chunk_df[REQUIRED_COLUMNS].values.tolist()

                # æ‰¹é‡æ’å…¥æ•°æ®
                conn.executemany(
                    "INSERT INTO papers_fts(title, authors, abstract, conference, year, pdf_url, source_file) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    data_to_insert
                )
                total_papers += len(data_to_insert)

            print(f"    [{i}/{total_files}] å·²ç´¢å¼•: {csv_path.name}")

        except Exception as e:
            print(f"    [!] å¤„ç†æ–‡ä»¶å¤±è´¥ {csv_path.name}: {e}")

    # æäº¤äº‹åŠ¡å¹¶è¿›è¡Œä¼˜åŒ–
    print("[*] æ­£åœ¨æäº¤å¹¶ä¼˜åŒ–æ•°æ®åº“ (å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    conn.commit()
    # Optimize å‘½ä»¤ä¼šé‡ç»„æ•°æ®åº“æ–‡ä»¶ï¼Œä½¿å…¶åœ¨æœç´¢æ—¶æ›´å¿«
    conn.execute("INSERT INTO papers_fts(papers_fts) VALUES('optimize')")
    conn.close()

    end_time = time.time()
    print(f"\n[âœ”] ç´¢å¼•æ„å»ºå®Œæˆï¼")
    print(f"    - æ€»è®¡ç´¢å¼•è®ºæ–‡: {total_papers} ç¯‡")
    print(f"    - æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"    - æ•°æ®åº“æ–‡ä»¶å¤§å°: {DB_PATH.stat().st_size / (1024 * 1024):.2f} MB")


if __name__ == "__main__":
    index_csv_files()

==================== End of: src\search\indexer.py ====================



==================== Start of: src\search\search_ai_assistant.py ====================

# FILE: src/search/search_ai_assistant.py (CLI Launcher - v1.1)

import sys
import math
import textwrap
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# --- ä» search_service å¯¼å…¥æ‰€æœ‰åŠŸèƒ½å’Œé…ç½® ---
from src.search.search_service import (
    initialize_components,
    keyword_search,
    semantic_search,
    get_stats_summary,
    save_results_to_markdown,
    _sqlite_conn,
    PROJECT_ROOT,
    SEARCH_RESULTS_DIR,
    RESULTS_PER_PAGE,
    Colors, # å¯¼å…¥Colors
    _initialized
)
# --- å¯¼å…¥CLIä¸“å±çš„AIå¯¹è¯äº¤äº’å‡½æ•° ---
from src.ai.glm_chat_service import start_ai_chat_session

# --- å®šä¹‰CLIä¸“å±çš„ print_colored å‡½æ•° ---
# ç¡®ä¿åœ¨CLIäº¤äº’ä¸­èƒ½å¤Ÿæ­£ç¡®æ‰“å°å½©è‰²æ–‡æœ¬
def print_colored(text, color, end='\n'):
    if sys.stdout.isatty():
        print(f"{color}{text}{Colors.ENDC}", end=end)
    else:
        print(text, end=end)

# --- CLIç‰¹æœ‰çš„Banner ---
def print_banner():
    banner_text = "--- PubCrawler v7.4: CLI AI Assistant (Refactored) ---"
    print_colored(banner_text, Colors.HEADER)
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    print_colored(f"[*] ç»“æœå°†ä¿å­˜è‡³: {SEARCH_RESULTS_DIR.resolve()}", Colors.UNDERLINE)

# --- CLIç‰¹æœ‰çš„ç»“æœç»Ÿè®¡æ‰“å° ---
def print_cli_stats_summary(stats_summary: Dict[str, Any]):
    if not stats_summary['total_found']: return
    print_colored("\n--- æŸ¥è¯¢ç»“æœç»Ÿè®¡ ---", Colors.HEADER)
    print(f"æ€»è®¡æ‰¾åˆ° {Colors.BOLD}{stats_summary['total_found']}{Colors.ENDC} ç¯‡ç›¸å…³è®ºæ–‡ã€‚")
    if stats_summary['distribution']:
        print("åˆ†å¸ƒæƒ…å†µ:")
        for conf_year, count in stats_summary['distribution'].items():
            print(f"  - {conf_year}: {count} ç¯‡")
    print_colored("--------------------", Colors.HEADER)

# --- CLIç‰¹æœ‰çš„åˆ†é¡µé€»è¾‘ ---
def interactive_pagination_cli(results: List[Dict[str, Any]], query: str, session_dir: Path):
    num_results = len(results)
    if num_results == 0:
        print_colored("[!] æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚", Colors.WARNING)
        return

    stats_summary = get_stats_summary(results)
    print_cli_stats_summary(stats_summary)

    total_pages = math.ceil(num_results / RESULTS_PER_PAGE)
    current_page = 1

    while True:
        start_idx, end_idx = (current_page - 1) * RESULTS_PER_PAGE, current_page * RESULTS_PER_PAGE
        page_results = results[start_idx:end_idx]

        print_colored(f"\n--- ç»“æœé¢„è§ˆ (ç¬¬ {current_page}/{total_pages} é¡µ) ---", Colors.HEADER)
        for i, paper in enumerate(page_results, start=start_idx + 1):
            title, authors, conf, year = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            display_line = f"  {Colors.OKCYAN}{conf} {year}{Colors.ENDC} | ä½œè€…: {textwrap.shorten(authors, 70)}"
            if 'similarity' in paper: display_line = f"  {Colors.OKGREEN}ç›¸ä¼¼åº¦: {paper['similarity']:.2f}{Colors.ENDC} |" + display_line
            print(f"\n{Colors.BOLD}[{i}]{Colors.ENDC} {title}\n{display_line}")

        if current_page >= total_pages: print("\n--- å·²æ˜¯æœ€åä¸€é¡µ ---"); break
        try:
            choice = input(
                f"\næŒ‰ {Colors.BOLD}[Enter]{Colors.ENDC} ä¸‹ä¸€é¡µ, '{Colors.BOLD}s{Colors.ENDC}' ä¿å­˜, '{Colors.BOLD}ai{Colors.ENDC}' å¯¹ç»“æœæé—®, '{Colors.BOLD}q{Colors.ENDC}' è¿”å›: ").lower()
            if choice == 'q': return
            if choice == 's': break
            if choice == 'ai':
                start_ai_chat_session(results)
                print_colored("\n[i] AIå¯¹è¯ç»“æŸï¼Œè¿”å›ç»“æœåˆ—è¡¨ã€‚", Colors.OKBLUE)
                continue
            current_page += 1
        except KeyboardInterrupt:
            return

    if input(f"\næ˜¯å¦å°†è¿™ {num_results} æ¡ç»“æœå…¨éƒ¨ä¿å­˜åˆ° Markdown? (y/n, é»˜è®¤y): ").lower() != 'n':
        save_results_to_markdown(results, query)

# --- ä¸»ç¨‹åº (CLIå…¥å£) ---
def main():
    # ç¡®ä¿åœ¨mainå‡½æ•°å¼€å§‹æ—¶è°ƒç”¨åˆå§‹åŒ–ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å—åŠ è½½æ—¶
    initialize_components()

    if not _initialized: # æ£€æŸ¥åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        print_colored(f"[{Colors.FAIL}âœ–{Colors.ENDC}] ä¸¥é‡é”™è¯¯: æœç´¢åç«¯æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿è¡ŒCLIã€‚", Colors.FAIL)
        sys.exit(1)

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    print_banner()
    print_colored("\n--- æœç´¢è¯­æ³• (AIå·²é›†æˆ & FTS5å·²ä¿®æ­£) ---", Colors.OKBLUE)
    print("  - `transformer author:vaswani`   (å…³é”®è¯ + ä½œè€…å­—æ®µæœç´¢)")
    print("  - `title:\"vision transformer\"`     (ç²¾ç¡®æ ‡é¢˜æœç´¢)")
    print("  - `\"large language model\" AND efficient` (çŸ­è¯­å’Œå…³é”®è¯ç»„åˆ)")
    print(f"  - `{Colors.BOLD}sem:{Colors.ENDC} efficiency of few-shot learning` (è¯­ä¹‰æœç´¢ï¼)")

    while True:
        try:
            q = input(f"\nğŸ” {Colors.BOLD}è¯·è¾“å…¥æŸ¥è¯¢{Colors.ENDC} (æˆ– 'exit' é€€å‡º): ").strip()
            if not q: continue
            if q.lower() == 'exit': break

            results = []
            if q.lower().startswith('sem:'):
                semantic_query = q[4:].strip()
                if semantic_query: results, _ = semantic_search(semantic_query)
            else:
                results, _ = keyword_search(q)

            interactive_pagination_cli(results, q, session_dir)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", Colors.FAIL)

    if _sqlite_conn:
        _sqlite_conn.close()
        print_colored("\n[âœ”] SQLiteè¿æ¥å·²å…³é—­ã€‚", Colors.OKBLUE)
    print("\nå†è§ï¼")


if __name__ == "__main__":
    main()

==================== End of: src\search\search_ai_assistant.py ====================



==================== Start of: src\search\search_service.py ====================

# FILE: src/search/search_service.py (Core Backend Services - v1.1)

import sqlite3
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pathlib import Path
import time
import re
import torch
from datetime import datetime
from collections import Counter
import os
from dotenv import load_dotenv
from zai import ZhipuAiClient
from typing import List, Dict, Any, Optional, Tuple

# --- å…¨å±€é…ç½® (ç»Ÿä¸€ç®¡ç†ï¼Œå…¶ä»–æ¨¡å—é€šè¿‡å¯¼å…¥è¿™ä¸ªæ–‡ä»¶æ¥è®¿é—®) ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
SEARCH_RESULTS_DIR = PROJECT_ROOT / "search_results"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"
RESULTS_PER_PAGE = 10  # ç”¨äºåˆ†é¡µçš„é»˜è®¤å€¼
AI_CONTEXT_PAPERS = 5  # æ¯æ¬¡æé—®æ—¶ï¼Œå‘é€ç»™AIçš„æœ€ç›¸å…³çš„è®ºæ–‡æ•°é‡

# --- åŠ è½½ç¯å¢ƒå˜é‡ ---
load_dotenv(PROJECT_ROOT / '.env')
ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY")

# --- å…¨å±€å¯è®¿é—®çš„åç«¯ç»„ä»¶å®ä¾‹ (ä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼Œé€šè¿‡ initialize_components å‡½æ•°åˆå§‹åŒ–) ---
_sqlite_conn: Optional[sqlite3.Connection] = None
_sentence_transformer_model: Optional[SentenceTransformer] = None
_chroma_collection: Optional[chromadb.api.models.Collection.Collection] = None
_zhipu_ai_client: Optional[ZhipuAiClient] = None
_ai_enabled: bool = False
_initialized: bool = False  # æ ‡è®°æ˜¯å¦å·²åˆå§‹åŒ–


# --- é¢œè‰²å®šä¹‰ (ä¿ç•™åœ¨æœåŠ¡å±‚ï¼Œä½œä¸ºé€šç”¨å¸¸é‡) ---
class Colors:
    HEADER = '\033[95m';
    OKBLUE = '\033[94m';
    OKCYAN = '\033[96m';
    OKGREEN = '\033[92m'
    WARNING = '\033[93m';
    FAIL = '\033[91m';
    ENDC = '\033[0m';
    BOLD = '\033[1m';
    UNDERLINE = '\033[4m'


# --- åˆå§‹åŒ–å‡½æ•° (æ‰€æœ‰åç«¯ç»„ä»¶çš„å•ä¸€å…¥å£ï¼Œç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡) ---
def initialize_components() -> None:
    """
    åˆå§‹åŒ–æ‰€æœ‰æœç´¢å’ŒAIåç«¯ç»„ä»¶ã€‚ç¡®ä¿åªè¿è¡Œä¸€æ¬¡ã€‚
    æ­¤å‡½æ•°ä¼šæ‰“å°çŠ¶æ€ä¿¡æ¯ï¼Œä½†é¢œè‰²å’Œè¾“å‡ºæ–¹å¼ç”±è°ƒç”¨è€…å†³å®šã€‚
    """
    global _sqlite_conn, _sentence_transformer_model, _chroma_collection, _zhipu_ai_client, _ai_enabled, _initialized

    if _initialized:
        # print("æœç´¢åç«¯æœåŠ¡å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–ã€‚") # è°ƒè¯•ç”¨
        return

    print(f"[{Colors.OKBLUE}*{Colors.ENDC}] æ­£åœ¨åˆå§‹åŒ–æœç´¢åç«¯æœåŠ¡...")

    # 1. åˆå§‹åŒ–SQLiteè¿æ¥
    try:
        _sqlite_conn = sqlite3.connect(str(DB_PATH), uri=True, check_same_thread=False)
        print(f"[{Colors.OKGREEN}âœ”{Colors.ENDC}] SQLiteæ•°æ®åº“ '{DB_PATH.name}' è¿æ¥æˆåŠŸã€‚")
    except Exception as e:
        print(f"[{Colors.FAIL}âœ–{Colors.ENDC}] é”™è¯¯: æ— æ³•è¿æ¥SQLiteæ•°æ®åº“: {e}")
        _sqlite_conn = None
        _initialized = True
        return

        # 2. åˆå§‹åŒ–SentenceTransformeræ¨¡å‹å’ŒChromaDB
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        _sentence_transformer_model = SentenceTransformer(MODEL_NAME, device=device)
        print(f"[{Colors.OKGREEN}âœ”{Colors.ENDC}] SentenceTransformeræ¨¡å‹ '{MODEL_NAME}' ({device}) åŠ è½½æˆåŠŸã€‚")

        _chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH, settings=Settings(anonymized_telemetry=False))
        _chroma_collection = _chroma_client.get_or_create_collection(name=COLLECTION_NAME,
                                                                     metadata={"hnsw:space": "cosine"})
        print(
            f"[{Colors.OKGREEN}âœ”{Colors.ENDC}] ChromaDBé›†åˆ '{COLLECTION_NAME}' ({_chroma_collection.count()} ä¸ªå‘é‡) å·²åŠ è½½ã€‚")
    except Exception as e:
        print(f"[{Colors.FAIL}âœ–{Colors.ENDC}] é”™è¯¯: æ— æ³•åˆå§‹åŒ–è¯­ä¹‰æœç´¢ç»„ä»¶: {e}")
        _sentence_transformer_model = None
        _chroma_collection = None
        _initialized = True
        if _sqlite_conn: _sqlite_conn.close()
        return

    # 3. åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯
    if ZHIPUAI_API_KEY:
        try:
            _zhipu_ai_client = ZhipuAiClient(api_key=ZHIPUAI_API_KEY)
            _ai_enabled = True
            print(f"[{Colors.OKGREEN}âœ”{Colors.ENDC}] æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            print(f"[{Colors.WARNING}âš {Colors.ENDC}] è­¦å‘Š: æ— æ³•åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯: {e}. AIå¯¹è¯åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
            _zhipu_ai_client = None
            _ai_enabled = False
    else:
        print(f"[{Colors.WARNING}âš {Colors.ENDC}] è­¦å‘Š: æœªè®¾ç½® ZHIPUAI_API_KEY. AIå¯¹è¯åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
        _ai_enabled = False

    _initialized = True
    print(f"[{Colors.OKBLUE}*{Colors.ENDC}] æœç´¢åç«¯æœåŠ¡åˆå§‹åŒ–å®Œæˆã€‚")


# --- æ ¸å¿ƒæœç´¢åŠŸèƒ½ ---

def keyword_search(raw_query: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    æ‰§è¡Œå…³é”®è¯æœç´¢ï¼Œè¿”å›ç»“æœåˆ—è¡¨å’Œç»Ÿè®¡æ‘˜è¦ã€‚
    """
    if not _initialized or _sqlite_conn is None:
        return [], {"error": "æœç´¢æœåŠ¡æœªåˆå§‹åŒ–æˆ–SQLiteè¿æ¥å¤±è´¥ã€‚"}

    COLUMN_MAP = {'author': 'authors', 'title': 'title', 'abstract': 'abstract'}
    parsed_query_parts = []
    pattern = re.compile(r'(\b\w+):(?:"([^"]*)"|(\S+))')
    remaining_query = raw_query

    for match in list(pattern.finditer(raw_query)):
        full_match_text = match.group(0)
        field_alias = match.group(1).lower()
        value = match.group(2) if match.group(2) is not None else match.group(3)
        if field_alias in COLUMN_MAP:
            db_column = COLUMN_MAP[field_alias]
            safe_value = value.replace('"', '""')
            if ' ' in value or not value.isalnum():
                parsed_query_parts.append(f'{db_column}:"{safe_value}"')
            else:
                parsed_query_parts.append(f'{db_column}:{safe_value}')
            remaining_query = re.sub(re.escape(full_match_text), '', remaining_query, 1)

    general_terms = re.findall(r'"[^"]*"|\S+', remaining_query.strip())
    for term in general_terms:
        safe_term = term.replace('"', '""')
        if term.startswith('"') and term.endswith('"'):
            parsed_query_parts.append(f'{safe_term}')
        else:
            parsed_query_parts.append(safe_term)

    final_fts_query = ' AND '.join(filter(None, parsed_query_parts))

    if not final_fts_query:
        return [], {"total_found": 0, "distribution": {}, "message": "å…³é”®è¯æœç´¢æŸ¥è¯¢ä¸ºç©ºæˆ–è§£æå¤±è´¥ã€‚"}

    try:
        cursor = _sqlite_conn.execute(
            "SELECT title, authors, abstract, conference, year FROM papers_fts WHERE papers_fts MATCH ? ORDER BY rank",
            (final_fts_query,)
        )
        raw_results = cursor.fetchall()
        results = [{"title": r[0], "authors": r[1], "abstract": r[2], "conference": r[3], "year": r[4]} for r in
                   raw_results]

        stats = get_stats_summary(results)
        stats['message'] = f"å…³é”®è¯æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ç¯‡ã€‚"
        return results, stats
    except sqlite3.OperationalError as e:
        return [], {"total_found": 0, "distribution": {},
                    "message": f"å…³é”®è¯æœç´¢å¤±è´¥: {e}. FTS5 Query: '{final_fts_query}'"}


def semantic_search(query: str, top_n: int = 20) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼Œè¿”å›ç»“æœåˆ—è¡¨å’Œç»Ÿè®¡æ‘˜è¦ã€‚
    """
    if not _initialized or _sentence_transformer_model is None or _chroma_collection is None or _sqlite_conn is None:
        return [], {"error": "æœç´¢æœåŠ¡æœªåˆå§‹åŒ–æˆ–ç»„ä»¶å¤±è´¥ã€‚"}

    start_t = time.time()
    query_embedding = _sentence_transformer_model.encode(query, convert_to_tensor=False)
    chroma_results = _chroma_collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_n)

    ids_found, distances = chroma_results['ids'][0], chroma_results['distances'][0]
    if not ids_found: return [], get_stats_summary([])

    placeholders = ','.join('?' for _ in ids_found)
    sql_query = f"SELECT rowid, title, authors, abstract, conference, year FROM papers_fts WHERE rowid IN ({placeholders})"
    cursor = _sqlite_conn.cursor()
    raw_sqlite_results = {str(r[0]): r[1:] for r in cursor.execute(sql_query, ids_found).fetchall()}

    final_results = []
    for i, paper_id in enumerate(ids_found):
        details = raw_sqlite_results.get(paper_id)
        if details:
            final_results.append({
                "title": details[0],
                "authors": details[1],
                "abstract": details[2],
                "conference": details[3],
                "year": details[4],
                "similarity": 1 - distances[i]
            })
    end_t = time.time()

    stats = get_stats_summary(final_results)
    stats['message'] = f"è¯­ä¹‰æœç´¢å®Œæˆ (è€—æ—¶: {end_t - start_t:.4f} ç§’, æ‰¾åˆ° {len(final_results)} ç¯‡)ã€‚"
    return final_results, stats


# --- è¾…åŠ©åŠŸèƒ½ (ä¸CLIå’ŒWeb UIå…±äº«) ---

def get_stats_summary(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ç”ŸæˆæŸ¥è¯¢ç»“æœçš„ç»Ÿè®¡æ‘˜è¦ã€‚
    """
    total_found = len(results)

    conf_year_counter = Counter([(p.get('conference', 'N/A'), p.get('year', 'N/A')) for p in results])
    distribution = {f"{conf} {year}": count for (conf, year), count in conf_year_counter.most_common()}

    return {"total_found": total_found, "distribution": distribution}


def format_papers_for_prompt(papers: List[Dict[str, Any]]) -> str:
    """å°†è®ºæ–‡åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ¸…æ™°çš„å­—ç¬¦ä¸²ï¼Œä½œä¸ºAIçš„ä¸Šä¸‹æ–‡ã€‚"""
    context = ""
    for i, paper in enumerate(papers, 1):
        context += f"[è®ºæ–‡ {i}]\n"
        context += f"æ ‡é¢˜: {paper.get('title', 'N/A')}\n"
        context += f"ä½œè€…: {paper.get('authors', 'N/A')}\n"
        context += f"æ‘˜è¦: {paper.get('abstract', 'N/A')}\n\n"
    return context


def save_results_to_markdown(results: List[Dict[str, Any]], query: str) -> str:
    """å°†æœç´¢ç»“æœä¿å­˜ä¸ºMarkdownæ–‡ä»¶ã€‚"""
    if not results: return "æ²¡æœ‰æœç´¢ç»“æœå¯ä¿å­˜ã€‚"

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# æœç´¢æŸ¥è¯¢: \"{query}\"\n\n**å…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title, authors, abstract = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('abstract',
                                                                                                         'N/A')
            conf, year = paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            similarity_str = f"- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {paper['similarity']:.2f}\n" if 'similarity' in paper else ""
            f.write(
                f"### {idx}. {title}\n\n- **ä½œè€…**: {authors}\n- **ä¼šè®®/å¹´ä»½**: {conf} {year}\n{similarity_str}\n**æ‘˜è¦:**\n> {abstract}\n\n---\n\n")

    return f"ç»“æœå·²ä¿å­˜åˆ°: {filename.resolve()}"


# --- AIå“åº”ç”Ÿæˆå™¨ (æœåŠ¡æ¨¡å—çš„æ ¸å¿ƒé€»è¾‘) ---
def generate_ai_response(chat_history: List[Dict[str, str]], search_results_context: List[Dict[str, Any]]) -> str:
    """
    æ ¹æ®æœç´¢ç»“æœä¸Šä¸‹æ–‡å’ŒèŠå¤©å†å²ç”ŸæˆAIå“åº”ã€‚
    chat_history: ä»…åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’ŒAIå“åº”ï¼Œä¸åŒ…å«ç³»ç»Ÿæ¶ˆæ¯å’Œåˆå§‹èƒŒæ™¯ã€‚
    search_results_context: åŸå§‹çš„è®ºæ–‡ç»“æœåˆ—è¡¨ã€‚
    """
    global _ai_enabled, _zhipu_ai_client  # ç¡®ä¿å¯ä»¥è®¿é—®å…¨å±€å˜é‡

    if not _ai_enabled or _zhipu_ai_client is None:
        return "[!] é”™è¯¯: AIå¯¹è¯åŠŸèƒ½æœªå¯ç”¨æˆ–æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ZHIPUAI_API_KEYã€‚"
    if not search_results_context:
        return "[!] æ²¡æœ‰å¯ä¾›AIå¯¹è¯çš„æœç´¢ç»“æœä¸Šä¸‹æ–‡ã€‚"

    context_papers = search_results_context[:AI_CONTEXT_PAPERS]
    formatted_context = format_papers_for_prompt(context_papers)

    full_messages = [
        {"role": "system",
         "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„è®ºæ–‡æ‘˜è¦ä¿¡æ¯ï¼Œç²¾å‡†ã€æ·±å…¥åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºæä¾›çš„ææ–™ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"},
        {"role": "user", "content": f"è¿™æ˜¯æˆ‘ä¸ºä½ æä¾›çš„èƒŒæ™¯çŸ¥è¯†ï¼Œè¯·ä»”ç»†é˜…è¯»ï¼š\n\n{formatted_context}"},
        {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å·²ç»ç†è§£äº†è¿™å‡ ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚è¯·é—®æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ"}
    ]
    full_messages.extend(chat_history)

    try:
        response_generator = _zhipu_ai_client.chat.completions.create(
            model="glm-4.5-flash",
            messages=full_messages,
            stream=True,
            temperature=0.7,
        )

        full_response_content = ""
        for chunk in response_generator:
            delta_content = chunk.choices[0].delta.content
            if delta_content:
                full_response_content += delta_content
        return full_response_content
    except Exception as e:
        return f"[!] è°ƒç”¨AIæ—¶å‡ºé”™: {e}"


# æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨åˆå§‹åŒ–ç»„ä»¶ (ç¡®ä¿åœ¨ä»»ä½•å‡½æ•°è¢«è°ƒç”¨å‰å®Œæˆ)
initialize_components()

==================== End of: src\search\search_service.py ====================



==================== Start of: src\search\search_ui.py ====================

# FILE: src/search/search_ai_assistant.py (v7.3 - AI Module Separated)

import sqlite3
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pathlib import Path
import textwrap
import time
import sys
import re
import torch
import math
from datetime import datetime
from collections import Counter
import os
from dotenv import load_dotenv

# --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: å¯¼å…¥æ–°çš„AIæœåŠ¡æ¨¡å— ---
from src.ai.glm_chat_service import start_ai_chat_session, print_colored, Colors

# ------------------------------------

# --- é…ç½® (å¤§éƒ¨åˆ†ä¸å˜ï¼Œä½†ä¸å†éœ€è¦AI_CONTEXT_PAPERSå’ŒZHIPUAI_API_KEYçš„å…¨å±€å¯¼å…¥) ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
DB_DIR = PROJECT_ROOT / "database"
SEARCH_RESULTS_DIR = PROJECT_ROOT / "search_results"
DB_PATH = DB_DIR / "papers.db"
CHROMA_DB_PATH = str(DB_DIR / "chroma_db")
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
COLLECTION_NAME = "papers"
RESULTS_PER_PAGE = 10


# --- ã€åˆ é™¤ã€‘: ä¸å†éœ€è¦åœ¨æ­¤æ–‡ä»¶ä¸­åŠ è½½ZHIPUAI_API_KEY ---
# ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY")
# --------------------------------------------------------------------------

# --- é¢œè‰²å’Œæ‰“å°å‡½æ•° (è¿™é‡Œåªéœ€ä¿ç•™print_bannerï¼Œprint_coloredå’ŒColorsä»ai_chat_serviceå¯¼å…¥) ---
# class Colors: ... (è¿™éƒ¨åˆ†å¯ä»¥ç›´æ¥åˆ é™¤ï¼Œå› ä¸ºå®ƒç°åœ¨æ˜¯ä»ai_chat_serviceå¯¼å…¥çš„)
# def print_colored(text, color): ... (è¿™éƒ¨åˆ†å¯ä»¥ç›´æ¥åˆ é™¤)

def print_banner():
    banner_text = "--- PubCrawler v7.3: AI Research Assistant (Module Refactored) ---"
    print_colored(banner_text, Colors.HEADER)
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    print_colored(f"[*] ç»“æœå°†ä¿å­˜è‡³: {SEARCH_RESULTS_DIR.resolve()}", Colors.UNDERLINE)


# --- æ–‡ä»¶ä¿å­˜ã€ç»Ÿè®¡ã€åˆ†é¡µæ¨¡å— (æ— å˜åŒ–ï¼Œä½†æ³¨æ„print_coloredç°åœ¨æ˜¯å¯¼å…¥çš„) ---
def save_results_to_markdown(results, query, session_dir):
    if not results: return []
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# æœç´¢æŸ¥è¯¢: \"{query}\"\n\n**å…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title, authors, abstract = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('abstract',
                                                                                                         'N/A')
            conf, year = paper.get('conference', 'N/A'), paper.get('year', 'N/A')
            similarity_str = f"- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {paper['similarity']:.2f}\n" if 'similarity' in paper else ""
            f.write(
                f"### {idx}. {title}\n\n- **ä½œè€…**: {authors}\n- **ä¼šè®®/å¹´ä»½**: {conf} {year}\n{similarity_str}\n**æ‘˜è¦:**\n> {abstract}\n\n---\n\n")
    print_colored(f"\n[âœ”] ç»“æœå·²æˆåŠŸä¿å­˜åˆ° Markdown æ–‡ä»¶!", Colors.OKGREEN)
    print_colored(f"      -> {filename.resolve()}", Colors.UNDERLINE)


def print_stats_summary(results):
    if not results: return
    total_found = len(results)
    print_colored("\n--- æŸ¥è¯¢ç»“æœç»Ÿè®¡ ---", Colors.HEADER)
    print(f"æ€»è®¡æ‰¾åˆ° {Colors.BOLD}{total_found}{Colors.ENDC} ç¯‡ç›¸å…³è®ºæ–‡ã€‚")
    conf_year_counter = Counter([(p.get('conference', 'N/A'), p.get('year', 'N/A')) for p in results])
    if conf_year_counter:
        print("åˆ†å¸ƒæƒ…å†µ:")
        for (conf, year), count in conf_year_counter.most_common():
            print(f"  - {conf} {year}: {count} ç¯‡")
    print_colored("--------------------", Colors.HEADER)


def interactive_pagination(results, query, session_dir):
    num_results = len(results)
    if num_results == 0: return
    print_stats_summary(results)
    total_pages, current_page = math.ceil(num_results / RESULTS_PER_PAGE), 1
    while True:
        start_idx, end_idx = (current_page - 1) * RESULTS_PER_PAGE, current_page * RESULTS_PER_PAGE
        page_results = results[start_idx:end_idx]
        print_colored(f"\n--- ç»“æœé¢„è§ˆ (ç¬¬ {current_page}/{total_pages} é¡µ) ---", Colors.HEADER)
        for i, paper in enumerate(page_results, start=start_idx + 1):
            title, authors, conf, year = paper.get('title', 'N/A'), paper.get('authors', 'N/A'), paper.get('conference',
                                                                                                           'N/A'), paper.get(
                'year', 'N/A')
            display_line = f"  {Colors.OKCYAN}{conf} {year}{Colors.ENDC} | ä½œè€…: {textwrap.shorten(authors, 70)}"
            if 'similarity' in paper: display_line = f"  {Colors.OKGREEN}ç›¸ä¼¼åº¦: {paper['similarity']:.2f}{Colors.ENDC} |" + display_line
            print(f"\n{Colors.BOLD}[{i}]{Colors.ENDC} {title}\n{display_line}")
        if current_page >= total_pages: print("\n--- å·²æ˜¯æœ€åä¸€é¡µ ---"); break
        try:
            choice = input(
                f"\næŒ‰ {Colors.BOLD}[Enter]{Colors.ENDC} ä¸‹ä¸€é¡µ, '{Colors.BOLD}s{Colors.ENDC}' ä¿å­˜, '{Colors.BOLD}ai{Colors.ENDC}' å¯¹ç»“æœæé—®, '{Colors.BOLD}q{Colors.ENDC}' è¿”å›: ").lower()
            if choice == 'q': return
            if choice == 's': break
            if choice == 'ai':
                start_ai_chat_session(results)  # <-- è°ƒç”¨æ–°çš„AIæœåŠ¡å‡½æ•°
                print_colored("\n[i] AIå¯¹è¯ç»“æŸï¼Œè¿”å›ç»“æœåˆ—è¡¨ã€‚", Colors.OKBLUE)
                continue
            current_page += 1
        except KeyboardInterrupt:
            return
    if input(f"\næ˜¯å¦å°†è¿™ {num_results} æ¡ç»“æœå…¨éƒ¨ä¿å­˜åˆ° Markdown? (y/n, é»˜è®¤y): ").lower() != 'n':
        save_results_to_markdown(results, query, session_dir)


# --- æœç´¢æ ¸å¿ƒ (æ— å˜åŒ–) ---
def keyword_search(conn, raw_query):
    COLUMN_MAP = {'author': 'authors', 'title': 'title', 'abstract': 'abstract'}
    parsed_query_parts = []
    pattern = re.compile(r'(\b\w+):(?:"([^"]*)"|(\S+))')
    remaining_query = raw_query
    for match in list(pattern.finditer(raw_query)):
        full_match_text = match.group(0)
        field_alias = match.group(1).lower()
        value = match.group(2) if match.group(2) is not None else match.group(3)
        if field_alias in COLUMN_MAP:
            db_column = COLUMN_MAP[field_alias]
            safe_value = value.replace('"', '""')
            if ' ' in value or not value.isalnum():
                parsed_query_parts.append(f'{db_column}:"{safe_value}"')
            else:
                parsed_query_parts.append(f'{db_column}:{safe_value}')
            remaining_query = re.sub(re.escape(full_match_text), '', remaining_query, 1)
    general_terms = re.findall(r'"[^"]*"|\S+', remaining_query.strip())
    for term in general_terms:
        safe_term = term.replace('"', '""')
        if term.startswith('"') and term.endswith('"'):
            parsed_query_parts.append(f'{safe_term}')
        else:
            parsed_query_parts.append(safe_term)
    final_fts_query = ' AND '.join(filter(None, parsed_query_parts))
    if not final_fts_query:
        print_colored("[!] å…³é”®è¯æœç´¢æŸ¥è¯¢ä¸ºç©ºæˆ–è§£æå¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œã€‚", Colors.WARNING);
        return []
    print(f"[*] æ­£åœ¨æ‰§è¡Œå…³é”®è¯æœç´¢ (FTS5 Query: '{final_fts_query}')...")
    try:
        cursor = conn.execute(
            "SELECT title, authors, abstract, conference, year FROM papers_fts WHERE papers_fts MATCH ? ORDER BY rank",
            (final_fts_query,))
        results = [{"title": r[0], "authors": r[1], "abstract": r[2], "conference": r[3], "year": r[4]} for r in
                   cursor.fetchall()]
        return results
    except sqlite3.OperationalError as e:
        print_colored(f"[!] å…³é”®è¯æœç´¢å¤±è´¥: {e}", Colors.FAIL)
        print_colored(f"    åŸå§‹æŸ¥è¯¢: '{raw_query}'", Colors.WARNING)
        print_colored(f"    ç”Ÿæˆçš„FTS5æŸ¥è¯¢: '{final_fts_query}'", Colors.WARNING)
        print_colored("    æç¤º: FTS5æŸ¥è¯¢è¯­æ³•ä¸¥æ ¼ï¼Œè¯·æ£€æŸ¥ 'AND/OR/NOT', çŸ­è¯­å¼•å·æˆ–å­—æ®µåæ˜¯å¦æœ‰è¯¯ã€‚", Colors.WARNING);
        return []


def semantic_search(conn, collection, model, query, top_n=20):
    print(f"[*] æ­£åœ¨æ‰§è¡Œè¯­ä¹‰æœç´¢: '{query}'...")
    start_t = time.time()
    query_embedding = model.encode(query, convert_to_tensor=False)
    chroma_results = collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_n)
    ids_found, distances = chroma_results['ids'][0], chroma_results['distances'][0]
    if not ids_found: return []
    placeholders = ','.join('?' for _ in ids_found)
    sql_query = f"SELECT rowid, title, authors, abstract, conference, year FROM papers_fts WHERE rowid IN ({placeholders})"
    cursor = conn.cursor()
    raw_sqlite_results = {str(r[0]): r[1:] for r in cursor.execute(sql_query, ids_found).fetchall()}
    final_results = []
    for i, paper_id in enumerate(ids_found):
        details = raw_sqlite_results.get(paper_id)
        if details:
            final_results.append(
                {"title": details[0], "authors": details[1], "abstract": details[2], "conference": details[3],
                 "year": details[4], "similarity": 1 - distances[i]})
    end_t = time.time()
    print(f"[âœ”] è€—æ—¶ {end_t - start_t:.4f} ç§’ï¼Œæ‰¾åˆ°å¹¶ç»„åˆäº† {len(final_results)} ä¸ªç»“æœã€‚")
    return final_results


# --- ä¸»ç¨‹åº (éœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œå› ä¸ºprint_coloredå’ŒColorsç°åœ¨ä»å¤–éƒ¨å¯¼å…¥) ---
def main():
    # ä¹‹å‰è¿™é‡Œæ˜¯æ£€æŸ¥ZHIPUAI_API_KEYï¼Œç°åœ¨ç”±ai_chat_service.pyå†…éƒ¨æ£€æŸ¥

    try:
        conn = sqlite3.connect(f"file:{DB_PATH}?mode=ro", uri=True)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = SentenceTransformer(MODEL_NAME, device=device)
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH, settings=Settings(anonymized_telemetry=False))
        collection = client.get_collection(name=COLLECTION_NAME)
    except Exception as e:
        print_colored(f"\n[!] æ— æ³•åˆå§‹åŒ–æ¨¡å—: {e}", Colors.FAIL); return

    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)

    print_banner()
    print_colored("\n--- æœç´¢è¯­æ³• (AIå·²é›†æˆ & FTS5å·²ä¿®æ­£) ---", Colors.OKBLUE)
    print("  - `transformer author:vaswani`   (å…³é”®è¯ + ä½œè€…å­—æ®µæœç´¢)")
    print("  - `title:\"vision transformer\"`     (ç²¾ç¡®æ ‡é¢˜æœç´¢)")
    print("  - `\"large language model\" AND efficient` (çŸ­è¯­å’Œå…³é”®è¯ç»„åˆ)")
    print(f"  - `{Colors.BOLD}sem:{Colors.ENDC} efficiency of few-shot learning` (è¯­ä¹‰æœç´¢ï¼)")

    # last_results ä¸å†éœ€è¦åœ¨æ­¤å¤„å­˜å‚¨ï¼Œå› ä¸º ai_chat_session ç›´æ¥æ¥æ”¶ç»“æœ

    while True:
        try:
            q = input(f"\nğŸ” {Colors.BOLD}è¯·è¾“å…¥æŸ¥è¯¢{Colors.ENDC} (æˆ– 'exit' é€€å‡º): ").strip()
            if not q: continue
            if q.lower() == 'exit': break

            results = []
            if q.lower().startswith('sem:'):
                semantic_query = q[4:].strip()
                if semantic_query: results = semantic_search(conn, collection, model, semantic_query)
            else:
                results = keyword_search(conn, q)

            # last_results = results # ä¸å†éœ€è¦èµ‹å€¼ç»™ last_results
            interactive_pagination(results, q, session_dir)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print_colored(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", Colors.FAIL)

    conn.close()
    print("\nå†è§ï¼")


if __name__ == "__main__":
    main()

==================== End of: src\search\search_ui.py ====================



==================== Start of: src\search\__init__.py ====================

# FILE: src/search/__init__.py
# Makes 'search' a Python package.

==================== End of: src\search\__init__.py ====================



==================== Start of: src\test\logo.py ====================

import time

# --- Logo å®šä¹‰ ---
# æˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰5ä¸ªç‰ˆæœ¬çš„Logo
# ä½¿ç”¨ \033[xxm ANSI è½¬ä¹‰åºåˆ—æ¥è®¾ç½®é¢œè‰²
# \033[0m æ˜¯é‡ç½®æ‰€æœ‰é¢œè‰²å’Œæ ·å¼
# \033[1m æ˜¯åŠ ç²—
# \033[9_m æ˜¯äº®è‰² (91=äº®çº¢, 92=äº®ç»¿, 93=äº®é»„, 94=äº®è“, 95=äº®æ´‹çº¢, 96=äº®é’è‰², 97=äº®ç™½)

v1_name = "V1: ç§‘æŠ€-æ•°æ®æµ (äº®ç»¿è‰²)"
v1_art = """
\033[92m
>> > P U B C R A W L E R >
[#]=======================[#]
>> > 1011010100101011010 >
\033[0m
"""

v2_name = "V2: çˆ¬è™«-èœ˜è››ç½‘ (äº®ç™½/äº®çº¢)"
v2_art = """
\033[97m
      /  \\
     /    \\
\033[91m \ \ \033[97m( PubCrawler )\033[91m / /
\033[97m  \ \ \  / / /  / /
   \ \/ /  \ \/ /
    \/ /    \/ /
\033[0m
"""

v3_name = "V3: ç°ä»£-ç»“æ„å— (äº®æ´‹çº¢/äº®é»„)"
v3_art = """
\033[95m
[P] [U] [B]
 \   |   /
\033[93m  [C]-[R]-[A]
\033[95m   /  |    \\
\033[93m [W]-[L]-[E]-[R]
\033[0m
"""

v4_name = "V4: ç®€æ´-ç”µè·¯æ¿ (äº®è“è‰²/äº®ç™½)"
v4_art = """
\033[94m
|--[P]--|--[U]--|--[B]--|
|        |       |
|__      |__     |__
   |        |       |
|--[C]--|--[R]--|--[A]--|
|        |       |
|__      |__     |__
   |        |       |
|--[W]--|--[L]--|--[E]--|--[R]--|
\033[97m
... data ... crawl ... analyze ...
\033[0m
"""

v5_name = "V5: å¤§æ°”-ä¿¡æ¯æ¡† (äº®ç™½/äº®é’è‰²)"
v5_art = """
\033[97m
+--------------------------+
|                          |
|  \033[96mP U B C R A W L E R\033[97m     |
|      \033[97m...initializing...  |
+--------------------------+
\033[0m
"""

# --- å­˜å‚¨å’Œæ‰“å° ---

logos_to_display = [
    (v1_name, v1_art),
    (v2_name, v2_art),
    (v3_name, v3_art),
    (v4_name, v4_art),
    (v5_name, v5_art)
]

def display_logos():
    """
    ä¾æ¬¡æ‰“å°æ‰€æœ‰Logoï¼Œå¸¦æš‚åœã€‚
    """
    print("æ­£åœ¨ä¸ºæ‚¨å±•ç¤º5æ¬¾ 'PubCrawler' ç§‘æŠ€æ„ŸLogoè®¾è®¡...")
    print("ï¼ˆè¯·ç¡®ä¿æ‚¨çš„æ§åˆ¶å°æ”¯æŒANSIé¢œè‰²ä»£ç ä»¥è·å¾—æœ€ä½³æ•ˆæœï¼‰")
    time.sleep(2)

    for name, art in logos_to_display:
        print("\n" * 4)  # æ‰“å°å‡ ä¸ªæ¢è¡Œç¬¦æ¥åˆ†éš”
        # \033[1m æ˜¯åŠ ç²—
        print(f"\033[1m--- {name} ---\033[0m")
        print(art)
        print(f"\033[1m--- End of {name.split(':')[0]} ---\033[0m")
        time.sleep(2.5) # æš‚åœ2.5ç§’

    print("\n" * 4)
    print("å…¨éƒ¨ç‰ˆæœ¬å±•ç¤ºå®Œæ¯•ï¼")
    print("æ‚¨å¯ä»¥ä»ä¸Šé¢çš„5ä¸ªç‰ˆæœ¬ä¸­é€‰æ‹©ä¸€ä¸ªï¼Œå°†å¯¹åº”çš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ v1_artï¼‰å¤åˆ¶åˆ°æ‚¨çš„é¡¹ç›®ä»£ç ä¸­ã€‚")

# --- ä¸»ç¨‹åºæ‰§è¡Œ ---
if __name__ == "__main__":
    display_logos()

==================== End of: src\test\logo.py ====================



==================== Start of: src\test\test_acl.py ====================

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ==============================================================================
# --- å®éªŒé…ç½® ---
# ==============================================================================

# 1. è®¾ç½®ç”¨äºæµ‹è¯•çš„çº¿ç¨‹æ•°åˆ—è¡¨ã€‚è„šæœ¬å°†ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªå€¼è¿è¡Œä¸€æ¬¡æµ‹è¯•ã€‚
#    å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªåˆ—è¡¨ï¼Œä¾‹å¦‚å¢åŠ  40, 56 ç­‰ã€‚
THREADS_TO_TEST = [4, 8, 12, 16, 24, 32, 48, 64]

# 2. é€‰æ‹©ä¸€ä¸ªå›ºå®šçš„å¹´ä»½è¿›è¡Œæµ‹è¯•ï¼Œä»¥ä¿è¯æ¯æ¬¡æµ‹è¯•çš„å·¥ä½œé‡ä¸€è‡´ã€‚
#    å»ºè®®é€‰æ‹©ä¸€ä¸ªè®ºæ–‡æ•°é‡è¾ƒå¤šçš„å¹´ä»½ï¼Œå¦‚ 2024 æˆ– 2025ã€‚
YEAR_FOR_TESTING = 2024

# 3. è®¾ç½®ç”¨äºæµ‹è¯•çš„è®ºæ–‡æ•°é‡ã€‚æ•°é‡ä¸å®œè¿‡å°‘ï¼ˆæ— æ³•ä½“ç°å·®è·ï¼‰ï¼Œä¹Ÿä¸å®œè¿‡å¤šï¼ˆæµ‹è¯•æ—¶é—´å¤ªé•¿ï¼‰ã€‚
#    100-200 æ˜¯ä¸€ä¸ªæ¯”è¾ƒç†æƒ³çš„èŒƒå›´ã€‚
PAPERS_FOR_TESTING = 150

# ==============================================================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
ACL_BASE_URL_PATTERN = "https://aclanthology.org/volumes/{year}.acl-long/"


def get_paper_links_for_workload(year: int, limit: int):
    """
    è·å–ä¸€ä¸ªå›ºå®šçš„å·¥ä½œè´Ÿè½½ï¼ˆè®ºæ–‡é“¾æ¥åˆ—è¡¨ï¼‰ç”¨äºæ‰€æœ‰æµ‹è¯•ã€‚
    """
    target_url = ACL_BASE_URL_PATTERN.format(year=year)
    print(f"[*] å‡†å¤‡å®éªŒç¯å¢ƒ: æ­£åœ¨ä» {target_url} è·å–è®ºæ–‡åˆ—è¡¨...")

    try:
        response = requests.get(target_url, headers=HEADERS, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'lxml')
        link_tags = soup.select('p.d-sm-flex strong a.align-middle')
        paper_links = [urljoin(target_url, tag['href']) for tag in link_tags if f'{year}.acl-long.0' not in tag['href']]

        actual_found = len(paper_links)
        print(f"[*] æ‰¾åˆ°äº† {actual_found} ç¯‡æœ‰æ•ˆè®ºæ–‡ã€‚")

        # æ™ºèƒ½é™åˆ¶ï¼šç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œä½†åˆä¸è¶…è¿‡å®é™…æ•°é‡
        actual_limit = min(limit, actual_found)
        if actual_limit < limit:
            print(f"[*] [è­¦å‘Š] æœŸæœ›æµ‹è¯• {limit} ç¯‡ï¼Œä½†åªæ‰¾åˆ° {actual_found} ç¯‡ã€‚å°†ä»¥ {actual_limit} ç¯‡ä¸ºå‡†ã€‚")

        print(f"[*] å®éªŒå·¥ä½œè´Ÿè½½å·²ç¡®å®š: {actual_limit} ç¯‡è®ºæ–‡ã€‚")
        return paper_links[:actual_limit]
    except requests.RequestException as e:
        print(f"[!] [é”™è¯¯] å‡†å¤‡å·¥ä½œè´Ÿè½½å¤±è´¥: {e}")
        return None


def scrape_single_paper_details(url: str):
    """çˆ¬å–å•ä¸ªè¯¦æƒ…é¡µçš„æ ¸å¿ƒå‡½æ•°ã€‚åœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬åªå…³å¿ƒå®ƒæ˜¯å¦æˆåŠŸå®Œæˆã€‚"""
    try:
        # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶ï¼Œå› ä¸ºå¹¶å‘æ—¶ç½‘ç»œå¯èƒ½ä¼šæ‹¥å µ
        response = requests.get(url, headers=HEADERS, timeout=25)
        response.raise_for_status()
        # è¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦è§£æï¼Œåªéœ€è¦ç¡®ä¿è¯·æ±‚æˆåŠŸè¿”å›å³å¯æ¨¡æ‹ŸçœŸå®è€—æ—¶
        return True
    except Exception:
        return False


def run_single_test(worker_count: int, urls_to_crawl: list):
    """
    ä½¿ç”¨æŒ‡å®šçš„çº¿ç¨‹æ•°ï¼Œå¯¹ç»™å®šçš„URLåˆ—è¡¨æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬å–æµ‹è¯•ã€‚
    """
    print("\n" + "-" * 60)
    print(f"ğŸ§ª æ­£åœ¨æµ‹è¯•: {worker_count} ä¸ªå¹¶å‘çº¿ç¨‹...")

    start_time = time.time()

    with ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(scrape_single_paper_details, url) for url in urls_to_crawl]

        # ä½¿ç”¨tqdmæ¥å¯è§†åŒ–è¿›åº¦
        for _ in tqdm(as_completed(futures), total=len(urls_to_crawl), desc=f"   - è¿›åº¦ ({worker_count}çº¿ç¨‹)"):
            pass

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"   -> å®Œæˆ! è€—æ—¶: {elapsed_time:.2f} ç§’")
    return elapsed_time


def main():
    """ä¸»å‡½æ•°ï¼Œè°ƒåº¦æ‰€æœ‰æµ‹è¯•å¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚"""
    print("=" * 60)
    print("      å¹¶å‘çº¿ç¨‹æ•°æ€§èƒ½ä¼˜åŒ–å™¨ for ACL Crawler")
    print("=" * 60)

    # 1. å‡†å¤‡ä¸€ä¸ªå›ºå®šçš„ã€ç”¨äºæ‰€æœ‰æµ‹è¯•çš„å·¥ä½œè´Ÿè½½
    workload_urls = get_paper_links_for_workload(YEAR_FOR_TESTING, PAPERS_FOR_TESTING)
    if not workload_urls:
        print("[!] æ— æ³•ç»§ç»­æµ‹è¯•ï¼Œå› ä¸ºæœªèƒ½è·å–åˆ°è®ºæ–‡åˆ—è¡¨ã€‚")
        return

    # 2. å¾ªç¯æ‰§è¡Œæµ‹è¯•
    experiment_results = []
    for num_threads in THREADS_TO_TEST:
        duration = run_single_test(num_threads, workload_urls)
        experiment_results.append({
            "threads": num_threads,
            "time": duration
        })
        # åœ¨æ¯æ¬¡æµ‹è¯•é—´æ­‡2ç§’ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿ç»­å†²å‡»
        time.sleep(2)

    # 3. åˆ†æç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
    if not experiment_results:
        print("[!] æ²¡æœ‰å®Œæˆä»»ä½•æµ‹è¯•ã€‚")
        return

    print("\n\n" + "#" * 60)
    print("ğŸ“Š            æœ€ç»ˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print(f"            (æµ‹è¯•è´Ÿè½½: {len(workload_urls)} ç¯‡è®ºæ–‡)")
    print("#" * 60)
    print(f"{'çº¿ç¨‹æ•°':<10} | {'æ€»è€—æ—¶ (ç§’)':<15} | {'æ¯ç§’çˆ¬å–è®ºæ–‡æ•°':<20}")
    print("-" * 60)

    best_result = None
    best_performance = 0

    for res in experiment_results:
        threads = res['threads']
        total_time = res['time']

        if total_time > 0:
            papers_per_second = len(workload_urls) / total_time
            print(f"{threads:<10} | {total_time:<15.2f} | {papers_per_second:<20.2f}")

            if papers_per_second > best_performance:
                best_performance = papers_per_second
                best_result = res
        else:
            print(f"{threads:<10} | {total_time:<15.2f} | {'N/A'}")

    print("-" * 60)

    # 4. ç»™å‡ºæœ€ç»ˆå»ºè®®
    if best_result:
        optimal_threads = best_result['threads']
        print("\nğŸ† ç»“è®º:")
        print(f"æ ¹æ®æœ¬æ¬¡åœ¨æ‚¨å½“å‰ç½‘ç»œç¯å¢ƒä¸‹çš„å®æµ‹ç»“æœï¼š")
        print(f"å½“çº¿ç¨‹æ•°è®¾ç½®ä¸º **{optimal_threads}** æ—¶ï¼Œçˆ¬å–æ•ˆç‡æœ€é«˜ï¼Œè¾¾åˆ°äº†æ¯ç§’ **{best_performance:.2f}** ç¯‡è®ºæ–‡ã€‚")
        print(f"å»ºè®®æ‚¨åœ¨ PubCrawler çš„ YAML é…ç½®æ–‡ä»¶ä¸­å°† ACL å’Œ CVF ä»»åŠ¡çš„ `max_workers` è®¾ç½®ä¸º **{optimal_threads}**ã€‚")
    else:
        print("\n[!] æœªèƒ½ç¡®å®šæœ€ä½³çº¿ç¨‹æ•°ã€‚")

    print("#" * 60)


if __name__ == "__main__":
    main()

==================== End of: src\test\test_acl.py ====================



==================== Start of: src\utils\console_logger.py ====================

# FILE: src/utils/console_logger.py (Banner Updated to Tech Pattern)

import logging
import sys

# å°è¯•å¯¼å…¥ coloramaï¼Œå¦‚æœå¤±è´¥åˆ™ä¼˜é›…é™çº§
try:
    import colorama
    from colorama import Fore, Style, Back

    colorama.init(autoreset=True)

    # å®šä¹‰é¢œè‰²å¸¸é‡
    COLORS = {
        'DEBUG': Style.DIM + Fore.WHITE,
        'INFO': Style.NORMAL + Fore.WHITE,
        'WARNING': Style.BRIGHT + Fore.YELLOW,
        'ERROR': Style.BRIGHT + Fore.RED,
        'CRITICAL': Style.BRIGHT + Back.RED + Fore.WHITE,
        'RESET': Style.RESET_ALL,

        # è‡ªå®šä¹‰é¢œè‰²ï¼Œç”¨äºç‰¹æ®Šé«˜äº®
        'BANNER_BLUE': Style.BRIGHT + Fore.BLUE,
        'BANNER_CYAN': Style.BRIGHT + Fore.CYAN,
        'BANNER_GREEN': Style.BRIGHT + Fore.GREEN,
        'BANNER_WHITE': Style.BRIGHT + Fore.WHITE,
        'PHASE': Style.BRIGHT + Fore.BLUE,
        'TASK_START': Style.BRIGHT + Fore.MAGENTA,
        'SUCCESS': Style.BRIGHT + Fore.GREEN,
        'STEP': Style.DIM + Fore.WHITE,  # <-- æˆ‘ä»¬ä¼šç”¨è¿™ä¸ª
    }

    IS_COLORAMA_AVAILABLE = True

except ImportError:
    # å¦‚æœæ²¡æœ‰å®‰è£… coloramaï¼Œåˆ™æ‰€æœ‰é¢œè‰²ä»£ç éƒ½ä¸ºç©ºå­—ç¬¦ä¸²
    COLORS = {key: '' for key in
              ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL', 'RESET', 'BANNER_BLUE', 'BANNER_CYAN', 'BANNER_GREEN',
               'BANNER_WHITE', 'PHASE', 'TASK_START', 'SUCCESS',
               'STEP']}
    IS_COLORAMA_AVAILABLE = False


class ColoredFormatter(logging.Formatter):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„æ—¥å¿—æ ¼å¼åŒ–å™¨ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºä¸­æ·»åŠ é¢œè‰²ã€‚
    """

    def __init__(self, fmt, datefmt=None, style='%'):
        super().__init__(fmt, datefmt, style)

    def format(self, record):
        # è·å–åŸå§‹çš„æ—¥å¿—æ¶ˆæ¯
        log_message = super().format(record)

        if IS_COLORAMA_AVAILABLE:
            # æ ¹æ®æ—¥å¿—çº§åˆ«åº”ç”¨ä¸åŒçš„é¢œè‰²
            level_color = COLORS.get(record.levelname, COLORS['INFO'])
            return f"{level_color}{log_message}{COLORS['RESET']}"
        else:
            return log_message


def print_banner():
    """æ‰“å°é¡¹ç›®å¯åŠ¨çš„ ASCII Art æ¨ªå¹… (ç§‘æŠ€æ„Ÿå›¾æ¡ˆç‰ˆ)ã€‚"""

    # -------------------ã€ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œã€‘-------------------
    # æŒ‰ç”¨æˆ·è¦æ±‚ï¼Œæ”¹ä¸ºç§‘æŠ€æ„Ÿå›¾æ¡ˆï¼Œæ”¾å¼ƒå¤§å­—æ¯å—
    # V7: ç§‘æŠ€-ç½‘ç»œèŠ‚ç‚¹ (äº®è“è‰²/äº®ç™½è‰²)

    # æˆ‘ä»¬ä½¿ç”¨ f-string æ¥åµŒå…¥é¢œè‰²ä»£ç 
    banner_art = f"""
{COLORS['BANNER_BLUE']}
        .--.
       / .. \\
    --(  PC  )--  {COLORS['BANNER_WHITE']}PubCrawler{COLORS['RESET']}
       \\ .. /    {COLORS['STEP']}[Initializing...]{COLORS['RESET']}
        '--'
{COLORS['RESET']}
"""

    # -------------------ã€ä¿®æ”¹ç»“æŸã€‘-------------------

    if IS_COLORAMA_AVAILABLE:
        print(banner_art)  # ç›´æ¥æ‰“å°åŒ…å«é¢œè‰²çš„ f-string

    else:
        # å¦‚æœ colorama ä¸å¯ç”¨ï¼Œæ‰“å°ä¸€ä¸ªæ‰‹åŠ¨å»é™¤é¢œè‰²ä»£ç çš„æ— è‰²ç‰ˆæœ¬
        no_color_art = r"""
        .--.
       / .. \
    --(  PC  )--  PubCrawler
       \ .. /    [Initializing...]
        '--'
"""
        print(no_color_art)

==================== End of: src\utils\console_logger.py ====================



==================== Start of: src\utils\downloader.py ====================

# FILE: src/utils/downloader.py (Tqdm Removed Version)

import requests
import re
from pathlib import Path

from src.crawlers.config import get_logger

logger = get_logger(__name__)

def download_single_pdf(paper: dict, pdf_dir: Path):
    """
    Downloads a single PDF file. This function is now designed to be called within a loop
    controlled by an external tqdm instance.
    """
    pdf_url = paper.get('pdf_url')
    title = paper.get('title', 'untitled')

    if not pdf_url:
        logger.warning(f"    -> Skipping download (no PDF URL): {title[:50]}...")
        return False

    sanitized_title = re.sub(r'[\\/*?:"<>|]', "", title).replace('\n', ' ').replace('\r', '')
    filename = (sanitized_title[:150] + ".pdf")
    filepath = pdf_dir / filename

    if filepath.exists():
        return True # Skip if already exists

    try:
        response = requests.get(pdf_url, stream=True, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except requests.exceptions.RequestException as e:
        logger.error(f"    [âœ– ERROR] Failed to download {pdf_url}. Reason: {e}")
        if filepath.exists(): filepath.unlink() # Clean up failed download
        return False
    except Exception as e:
        logger.error(f"    [âœ– ERROR] An unexpected error occurred for {pdf_url}. Reason: {e}")
        if filepath.exists(): filepath.unlink()
        return False

==================== End of: src\utils\downloader.py ====================



==================== Start of: src\utils\formatter.py ====================

# FILE: src/utils/formatter.py

import pandas as pd
from pathlib import Path
from datetime import datetime


def save_as_markdown(papers: list, task_name: str, output_dir: Path, wordcloud_path: str = None):
    """Saves a list of paper dictionaries as a formatted Markdown file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = output_dir / f"{task_name}_report_{timestamp}.md"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# {task_name} Papers ({timestamp})\n\n")
        f.write(f"Total papers found matching criteria: **{len(papers)}**\n\n")

        if wordcloud_path:
            f.write(f"## Trend Word Cloud\n\n")
            # --- ä¿®å¤ç‚¹: ç¡®ä¿è·¯å¾„åœ¨Markdownä¸­æ˜¯æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„ ---
            f.write(f"![Word Cloud](./{Path(wordcloud_path).name})\n\n")

        f.write("---\n\n")

        for i, paper in enumerate(papers, 1):
            title = paper.get('title', 'N/A').replace('\n', ' ')

            # --- ä¿®å¤ç‚¹: å¥å£®åœ°å¤„ç†ä½œè€…å­—æ®µï¼Œæ— è®ºæ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯åˆ—è¡¨ ---
            authors_data = paper.get('authors', 'N/A')
            if isinstance(authors_data, list):
                authors = ", ".join(authors_data)
            else:
                authors = str(authors_data)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            authors = authors.replace('\n', ' ')

            abstract = paper.get('abstract', 'N/A').replace('\n', ' ')
            pdf_url = paper.get('pdf_url', '#')

            f.write(f"### {i}. {title}\n\n")
            f.write(f"**Authors:** *{authors}*\n\n")

            if pdf_url and pdf_url != '#':
                f.write(f"**[PDF Link]({pdf_url})**\n\n")

            f.write(f"**Abstract:**\n")
            f.write(f"> {abstract}\n\n")
            f.write("---\n\n")

    print(f"Successfully saved Markdown report to {filename}")


def save_as_summary_txt(papers: list, task_name: str, output_dir: Path):
    """Saves a list of paper dictionaries as a formatted TXT file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = output_dir / f"{task_name}_summary_{timestamp}.txt"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"--- {task_name} Summary ({timestamp}) ---\n")
        f.write(f"Total papers found: {len(papers)}\n")
        f.write("=" * 40 + "\n\n")

        for i, paper in enumerate(papers, 1):
            title = paper.get('title', 'N/A').replace('\n', ' ')

            authors_data = paper.get('authors', 'N/A')
            if isinstance(authors_data, list):
                authors = ", ".join(authors_data)
            else:
                authors = str(authors_data)
            authors = authors.replace('\n', ' ')

            abstract = paper.get('abstract', 'N/A').replace('\n', ' ')
            pdf_url = paper.get('pdf_url', 'N/A')

            f.write(f"[{i}] Title: {title}\n")
            f.write(f"    Authors: {authors}\n")
            f.write(f"    PDF URL: {pdf_url}\n")
            f.write(f"    Abstract: {abstract}\n\n")

    print(f"Successfully saved TXT summary to {filename}")


def save_as_csv(papers: list, task_name: str, output_dir: Path):
    """Saves a list of paper dictionaries as a CSV file."""
    if not papers:
        return

    timestamp = datetime.now().strftime("%Y%m%d")
    filename = output_dir / f"{task_name}_data_{timestamp}.csv"

    # --- ä¿®å¤ç‚¹: åœ¨è½¬æ¢ä¸ºDataFrameä¹‹å‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ—è¡¨éƒ½å˜æˆå­—ç¬¦ä¸² ---
    processed_papers = []
    for paper in papers:
        new_paper = paper.copy()
        for key, value in new_paper.items():
            if isinstance(value, list):
                new_paper[key] = ", ".join(map(str, value))
        processed_papers.append(new_paper)

    df = pd.DataFrame(processed_papers)

    cols = ['title', 'authors', 'abstract', 'pdf_url', 'keywords', 'source_url']
    df_cols = [c for c in cols if c in df.columns] + [c for c in df.columns if c not in cols]
    df = df[df_cols]

    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"Successfully saved CSV data to {filename}")

==================== End of: src\utils\formatter.py ====================



==================== Start of: src\utils\network_utils.py ====================

# FILE: src/utils/network_utils.py

import requests
import time
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# è·å–ä¸€ä¸ªç®€å•çš„æ—¥å¿—è®°å½•å™¨ï¼Œæˆ–è€…ä½ å¯ä»¥ä»ä¸»é…ç½®ä¸­ä¼ é€’ä¸€ä¸ª
logger = logging.getLogger(__name__)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_session_with_retries(
    retries=5,
    backoff_factor=1,
    status_forcelist=(500, 502, 503, 504),
    session=None,
):
    """
    åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é‡è¯•æœºåˆ¶çš„ requests Session å¯¹è±¡ã€‚
    è¿™å¯¹äºå¤„ç†ä¸´æ—¶çš„ç½‘ç»œé”™è¯¯æˆ–æœåŠ¡å™¨ä¸ç¨³å®šéå¸¸æœ‰æ•ˆã€‚
    """
    session = session or requests.Session()
    retry_strategy = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session


def robust_get(url: str, timeout: int = 30, retries: int = 5, backoff_factor: float = 1.0):
    """
    ä¸€ä¸ªå¥å£®çš„ GET è¯·æ±‚å‡½æ•°ï¼Œé›†æˆäº†é‡è¯•å’Œæ›´é•¿çš„è¶…æ—¶ã€‚
    :param url: è¦è¯·æ±‚çš„ URL
    :param timeout: å•æ¬¡è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    :param retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :param backoff_factor: é‡è¯•çš„é€€é¿å› å­ (e.g., 1s, 2s, 4s...)
    :return: requests.Response å¯¹è±¡æˆ– None
    """
    session = get_session_with_retries(retries=retries, backoff_factor=backoff_factor)
    try:
        response = session.get(url, headers=HEADERS, timeout=timeout)
        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç æ˜¯ 4xx æˆ– 5xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        return response
    except requests.exceptions.RequestException as e:
        # ä½¿ç”¨ logger.error è€Œä¸æ˜¯ printï¼Œä»¥ä¾¿è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        logger.error(f"    [âœ– NETWORK ERROR] è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° for URL: {url}. Error: {e}")
        return None

==================== End of: src\utils\network_utils.py ====================



==================== Start of: src\utils\tqdm_logger.py ====================

# FILE: src/utils/tqdm_logger.py

import logging
from tqdm import tqdm

class TqdmLoggingHandler(logging.Handler):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„æ—¥å¿—å¤„ç†å™¨ï¼Œå®ƒèƒ½å°†æ—¥å¿—æ¶ˆæ¯é€šè¿‡ tqdm.write() è¾“å‡ºï¼Œ
    ä»è€Œé¿å…ä¸ tqdm è¿›åº¦æ¡çš„æ˜¾ç¤ºå‘ç”Ÿå†²çªã€‚
    """
    def __init__(self, level=logging.NOTSET):
        super().__init__(level)

    def emit(self, record):
        try:
            msg = self.format(record)
            # ä½¿ç”¨ tqdm.write æ¥æ‰“å°æ¶ˆæ¯ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ¢è¡Œï¼Œä¸”ä¸ä¼šå¹²æ‰°è¿›åº¦æ¡
            tqdm.write(msg)
            self.flush()
        except Exception:
            self.handleError(record)

==================== End of: src\utils\tqdm_logger.py ====================

