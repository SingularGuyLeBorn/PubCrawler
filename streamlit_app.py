# FILE: streamlit_app.py
# æ”¾ç½®äºé¡¹ç›®æ ¹ç›®å½•ï¼Œä¸ 'src' å’Œ 'app.py' åŒçº§ã€‚
# è¿è¡Œ: streamlit run streamlit_app.py

import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import logging
import re
import math
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import traceback  # ç”¨äºæ›´è¯¦ç»†çš„é”™è¯¯æ•è·

# -----------------------------------------------------------------
# 1. å¯¼å…¥é¡¹ç›®æ¨¡å—
# -----------------------------------------------------------------
try:
    from src.search.search_service import (
        initialize_components, keyword_search, semantic_search,
        generate_ai_response, get_stats_summary, _initialized,
        ZHIPUAI_API_KEY, SEARCH_RESULTS_DIR
    )
    from src.crawlers.config import METADATA_OUTPUT_DIR, TRENDS_OUTPUT_DIR
    # ã€v1.8 æ ¸å¿ƒã€‘ä» trends.py å¯¼å…¥åˆ†æé€»è¾‘
    from src.analysis.trends import _load_trend_config, _create_analysis_df
except ImportError as e:
    st.error(f"å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥ï¼é”™è¯¯: {e}\n\nè¯·ç¡®ä¿æ»¡è¶³æ‰€æœ‰ä¾èµ–å’Œæ–‡ä»¶ç»“æ„è¦æ±‚ã€‚")
    st.stop()

# -----------------------------------------------------------------
# 2. åº”ç”¨çº§å¸¸é‡
# -----------------------------------------------------------------
STREAMLIT_AI_CONTEXT_PAPERS = 20
RESULTS_PER_PAGE = 25
ANALYSIS_TOP_N = 50  # è¶‹åŠ¿åˆ†æå›¾è¡¨é»˜è®¤æ˜¾ç¤º Top N ä¸»é¢˜

# -----------------------------------------------------------------
# 3. Streamlit é¡µé¢é…ç½®ä¸åç«¯åˆå§‹åŒ–
# -----------------------------------------------------------------
st.set_page_config(page_title="PubCrawler Pro ğŸš€", layout="wide", initial_sidebar_state="expanded")


@st.cache_resource
def load_backend_components():
    print("--- [Streamlit] æ­£åœ¨åˆå§‹åŒ– PubCrawler åç«¯æœåŠ¡... ---")
    if not _initialized:
        try:
            initialize_components()
        except Exception as e:
            logging.error(f"åç«¯åˆå§‹åŒ–å¤±è´¥: {e}");
            return False
    if not _initialized: print("--- [Streamlit] åç«¯åˆå§‹åŒ–å¤±è´¥! ---"); return False
    print("--- [Streamlit] åç«¯æœåŠ¡å‡†å¤‡å°±ç»ªã€‚ ---")
    return True


backend_ready = load_backend_components()
if not backend_ready:
    st.error("åç«¯æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼è¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚");
    st.stop()

# -----------------------------------------------------------------
# 4. Streamlit é¡µé¢çŠ¶æ€ç®¡ç†
# -----------------------------------------------------------------
if "chat_history" not in st.session_state: st.session_state.chat_history: List[Dict[str, str]] = []
if "current_search_results" not in st.session_state: st.session_state.current_search_results: List[Dict[str, Any]] = []
if "current_filtered_results" not in st.session_state: st.session_state.current_filtered_results: List[
    Dict[str, Any]] = []
if "current_query" not in st.session_state: st.session_state.current_query: str = ""
if "current_page" not in st.session_state: st.session_state.current_page: int = 1


# -----------------------------------------------------------------
# 5. è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------
@st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
def find_analysis_files():
    """
    æ‰«æ output/ ç›®å½•æŸ¥æ‰¾åˆ†æ CSV æ–‡ä»¶ã€‚
    v1.8: æ”¹è¿›äº†æ–‡ä»¶ç±»å‹è¯†åˆ«é€»è¾‘ï¼Œä½¿å…¶æ›´ç²¾ç¡®ã€‚
    è¿”å›:
        analysis_data (dict): æŒ‰ ä¼šè®®/å¹´ä»½ ç»„ç»‡çš„ {"csvs": [{"path": Path, "type": str}]} å­—å…¸ã€‚
        all_conferences (list): æ‰¾åˆ°çš„æ‰€æœ‰ä¼šè®®åç§°åˆ—è¡¨ã€‚
        all_years (list): æ‰¾åˆ°çš„æ‰€æœ‰å¹´ä»½åˆ—è¡¨ã€‚
    """
    scan_dirs = {"metadata": METADATA_OUTPUT_DIR, "trends": TRENDS_OUTPUT_DIR}
    analysis_data = {}
    all_conferences = set();
    all_years = set()

    for dir_type, base_path in scan_dirs.items():
        if not base_path.exists(): continue
        all_csv_files_in_dir = list(base_path.rglob("*.csv"))

        for f in all_csv_files_in_dir:
            try:
                conf, year, csv_type = None, None, "unknown"
                relative_path = f.relative_to(base_path)
                parts = relative_path.parts

                # ---ã€v1.8 æ ¸å¿ƒæ”¹è¿›ï¼šè¯†åˆ«é€»è¾‘ã€‘---
                if dir_type == "metadata":
                    # è§„åˆ™ 1: analysis/4_summary_table... or analysis/summary_table... -> "summary_table"
                    if "analysis" in parts and ("summary_table" in f.name or "4_summary_table" in f.name):
                        csv_type = "summary_table"
                        year = parts[-3]
                        conf = parts[-4]
                    # è§„åˆ™ 2: *_data_*.csv -> "raw_data"
                    elif "_data_" in f.name:
                        csv_type = "raw_data"
                        year = parts[-2]
                        conf = parts[-3]
                    # è§„åˆ™ 3: å…¶ä»–åœ¨ analysis/ ä¸‹çš„æ–‡ä»¶ -> "analysis_other"
                    elif "analysis" in parts:
                        csv_type = "analysis_other"
                        year = parts[-3]
                        conf = parts[-4]

                elif dir_type == "trends":
                    # è§„åˆ™ 4: åœ¨ trends/<conf>/ ä¸‹çš„ csv -> "trends"
                    if len(parts) == 2:
                        csv_type = "trends"
                        year = "Cross-Year"
                        conf = parts[-2]

                # --- å­˜å‚¨æ‰¾åˆ°çš„æ–‡ä»¶ä¿¡æ¯ ---
                if conf and year:
                    all_conferences.add(conf);
                    all_years.add(year)
                    if conf not in analysis_data: analysis_data[conf] = {}
                    if year not in analysis_data[conf]: analysis_data[conf][year] = {"csvs": []}
                    file_entry = {"path": f, "type": csv_type}
                    if not any(item["path"] == f for item in analysis_data[conf][year]["csvs"]):
                        analysis_data[conf][year]["csvs"].append(file_entry)
            except Exception as scan_e:
                logging.warning(f"æ‰«ææ–‡ä»¶ {f} æ—¶å‡ºé”™: {scan_e}")

    return analysis_data, sorted(list(all_conferences)), sorted(list(all_years),
                                                                key=lambda y: "9999" if y == "Cross-Year" else y,
                                                                reverse=True)


def save_results_to_markdown_fixed(results: List[Dict[str, Any]], query: str) -> str:
    """ ä¿å­˜ Markdownï¼ŒåŒ…å«æ‘˜è¦å’Œ PDF é“¾æ¥ã€‚"""
    if not results: return "æ²¡æœ‰æœç´¢ç»“æœå¯ä¿å­˜ã€‚"
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# æœç´¢æŸ¥è¯¢: \"{query}\"\n\n**å…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title = paper.get('title', 'N/A');
            authors = paper.get('authors', 'N/A')
            abstract = paper.get('abstract', 'N/A');
            conf = paper.get('conference', 'N/A')
            year = paper.get('year', 'N/A');
            pdf_url = paper.get('pdf_url', '#')
            f.write(f"### {idx}. {title}\n\n");
            f.write(f"- **ä½œè€…**: {authors}\n")
            f.write(f"- **ä¼šè®®/å¹´ä»½**: {conf} {year}\n")
            if 'similarity' in paper: f.write(f"- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {paper['similarity']:.3f}\n")
            if pdf_url and pdf_url != '#':
                pdf_display_name = pdf_url.split('/')[-1] if '/' in pdf_url else "é“¾æ¥"
                f.write(f"- **PDF é“¾æ¥**: [{pdf_display_name}]({pdf_url})\n")
            f.write(f"\n**æ‘˜è¦:**\n> {abstract}\n\n---\n\n")
    return str(filename.resolve())


# -----------------------------------------------------------------
# 6. é¡µé¢æ¸²æŸ“å‡½æ•°
# -----------------------------------------------------------------

def render_search_and_chat_page():
    """ æ¸²æŸ“ "AI åŠ©æ‰‹ & æœç´¢" é¡µé¢ """
    st.header("ğŸ” PubCrawler Pro: AI åŠ©æ‰‹ & æœç´¢", divider="rainbow")
    _, conf_list, year_list = find_analysis_files()
    search_query = st.text_input("æœç´¢æœ¬åœ°å­¦æœ¯çŸ¥è¯†åº“...", key="search_input",
                                 placeholder="è¾“å…¥å…³é”®è¯æˆ– 'sem:' å‰ç¼€è¿›è¡Œè¯­ä¹‰æœç´¢",
                                 help="å…³é”®è¯æœç´¢: `transformer author:vaswani` | è¯­ä¹‰æœç´¢: `sem: few-shot learning efficiency`")
    col_f1, col_f2 = st.columns(2)
    with col_f1:
        selected_conferences = st.multiselect("ç­›é€‰ä¼šè®®", options=conf_list, key="filter_conf")
    with col_f2:
        selected_years = st.multiselect("ç­›é€‰å¹´ä»½", options=year_list, key="filter_year")
    is_new_search = (st.session_state.search_input != st.session_state.current_query)
    if is_new_search:
        with st.spinner(f"æ­£åœ¨æœç´¢: {st.session_state.search_input}..."):
            results, stats = [], {};
            query = st.session_state.search_input.strip()
            if query.lower().startswith('sem:'):
                query_text = query[4:].strip();
                if query_text: results, stats = semantic_search(query_text)
            elif query:
                results, stats = keyword_search(query)
            st.session_state.current_search_results = results
            st.session_state.current_query = st.session_state.search_input
            st.session_state.chat_history = [];
            st.session_state.current_page = 1
            st.toast(stats.get('message', 'æœç´¢å®Œæˆ!'))
    if st.session_state.current_search_results:
        temp_filtered_results = st.session_state.current_search_results
        if selected_conferences: temp_filtered_results = [p for p in temp_filtered_results if
                                                          p.get('conference') in selected_conferences]
        if selected_years: temp_filtered_results = [p for p in temp_filtered_results if
                                                    str(p.get('year')) in selected_years]
        st.session_state.current_filtered_results = temp_filtered_results
    else:
        st.session_state.current_filtered_results = []
    if is_new_search: st.session_state.current_page = 1
    col_results, col_chat = st.columns([0.6, 0.4])
    with col_results:
        results_to_display = st.session_state.current_filtered_results
        st.subheader(
            f"æœç´¢ç»“æœ (ç­›é€‰å: {len(results_to_display)} ç¯‡ / åŸå§‹: {len(st.session_state.current_search_results)} ç¯‡)")
        if results_to_display:
            with st.container(border=True, height=300):
                stats = get_stats_summary(results_to_display);
                c1, c2 = st.columns(2)
                c1.metric("ç­›é€‰åæ‰¾åˆ°", f"{stats['total_found']} ç¯‡")
                if c2.button("ğŸ“¥ ä¿å­˜å½“å‰ *ç­›é€‰å* çš„ç»“æœåˆ° Markdown", use_container_width=True):
                    with st.spinner("æ­£åœ¨ä¿å­˜..."):
                        save_path = save_results_to_markdown_fixed(results_to_display, st.session_state.current_query)
                        st.success(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
                st.write("**ä¼šè®®/å¹´ä»½åˆ†å¸ƒ (ç­›é€‰å):**");
                st.dataframe(pd.DataFrame(stats['distribution'].items(), columns=['æ¥æº', 'è®ºæ–‡æ•°']),
                             use_container_width=True, hide_index=True)
            st.divider()
            total_items = len(results_to_display);
            total_pages = math.ceil(total_items / RESULTS_PER_PAGE)
            if st.session_state.current_page > total_pages: st.session_state.current_page = max(1, total_pages)
            page_display_text = f"ç¬¬ {st.session_state.current_page} / {total_pages} é¡µ ({total_items} æ¡)" if total_pages > 0 else "æ— ç»“æœ"
            col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
            with col_page1:
                if st.button("ä¸Šä¸€é¡µ", disabled=st.session_state.current_page <= 1,
                             use_container_width=True): st.session_state.current_page -= 1; st.rerun()
            with col_page2:
                st.markdown(f"<div style='text-align: center; margin-top: 8px;'>{page_display_text}</div>",
                            unsafe_allow_html=True)
            with col_page3:
                if st.button("ä¸‹ä¸€é¡µ", disabled=st.session_state.current_page >= total_pages,
                             use_container_width=True): st.session_state.current_page += 1; st.rerun()
            start_idx = (st.session_state.current_page - 1) * RESULTS_PER_PAGE;
            end_idx = start_idx + RESULTS_PER_PAGE
            paginated_results = results_to_display[start_idx:end_idx]
            for i, paper in enumerate(paginated_results, start=start_idx + 1):
                with st.expander(f"**{i}. {paper.get('title', 'N/A')}**"):
                    if 'similarity' in paper: st.markdown(f"**è¯­ä¹‰ç›¸ä¼¼åº¦**: `{paper['similarity']:.3f}`")
                    st.markdown(f"**ä½œè€…**: *{paper.get('authors', 'N/A')}*");
                    st.markdown(f"**ä¼šè®®/å¹´ä»½**: {paper.get('conference', 'N/A')} {paper.get('year', 'N/A')}")
                    abstract_text = paper.get('abstract', None)
                    if abstract_text is None or abstract_text == '' or abstract_text == 'N/A' or pd.isna(abstract_text):
                        st.markdown(
                            f"**æ‘˜è¦**: <span style='color:orange; font-style: italic;'>[æ‘˜è¦ä¿¡æ¯ç¼ºå¤±æˆ–ä¸ºç©º]</span>",
                            unsafe_allow_html=True)
                    else:
                        st.markdown(f"**æ‘˜è¦**: \n> {abstract_text}")
                    pdf_url = paper.get('pdf_url', '#');
                    if pdf_url and pdf_url != '#': st.link_button("ğŸ”— æ‰“å¼€ PDF é“¾æ¥", pdf_url)
        elif st.session_state.current_query:
            st.info("æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼ˆæˆ–è¢«ç­›é€‰æ¡ä»¶è¿‡æ»¤ï¼‰ã€‚")
        else:
            st.info("è¯·è¾“å…¥æŸ¥è¯¢ä»¥å¼€å§‹æœç´¢ã€‚")
    with col_chat:
        st.subheader(f"ğŸ¤– AI å¯¹è¯åŠ©æ‰‹");
        st.info(f"AI å°†åŸºäºä¸Šæ–‡æœç´¢åˆ°çš„ **Top {STREAMLIT_AI_CONTEXT_PAPERS}** ç¯‡è®ºæ–‡è¿›è¡Œå›ç­”ã€‚")
        chat_container = st.container(height=500, border=True)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]): st.markdown(message["content"])
        if not ZHIPUAI_API_KEY:
            st.error("æœªé…ç½® ZHIPUAI_API_KEY!"); chat_disabled = True
        elif not st.session_state.current_filtered_results:
            st.info("è¯·å…ˆæœç´¢å¹¶ç¡®ä¿æœ‰ç»“æœå†å¯¹è¯ã€‚"); chat_disabled = True
        else:
            chat_disabled = False
        if prompt := st.chat_input("åŸºäºæœç´¢ç»“æœæé—®...", disabled=chat_disabled, key="chat_input"):
            st.session_state.chat_history.append({"role": "user", "content": prompt});
            st.rerun()
        if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
            with chat_container:
                for message in st.session_state.chat_history:
                    with st.chat_message(message["role"]): st.markdown(message["content"])
                with st.chat_message("assistant"):
                    with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                        response = generate_ai_response(st.session_state.chat_history,
                                                        st.session_state.current_filtered_results[
                                                            :STREAMLIT_AI_CONTEXT_PAPERS])
                        st.markdown(response)
            st.session_state.chat_history.append({"role": "assistant", "content": response});
            st.rerun()
        if st.session_state.chat_history and not chat_disabled:
            if st.button("æ¸…é™¤å¯¹è¯å†å²", use_container_width=True): st.session_state.chat_history = []; st.rerun()


def render_analysis_dashboard_page():
    """
    æ¸²æŸ“ "è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜" é¡µé¢ (v1.8 - ç»Ÿä¸€åŸå§‹æ•°æ®å’Œæ±‡æ€»æ•°æ®çš„åˆ†ææµç¨‹)
    """
    st.header("ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜", divider="rainbow")
    st.info(
        "é€‰æ‹©ä¼šè®®ã€å¹´ä»½å’Œåˆ†ææ–‡ä»¶ã€‚ç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œå¹¶ä¸º **åŸå§‹æ•°æ®** æˆ– **æ±‡æ€»æ•°æ®** ç”Ÿæˆå¯äº¤äº’çš„è¶‹åŠ¿å›¾è¡¨ã€‚"
    )

    analysis_data, _, _ = find_analysis_files()
    if not analysis_data:
        st.warning("æœªæ‰¾åˆ°ä»»ä½•åˆ†ææ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ `run_crawler.py` é‡‡é›†æ•°æ®ã€‚");
        st.stop()

    # --- ç”¨æˆ·é€‰æ‹© ---
    selected_conf = st.selectbox("1. é€‰æ‹©ä¼šè®®", options=sorted(analysis_data.keys()))
    if not selected_conf: st.stop()

    conf_data = analysis_data[selected_conf]
    sorted_years = sorted(conf_data.keys(), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)
    selected_year = st.selectbox(f"2. é€‰æ‹© {selected_conf} çš„å¹´ä»½æˆ–è·¨å¹´æ•°æ®", options=sorted_years)
    if not selected_year: st.stop()

    if selected_year not in conf_data or not conf_data[selected_year]["csvs"]:
        st.warning(f"æœªæ‰¾åˆ° {selected_conf} {selected_year} çš„ CSV æ•°æ®æ–‡ä»¶ã€‚");
        st.stop()

    files_info = conf_data[selected_year]
    csv_options_sorted = sorted(files_info["csvs"], key=lambda item: (
        0 if item['type'] == 'raw_data' else 1 if item['type'] == 'summary_table' else 2))
    csv_options = {f"{item['path'].name} (ç±»å‹: {item['type']})": item for item in csv_options_sorted}
    selected_csv_label = st.selectbox("3. é€‰æ‹©è¦åˆ†æçš„ CSV æ–‡ä»¶", options=csv_options.keys())
    if not selected_csv_label: st.stop()

    selected_csv_info = csv_options[selected_csv_label]
    csv_path = selected_csv_info["path"]
    csv_type = selected_csv_info["type"]

    st.markdown(f"#### æ­£åœ¨åˆ†æ: `{csv_path.name}`")

    # --- åŠ è½½å¹¶å¤„ç† CSV ---
    try:
        df = pd.read_csv(csv_path)
        if df.empty:
            st.warning("CSV æ–‡ä»¶ä¸ºç©ºã€‚");
            st.stop()

        # --- ã€v1.8 æ ¸å¿ƒæ”¹åŠ¨ï¼šç»Ÿä¸€åˆ†ææµç¨‹ã€‘---
        st.markdown("---")
        analysis_df = None
        is_analyzed = False

        if csv_type == "summary_table":
            st.success("âœ”ï¸ æ£€æµ‹åˆ° **æ±‡æ€»åˆ†ææ–‡ä»¶**ã€‚ç›´æ¥ä½¿ç”¨å…¶æ•°æ®ç”Ÿæˆå›¾è¡¨ã€‚")
            analysis_df = df
            is_analyzed = True

        elif csv_type == "raw_data":
            st.info("ğŸ’¡ æ£€æµ‹åˆ° **åŸå§‹æ•°æ®æ–‡ä»¶**ã€‚æ­£åœ¨è¿›è¡Œå³æ—¶è¶‹åŠ¿åˆ†æ...")
            trend_config = _load_trend_config()
            if trend_config:
                with st.spinner("æ­£åœ¨åŸºäº `trends.yaml` é…ç½®åˆ†æåŸå§‹è®ºæ–‡æ•°æ®..."):
                    analysis_df = _create_analysis_df(df, trend_config)
                if not analysis_df.empty:
                    st.success("âœ”ï¸ å³æ—¶åˆ†æå®Œæˆï¼å·²ç”Ÿæˆä¸‹æ–¹å›¾è¡¨ã€‚")
                    is_analyzed = True
                else:
                    st.warning("åˆ†æå®Œæˆï¼Œä½†æœªèƒ½ä»åŸå§‹æ•°æ®ä¸­åŒ¹é…åˆ°ä»»ä½• `trends.yaml` ä¸­å®šä¹‰çš„å…³é”®è¯ã€‚")
            else:
                st.error("æ— æ³•åŠ è½½ `configs/trends.yaml` é…ç½®æ–‡ä»¶ï¼Œåˆ†æä¸­æ­¢ã€‚")

        # --- ç»Ÿä¸€ç»˜å›¾é€»è¾‘ ---
        if is_analyzed and analysis_df is not None:
            # å›¾è¡¨ 1: ä¸»é¢˜çƒ­åº¦ (æŒ‰è®ºæ–‡æ•°)
            st.markdown(f"##### 1. ä¸»é¢˜çƒ­åº¦æ’å (Top {ANALYSIS_TOP_N} è®ºæ–‡æ•°)")
            if 'paper_count' in analysis_df.columns and 'Topic_Name' in analysis_df.columns:
                fig_hotness = px.bar(analysis_df.sort_values(by='paper_count', ascending=False).head(ANALYSIS_TOP_N),
                                     x='paper_count', y='Topic_Name', orientation='h',
                                     title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} çƒ­é—¨ä¸»é¢˜ (è®ºæ–‡æ•°é‡)',
                                     labels={'paper_count': 'è®ºæ–‡æ•°é‡', 'Topic_Name': 'ä¸»é¢˜'},
                                     text_auto=True, height=max(600, len(analysis_df.head(ANALYSIS_TOP_N)) * 20))
                fig_hotness.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig_hotness, use_container_width=True)
            else:
                st.caption("æ— æ³•ç”Ÿæˆå›¾è¡¨ï¼šç¼ºå°‘ `paper_count` æˆ– `Topic_Name` åˆ—ã€‚")

            # å›¾è¡¨ 2: ä¸»é¢˜è´¨é‡ (æŒ‰å¹³å‡åˆ†)
            if 'avg_rating' in analysis_df.columns and analysis_df['avg_rating'].notna().any():
                st.markdown(f"##### 2. ä¸»é¢˜è´¨é‡æ’å (Top {ANALYSIS_TOP_N} å¹³å‡å®¡ç¨¿åˆ†)")
                df_quality = analysis_df.dropna(subset=['avg_rating']).sort_values(by='avg_rating',
                                                                                   ascending=False).head(ANALYSIS_TOP_N)
                if not df_quality.empty:
                    fig_quality = px.bar(df_quality, x='avg_rating', y='Topic_Name', orientation='h',
                                         title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜ (å¹³å‡å®¡ç¨¿åˆ†)',
                                         labels={'avg_rating': 'å¹³å‡å®¡ç¨¿åˆ†', 'Topic_Name': 'ä¸»é¢˜'},
                                         text_auto='.2f', height=max(600, len(df_quality) * 20))
                    fig_quality.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig_quality, use_container_width=True)
                else:
                    st.caption("æœªèƒ½ç”Ÿæˆå›¾è¡¨ï¼šç­›é€‰åæ•°æ®ä¸ºç©ºã€‚")
            else:
                st.caption("ä¸»é¢˜è´¨é‡å›¾è¡¨ä¸å¯ç”¨ (ç¼ºå°‘ `avg_rating` æ•°æ®)ã€‚")

            # å›¾è¡¨ 3: å†³ç­–æ„æˆ
            decision_cols = [col for col in ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A'] if
                             col in analysis_df.columns]
            if 'acceptance_rate' in analysis_df.columns and decision_cols:
                st.markdown(f"##### 3. ä¸»é¢˜æ¥æ”¶æ„æˆ (Top {ANALYSIS_TOP_N} æŒ‰æ¥æ”¶ç‡æ’åº)")
                df_decision = analysis_df.dropna(subset=['acceptance_rate']).sort_values(by='acceptance_rate',
                                                                                         ascending=False).head(
                    ANALYSIS_TOP_N)
                if not df_decision.empty:
                    df_plot = df_decision.set_index('Topic_Name')[decision_cols].copy()
                    df_plot = df_plot.loc[(df_plot.sum(axis=1) > 0)]
                    if not df_plot.empty:
                        df_plot_normalized = df_plot.div(df_plot.sum(axis=1), axis=0)
                        fig_stack = px.bar(df_plot_normalized, y=df_plot_normalized.index, x=df_plot_normalized.columns,
                                           orientation='h',
                                           title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜å†³ç­–æ„æˆ (æŒ‰æ¥æ”¶ç‡æ’åº)',
                                           labels={'value': 'è®ºæ–‡æ¯”ä¾‹', 'variable': 'å†³ç­–ç±»å‹', 'y': 'ä¸»é¢˜'},
                                           height=max(600, len(df_plot_normalized) * 25), text_auto='.1%')
                        fig_stack.update_layout(yaxis={'categoryorder': 'total ascending'}, xaxis_tickformat=".0%",
                                                legend_title_text='å†³ç­–ç±»å‹')
                        fig_stack.update_traces(hovertemplate='<b>%{y}</b><br>%{variable}: %{x:.1%}<extra></extra>')
                        st.plotly_chart(fig_stack, use_container_width=True)
                else:
                    st.caption("æœªèƒ½ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šç­›é€‰åæ•°æ®ä¸ºç©ºã€‚")
            else:
                st.caption("ä¸»é¢˜å†³ç­–æ„æˆå›¾è¡¨ä¸å¯ç”¨ (ç¼ºå°‘ `acceptance_rate` æˆ–å†³ç­–åˆ—æ•°æ®)ã€‚")

        # --- è·¨å¹´ä»½è¶‹åŠ¿æ–‡ä»¶å¤„ç†é€»è¾‘ (ä¿æŒç‹¬ç«‹) ---
        elif csv_type == "trends":
            st.subheader("ğŸ“ˆ è·¨å¹´ä»½è¶‹åŠ¿å›¾")
            if 'year' in df.columns or df.index.name == 'year':
                df_indexed = df.set_index('year') if 'year' in df.columns else df
                numeric_cols = df_indexed.select_dtypes(include='number').columns
                if not numeric_cols.empty:
                    top_topics = df_indexed[numeric_cols].sum().nlargest(15).index
                    df_top = df_indexed[top_topics]
                    fig_line = px.line(df_top, x=df_top.index, y=df_top.columns, markers=True,
                                       title=f'{selected_conf} - Top 15 ä¸»é¢˜å†å¹´è®ºæ–‡æ•°è¶‹åŠ¿',
                                       labels={'year': 'å¹´ä»½', 'value': 'è®ºæ–‡æ•°é‡', 'variable': 'ä¸»é¢˜'})
                    fig_line.update_layout(xaxis_type='category')
                    st.plotly_chart(fig_line, use_container_width=True)
                else:
                    st.warning("è¶‹åŠ¿æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨äºç»˜å›¾çš„æ•°å€¼åˆ—ã€‚")
            else:
                st.warning(f"æ— æ³•ä¸º `{csv_path.name}` ç”Ÿæˆè¶‹åŠ¿æŠ˜çº¿å›¾ã€‚è¯·ç¡®ä¿ CSV æœ‰ä¸€ä¸ªå¹´ä»½åˆ—/ç´¢å¼•ã€‚")

        # --- å…¶ä»–æ–‡ä»¶ç±»å‹çš„é¢„è§ˆé€»è¾‘ ---
        else:  # 'analysis_other', 'unknown'
            st.subheader("ğŸ“„ æ•°æ®é¢„è§ˆ")
            st.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹ä¸º **{csv_type}**ã€‚æ­¤ç±»æ–‡ä»¶ä¸é€‚ç”¨äºæ ‡å‡†å›¾è¡¨ç”Ÿæˆï¼Œä»…æä¾›æ•°æ®è¡¨æ ¼é¢„è§ˆã€‚")
            st.dataframe(df, height=500, use_container_width=True)

        # --- æ˜¾ç¤º/ä¸‹è½½åŸå§‹æ•°æ®è¡¨æ ¼ (é€‚ç”¨äºæ‰€æœ‰ç±»å‹) ---
        st.markdown("---")
        with st.expander("æŸ¥çœ‹/ä¸‹è½½å½“å‰ CSV çš„åŸå§‹æ•°æ®"):
            st.dataframe(df, height=300, use_container_width=True)
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½æ•°æ®: {csv_path.name}",
                data=df.to_csv(index=False).encode('utf-8-sig'),
                file_name=csv_path.name,
                mime='text/csv',
                key=f"download_{csv_path.stem}"
            )

    except Exception as e:
        st.error(f"å¤„ç†æˆ–å¯è§†åŒ–æ–‡ä»¶ {csv_path.name} æ—¶å‡ºé”™: {e}")
        st.error(traceback.format_exc())


# -----------------------------------------------------------------
# 7. ä¸»åº”ç”¨é€»è¾‘ï¼šä¾§è¾¹æ å¯¼èˆª
# -----------------------------------------------------------------
st.sidebar.title("PubCrawler Pro ğŸš€");
st.sidebar.caption("v1.8 - Unified Analysis")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½é¡µé¢", ["ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜", "ğŸ¤– AI åŠ©æ‰‹ & æœç´¢"], key="page_selection")
st.sidebar.divider()
analysis_data_sidebar, conf_list_sidebar, _ = find_analysis_files()
if conf_list_sidebar:
    with st.sidebar.expander("ç›®å‰å·²ç´¢å¼•çš„ä¼šè®®", expanded=True):
        st.dataframe(pd.DataFrame({"ä¼šè®®åç§°": conf_list_sidebar}), use_container_width=True, hide_index=True)

if page == "ğŸ¤– AI åŠ©æ‰‹ & æœç´¢":
    render_search_and_chat_page()
elif page == "ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜":
    render_analysis_dashboard_page()
# END OF FILE: streamlit_app.py