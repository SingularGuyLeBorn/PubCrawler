# FILE: streamlit_app.py
# æ”¾ç½®äºé¡¹ç›®æ ¹ç›®å½•ï¼Œä¸ 'src' å’Œ 'app.py' åŒçº§ã€‚
# è¿è¡Œ: streamlit run streamlit_app.py

import streamlit as st
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import logging
import re
import math
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import traceback # ç”¨äºæ›´è¯¦ç»†çš„é”™è¯¯æ•è·

# -----------------------------------------------------------------
# 1. å¯¼å…¥é¡¹ç›®æ¨¡å— (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
try:
    from src.search.search_service import (
        initialize_components, keyword_search, semantic_search,
        generate_ai_response, get_stats_summary, _initialized,
        ZHIPUAI_API_KEY, SEARCH_RESULTS_DIR
    )
    from src.crawlers.config import METADATA_OUTPUT_DIR, TRENDS_OUTPUT_DIR
except ImportError as e:
    st.error(f"å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥ï¼é”™è¯¯: {e}\n\nè¯·ç¡®ä¿æ»¡è¶³æ‰€æœ‰ä¾èµ–å’Œæ–‡ä»¶ç»“æ„è¦æ±‚ã€‚")
    st.stop()

# -----------------------------------------------------------------
# 2. åº”ç”¨çº§å¸¸é‡ (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
STREAMLIT_AI_CONTEXT_PAPERS = 20
RESULTS_PER_PAGE = 25
ANALYSIS_TOP_N = 50 # è¶‹åŠ¿åˆ†æå›¾è¡¨é»˜è®¤æ˜¾ç¤º Top N ä¸»é¢˜

# -----------------------------------------------------------------
# 3. Streamlit é¡µé¢é…ç½®ä¸åç«¯åˆå§‹åŒ– (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
st.set_page_config(page_title="PubCrawler Pro ğŸš€", layout="wide", initial_sidebar_state="expanded")

@st.cache_resource
def load_backend_components():
    print("--- [Streamlit] æ­£åœ¨åˆå§‹åŒ– PubCrawler åç«¯æœåŠ¡... ---")
    if not _initialized:
        try: initialize_components()
        except Exception as e:
            logging.error(f"åç«¯åˆå§‹åŒ–å¤±è´¥: {e}"); return False
    if not _initialized: print("--- [Streamlit] åç«¯åˆå§‹åŒ–å¤±è´¥! ---"); return False
    print("--- [Streamlit] åç«¯æœåŠ¡å‡†å¤‡å°±ç»ªã€‚ ---")
    return True

backend_ready = load_backend_components()
if not backend_ready:
    st.error("åç«¯æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼è¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚"); st.stop()

# -----------------------------------------------------------------
# 4. Streamlit é¡µé¢çŠ¶æ€ç®¡ç† (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
if "chat_history" not in st.session_state: st.session_state.chat_history: List[Dict[str, str]] = []
if "current_search_results" not in st.session_state: st.session_state.current_search_results: List[Dict[str, Any]] = []
if "current_filtered_results" not in st.session_state: st.session_state.current_filtered_results: List[Dict[str, Any]] = []
if "current_query" not in st.session_state: st.session_state.current_query: str = ""
if "current_page" not in st.session_state: st.session_state.current_page: int = 1

# -----------------------------------------------------------------
# 5. è¾…åŠ©å‡½æ•°
# -----------------------------------------------------------------
def find_analysis_files():
    """
    æ‰«æ output/ ç›®å½•æŸ¥æ‰¾åˆ†æ CSV æ–‡ä»¶ï¼Œæ— ç¼“å­˜ã€‚
    v1.7: æ›´ç²¾ç¡®åœ°è¯†åˆ« CSV ç±»å‹ï¼Œå¢åŠ è°ƒè¯•ä¿¡æ¯ã€‚
    è¿”å›:
        analysis_data (dict): æŒ‰ ä¼šè®®/å¹´ä»½ ç»„ç»‡çš„ {"csvs": [{"path": Path, "type": str}]} å­—å…¸ã€‚
        all_conferences (list): æ‰¾åˆ°çš„æ‰€æœ‰ä¼šè®®åç§°åˆ—è¡¨ã€‚
        all_years (list): æ‰¾åˆ°çš„æ‰€æœ‰å¹´ä»½åˆ—è¡¨ã€‚
    """
    print("--- [Streamlit] æ­£åœ¨æ‰«æåˆ†ææ–‡ä»¶ (v1.7 - æ— ç¼“å­˜)... ---")
    # st.write("--- [Streamlit] æ­£åœ¨æ‰«æåˆ†ææ–‡ä»¶ (v1.7 - æ— ç¼“å­˜)... ---") # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š
    scan_dirs = {"metadata": METADATA_OUTPUT_DIR, "trends": TRENDS_OUTPUT_DIR}
    analysis_data = {}
    all_conferences = set(); all_years = set()
    found_files_debug = [] # ç”¨äºè°ƒè¯•

    for dir_type, base_path in scan_dirs.items():
        if not base_path.exists(): continue
        # ç»Ÿä¸€æŸ¥æ‰¾æ‰€æœ‰ CSV æ–‡ä»¶
        all_csv_files_in_dir = list(base_path.rglob("*.csv"))

        for f in all_csv_files_in_dir:
            try:
                conf, year = None, None
                csv_type = "unknown" # é»˜è®¤ç±»å‹

                relative_path = f.relative_to(METADATA_OUTPUT_DIR if dir_type == "metadata" else TRENDS_OUTPUT_DIR)
                parts = relative_path.parts

                # ---ã€v1.7 æ ¸å¿ƒæ”¹è¿›ï¼šè¯†åˆ«é€»è¾‘ã€‘---
                if dir_type == "metadata":
                    # è§„åˆ™ 1: è·¯å¾„æ˜¯å¦åŒ¹é… <conf>/<year>/analysis/<filename>.csv
                    if len(parts) >= 3 and parts[-2] == "analysis":
                        year = parts[-3]
                        conf = parts[-4] if len(parts) >= 4 else None # e.g., output/metadata/ICLR/2025/analysis/file.csv
                        if conf:
                             # æ ¹æ®æ–‡ä»¶åè¿›ä¸€æ­¥ç»†åŒ–
                            if "summary_table" in f.name or "4_summary_table" in f.name:
                                csv_type = "summary_table"
                            else:
                                csv_type = "analysis_other" # å…¶ä»–åœ¨ analysis ä¸‹çš„æ–‡ä»¶
                    # è§„åˆ™ 2: æ–‡ä»¶åæ˜¯å¦åŒ…å« _data_ ä¸”è·¯å¾„åŒ¹é… <conf>/<year>/<filename>.csv
                    elif "_data_" in f.name and len(parts) == 3:
                        year = parts[-2]
                        conf = parts[-3]
                        csv_type = "raw_data"
                    # å¯é€‰ï¼šæ·»åŠ å…¶ä»–è§„åˆ™æ¥è¯†åˆ« metadata ä¸‹çš„å…¶ä»–ç±»å‹ CSV

                elif dir_type == "trends":
                    # è§„åˆ™ 3: è·¯å¾„æ˜¯å¦åŒ¹é… <conf>/<filename>.csv
                    if len(parts) == 2:
                        conf = parts[-2]
                        year = "Cross-Year"
                        csv_type = "trends"

                # --- å­˜å‚¨æ‰¾åˆ°çš„æ–‡ä»¶ä¿¡æ¯ ---
                if conf and year:
                    all_conferences.add(conf); all_years.add(year)
                    if conf not in analysis_data: analysis_data[conf] = {}
                    if year not in analysis_data[conf]: analysis_data[conf][year] = {"csvs": []}
                    file_entry = {"path": f, "type": csv_type}
                    # è°ƒè¯•ä¿¡æ¯: è®°å½•æ¯ä¸ªæ–‡ä»¶çš„è·¯å¾„å’Œè¯†åˆ«å‡ºçš„ç±»å‹
                    found_files_debug.append(f"Path: {relative_path}, Conf: {conf}, Year: {year}, Type: {csv_type}")
                    if not any(item["path"] == f for item in analysis_data[conf][year]["csvs"]):
                        analysis_data[conf][year]["csvs"].append(file_entry)
            except Exception as scan_e:
                print(f"æ‰«ææ–‡ä»¶ {f} æ—¶å‡ºé”™: {scan_e}")
                # st.write(f"æ‰«ææ–‡ä»¶ {f} æ—¶å‡ºé”™: {scan_e}") # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š


    # ---ã€v1.7 è°ƒè¯•è¾“å‡ºã€‘---
    # print("\n--- æ–‡ä»¶æ‰«æè°ƒè¯•ä¿¡æ¯ ---")
    # for entry in found_files_debug:
    #     print(entry)
    # print("--- è°ƒè¯•ç»“æŸ ---\n")
    # # åœ¨ Streamlit ç•Œé¢æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼ˆè°ƒè¯•å®Œæˆåæ³¨é‡Šæ‰ï¼‰
    # with st.expander("æ–‡ä»¶æ‰«æè°ƒè¯•ä¿¡æ¯ (v1.7)", expanded=False):
    #      st.json(analysis_data) # æ›´ç»“æ„åŒ–åœ°æ˜¾ç¤ºç»“æœ
    #      st.write(found_files_debug) # æ˜¾ç¤ºåŸå§‹è°ƒè¯•åˆ—è¡¨

    print(f"--- [Streamlit] æ‰«æå®Œæˆï¼Œæ‰¾åˆ°ä¼šè®®: {list(all_conferences)}, å¹´ä»½: {list(all_years)} ---")
    return analysis_data, sorted(list(all_conferences)), sorted(list(all_years), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)


def save_results_to_markdown_fixed(results: List[Dict[str, Any]], query: str) -> str:
    """ (v1.3) ä¿å­˜ Markdownï¼ŒåŒ…å«æ‘˜è¦å’Œ PDF é“¾æ¥ã€‚"""
    # ... (å†…éƒ¨é€»è¾‘ä¿æŒä¸å˜) ...
    if not results: return "æ²¡æœ‰æœç´¢ç»“æœå¯ä¿å­˜ã€‚"
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    session_dir = SEARCH_RESULTS_DIR / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    session_dir.mkdir(exist_ok=True)
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")[:50]
    filename = session_dir / f"search_{safe_query}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# æœç´¢æŸ¥è¯¢: \"{query}\"\n\n**å…±æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ**\n\n---\n\n")
        for idx, paper in enumerate(results, 1):
            title = paper.get('title', 'N/A'); authors = paper.get('authors', 'N/A')
            abstract = paper.get('abstract', 'N/A'); conf = paper.get('conference', 'N/A')
            year = paper.get('year', 'N/A'); pdf_url = paper.get('pdf_url', '#')
            f.write(f"### {idx}. {title}\n\n"); f.write(f"- **ä½œè€…**: {authors}\n")
            f.write(f"- **ä¼šè®®/å¹´ä»½**: {conf} {year}\n")
            if 'similarity' in paper: f.write(f"- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {paper['similarity']:.3f}\n")
            if pdf_url and pdf_url != '#':
                pdf_display_name = pdf_url.split('/')[-1] if '/' in pdf_url else "é“¾æ¥"
                f.write(f"- **PDF é“¾æ¥**: [{pdf_display_name}]({pdf_url})\n")
            f.write(f"\n**æ‘˜è¦:**\n> {abstract}\n\n---\n\n")
    return str(filename.resolve())


def get_numeric_columns(df): return df.select_dtypes(include=['number']).columns.tolist()
def get_categorical_columns(df): return df.select_dtypes(include=['object', 'category']).columns.tolist()

# -----------------------------------------------------------------
# 6. é¡µé¢æ¸²æŸ“å‡½æ•°
# -----------------------------------------------------------------

def render_search_and_chat_page():
    """ æ¸²æŸ“ "AI åŠ©æ‰‹ & æœç´¢" é¡µé¢ (v1.7 é€»è¾‘) """
    # ... (å¤§éƒ¨åˆ†é€»è¾‘ä¸å˜) ...
    st.header("ğŸ” PubCrawler Pro: AI åŠ©æ‰‹ & æœç´¢", divider="rainbow")
    _, conf_list, year_list = find_analysis_files()
    search_query = st.text_input("æœç´¢æœ¬åœ°å­¦æœ¯çŸ¥è¯†åº“...", key="search_input", placeholder="è¾“å…¥å…³é”®è¯æˆ– 'sem:' å‰ç¼€è¿›è¡Œè¯­ä¹‰æœç´¢", help="å…³é”®è¯æœç´¢: `transformer author:vaswani` | è¯­ä¹‰æœç´¢: `sem: few-shot learning efficiency`")
    col_f1, col_f2 = st.columns(2)
    with col_f1: selected_conferences = st.multiselect("ç­›é€‰ä¼šè®®", options=conf_list, key="filter_conf")
    with col_f2: selected_years = st.multiselect("ç­›é€‰å¹´ä»½", options=year_list, key="filter_year")
    is_new_search = (st.session_state.search_input != st.session_state.current_query)
    if is_new_search:
        with st.spinner(f"æ­£åœ¨æœç´¢: {st.session_state.search_input}..."):
            results, stats = [], {}; query = st.session_state.search_input.strip()
            if query.lower().startswith('sem:'):
                query_text = query[4:].strip();
                if query_text: results, stats = semantic_search(query_text)
            elif query: results, stats = keyword_search(query)
            st.session_state.current_search_results = results
            st.session_state.current_query = st.session_state.search_input
            st.session_state.chat_history = []; st.session_state.current_page = 1
            st.toast(stats.get('message', 'æœç´¢å®Œæˆ!'))
    if st.session_state.current_search_results:
        temp_filtered_results = st.session_state.current_search_results
        if selected_conferences: temp_filtered_results = [p for p in temp_filtered_results if p.get('conference') in selected_conferences]
        if selected_years: temp_filtered_results = [p for p in temp_filtered_results if str(p.get('year')) in selected_years]
        st.session_state.current_filtered_results = temp_filtered_results
    else: st.session_state.current_filtered_results = []
    if is_new_search: st.session_state.current_page = 1
    col_results, col_chat = st.columns([0.6, 0.4])
    with col_results:
        results_to_display = st.session_state.current_filtered_results
        st.subheader(f"æœç´¢ç»“æœ (ç­›é€‰å: {len(results_to_display)} ç¯‡ / åŸå§‹: {len(st.session_state.current_search_results)} ç¯‡)")
        if results_to_display:
            with st.container(border=True, height=300):
                stats = get_stats_summary(results_to_display); c1, c2 = st.columns(2)
                c1.metric("ç­›é€‰åæ‰¾åˆ°", f"{stats['total_found']} ç¯‡")
                if c2.button("ğŸ“¥ ä¿å­˜å½“å‰ *ç­›é€‰å* çš„ç»“æœåˆ° Markdown", use_container_width=True):
                    with st.spinner("æ­£åœ¨ä¿å­˜..."):
                        save_path = save_results_to_markdown_fixed(results_to_display, st.session_state.current_query)
                        st.success(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
                st.write("**ä¼šè®®/å¹´ä»½åˆ†å¸ƒ (ç­›é€‰å):**"); st.dataframe(pd.DataFrame(stats['distribution'].items(), columns=['æ¥æº', 'è®ºæ–‡æ•°']), use_container_width=True, hide_index=True)
            st.divider()
            total_items = len(results_to_display); total_pages = math.ceil(total_items / RESULTS_PER_PAGE)
            if st.session_state.current_page > total_pages: st.session_state.current_page = max(1, total_pages)
            page_display_text = f"ç¬¬ {st.session_state.current_page} / {total_pages} é¡µ ({total_items} æ¡)" if total_pages > 0 else "æ— ç»“æœ"
            col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
            with col_page1:
                 if st.button("ä¸Šä¸€é¡µ", disabled=st.session_state.current_page <= 1, use_container_width=True): st.session_state.current_page -= 1; st.rerun()
            with col_page2: st.markdown(f"<div style='text-align: center; margin-top: 8px;'>{page_display_text}</div>", unsafe_allow_html=True)
            with col_page3:
                 if st.button("ä¸‹ä¸€é¡µ", disabled=st.session_state.current_page >= total_pages, use_container_width=True): st.session_state.current_page += 1; st.rerun()
            start_idx = (st.session_state.current_page - 1) * RESULTS_PER_PAGE; end_idx = start_idx + RESULTS_PER_PAGE
            paginated_results = results_to_display[start_idx:end_idx]
            for i, paper in enumerate(paginated_results, start=start_idx + 1):
                with st.expander(f"**{i}. {paper.get('title', 'N/A')}**"):
                    if 'similarity' in paper: st.markdown(f"**è¯­ä¹‰ç›¸ä¼¼åº¦**: `{paper['similarity']:.3f}`")
                    st.markdown(f"**ä½œè€…**: *{paper.get('authors', 'N/A')}*"); st.markdown(f"**ä¼šè®®/å¹´ä»½**: {paper.get('conference', 'N/A')} {paper.get('year', 'N/A')}")
                    abstract_text = paper.get('abstract', None)
                    if abstract_text is None or abstract_text == '' or abstract_text == 'N/A' or pd.isna(abstract_text):
                        st.markdown(f"**æ‘˜è¦**: <span style='color:orange; font-style: italic;'>[æ‘˜è¦ä¿¡æ¯ç¼ºå¤±æˆ–ä¸ºç©º]</span>", unsafe_allow_html=True)
                    else:
                        st.markdown(f"**æ‘˜è¦**: \n> {abstract_text}")
                    pdf_url = paper.get('pdf_url', '#');
                    if pdf_url and pdf_url != '#': st.link_button("ğŸ”— æ‰“å¼€ PDF é“¾æ¥", pdf_url)
        elif st.session_state.current_query: st.info("æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼ˆæˆ–è¢«ç­›é€‰æ¡ä»¶è¿‡æ»¤ï¼‰ã€‚")
        else: st.info("è¯·è¾“å…¥æŸ¥è¯¢ä»¥å¼€å§‹æœç´¢ã€‚")
    with col_chat:
        st.subheader(f"ğŸ¤– AI å¯¹è¯åŠ©æ‰‹"); st.info(f"AI å°†åŸºäºä¸Šæ–‡æœç´¢åˆ°çš„ **Top {STREAMLIT_AI_CONTEXT_PAPERS}** ç¯‡è®ºæ–‡è¿›è¡Œå›ç­”ã€‚")
        chat_container = st.container(height=500, border=True)
        with chat_container:
            for message in st.session_state.chat_history:
                with st.chat_message(message["role"]): st.markdown(message["content"])
        if not ZHIPUAI_API_KEY: st.error("æœªé…ç½® ZHIPUAI_API_KEY!"); chat_disabled = True
        elif not st.session_state.current_filtered_results: st.info("è¯·å…ˆæœç´¢å¹¶ç¡®ä¿æœ‰ç»“æœå†å¯¹è¯ã€‚"); chat_disabled = True
        else: chat_disabled = False
        if prompt := st.chat_input("åŸºäºæœç´¢ç»“æœæé—®...", disabled=chat_disabled, key="chat_input"):
            st.session_state.chat_history.append({"role": "user", "content": prompt}); st.rerun()
        if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
            with chat_container:
                for message in st.session_state.chat_history:
                    with st.chat_message(message["role"]): st.markdown(message["content"])
                with st.chat_message("assistant"):
                    with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                        response = generate_ai_response(st.session_state.chat_history, st.session_state.current_filtered_results[:STREAMLIT_AI_CONTEXT_PAPERS])
                        st.markdown(response)
            st.session_state.chat_history.append({"role": "assistant", "content": response}); st.rerun()
        if st.session_state.chat_history and not chat_disabled:
            if st.button("æ¸…é™¤å¯¹è¯å†å²", use_container_width=True): st.session_state.chat_history = []; st.rerun()

def render_analysis_dashboard_page():
    """
    æ¸²æŸ“ "è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜" é¡µé¢ (v1.7 - æ”¹è¿› CSV ç±»å‹è¯†åˆ«)
    """
    st.header("ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜ (æ ¸å¿ƒå›¾è¡¨)", divider="rainbow")
    st.info(
        "é€‰æ‹©ä¼šè®®ã€å¹´ä»½å’Œå…·ä½“çš„ CSV åˆ†ææ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆæ ¸å¿ƒè¶‹åŠ¿å›¾è¡¨æˆ–å±•ç¤ºåŸå§‹æ•°æ®ã€‚"
    )

    analysis_data, all_conferences, _ = find_analysis_files()

    if not analysis_data:
        st.warning("æœªæ‰¾åˆ°ä»»ä½•åˆ†ææ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ `run_crawler.py`ã€‚")
        st.stop()

    # --- ç”¨æˆ·é€‰æ‹© ---
    selected_conf = st.selectbox("1. é€‰æ‹©ä¼šè®®", options=sorted(analysis_data.keys()))
    if not selected_conf: st.stop()

    conf_data = analysis_data[selected_conf]
    sorted_years = sorted(conf_data.keys(), key=lambda y: "9999" if y == "Cross-Year" else y, reverse=True)
    selected_year = st.selectbox(f"2. é€‰æ‹© {selected_conf} çš„å¹´ä»½æˆ–è·¨å¹´æ•°æ®", options=sorted_years)
    if not selected_year: st.stop()

    # æ£€æŸ¥æ‰€é€‰å¹´ä»½æ˜¯å¦å­˜åœ¨æ•°æ®
    if selected_year not in conf_data or not conf_data[selected_year]["csvs"]:
        st.warning(f"æœªæ‰¾åˆ° {selected_conf} {selected_year} çš„ CSV æ•°æ®æ–‡ä»¶ã€‚")
        st.stop()

    files_info = conf_data[selected_year]

    # ---ã€v1.7ã€‘æ”¹è¿› CSV æ–‡ä»¶é€‰æ‹© ---
    # ä½¿ç”¨æ›´æ¸…æ™°çš„æ ‡ç­¾ï¼Œä¼˜å…ˆæ˜¾ç¤º summary_table
    csv_options_sorted = sorted(files_info["csvs"], key=lambda item: 0 if item['type'] == 'summary_table' else (1 if item['type'] == 'trends' else 2))
    csv_options = {f"{item['path'].name} ({item['type']})": item for item in csv_options_sorted}
    selected_csv_label = st.selectbox("3. é€‰æ‹©è¦åˆ†æçš„ CSV æ–‡ä»¶", options=csv_options.keys())
    if not selected_csv_label: st.stop()

    selected_csv_info = csv_options[selected_csv_label]
    csv_path = selected_csv_info["path"]
    csv_type = selected_csv_info["type"]

    st.markdown(f"#### æ­£åœ¨åˆ†æ: `{csv_path.name}` (è¯†åˆ«ç±»å‹: `{csv_type}`)")

    # --- åŠ è½½å¹¶å¤„ç† CSV ---
    try:
        df = pd.read_csv(csv_path)

        if df.empty:
            st.warning("CSV æ–‡ä»¶ä¸ºç©ºã€‚"); st.stop()

        # --- ã€æ ¸å¿ƒ v1.7ã€‘æ ¹æ®è¯†åˆ«å‡ºçš„ CSV ç±»å‹å†³å®šå±•ç¤ºå†…å®¹ ---
        st.markdown("---")

        # <<< --- A. å¦‚æœè¯†åˆ«ä¸ºåˆ†ææ±‡æ€»æ–‡ä»¶ (summary_table) --- >>>
        if csv_type == "summary_table":
            st.subheader("ğŸ“ˆ æ ¸å¿ƒåˆ†æå›¾è¡¨")
            # ... (å›¾è¡¨ç”Ÿæˆé€»è¾‘ä¸ v1.6 ç›¸åŒï¼Œä½†å¢åŠ äº†æ›´ä¸¥æ ¼çš„åˆ—æ£€æŸ¥) ...

            # å›¾è¡¨ 1: ä¸»é¢˜çƒ­åº¦ (æŒ‰è®ºæ–‡æ•°)
            required_cols_heat = ['Topic_Name', 'paper_count']
            if all(col in df.columns for col in required_cols_heat):
                st.markdown(f"##### 1. ä¸»é¢˜çƒ­åº¦æ’å (Top {ANALYSIS_TOP_N} è®ºæ–‡æ•°)")
                # æ£€æŸ¥æ•°æ®ç±»å‹æ˜¯å¦æ­£ç¡®
                if pd.api.types.is_numeric_dtype(df['paper_count']) and pd.api.types.is_string_dtype(df['Topic_Name']):
                    df_sorted_count = df.sort_values(by='paper_count', ascending=False).head(ANALYSIS_TOP_N)
                    fig_bar_count = px.bar(df_sorted_count, x='paper_count', y='Topic_Name', orientation='h',
                                     title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} çƒ­é—¨ä¸»é¢˜ (è®ºæ–‡æ•°é‡)',
                                     labels={'paper_count': 'è®ºæ–‡æ•°é‡', 'Topic_Name': 'ä¸»é¢˜'},
                                     text_auto=True, height=max(600, len(df_sorted_count)*20))
                    fig_bar_count.update_layout(yaxis={'categoryorder':'total ascending'})
                    st.plotly_chart(fig_bar_count, use_container_width=True)
                else:
                    st.caption(f"æ— æ³•ç”Ÿæˆä¸»é¢˜çƒ­åº¦å›¾ï¼š`paper_count` åˆ—å¿…é¡»æ˜¯æ•°å€¼ï¼Œ`Topic_Name` åˆ—å¿…é¡»æ˜¯æ–‡æœ¬ã€‚")
            else:
                st.caption(f"æ— æ³•ç”Ÿæˆä¸»é¢˜çƒ­åº¦å›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_cols_heat}")

            # å›¾è¡¨ 2: ä¸»é¢˜è´¨é‡ (æŒ‰å¹³å‡åˆ†)
            required_cols_quality = ['Topic_Name', 'avg_rating']
            if all(col in df.columns for col in required_cols_quality):
                 if pd.api.types.is_numeric_dtype(df['avg_rating']) and df['avg_rating'].notna().any():
                    st.markdown(f"##### 2. ä¸»é¢˜è´¨é‡æ’å (Top {ANALYSIS_TOP_N} å¹³å‡å®¡ç¨¿åˆ†)")
                    df_sorted_rating = df.dropna(subset=['avg_rating']).sort_values(by='avg_rating', ascending=False).head(ANALYSIS_TOP_N)
                    if not df_sorted_rating.empty:
                        fig_bar_rating = px.bar(df_sorted_rating, x='avg_rating', y='Topic_Name', orientation='h',
                                         title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜ (å¹³å‡å®¡ç¨¿åˆ†)',
                                         labels={'avg_rating': 'å¹³å‡å®¡ç¨¿åˆ†', 'Topic_Name': 'ä¸»é¢˜'},
                                         text_auto='.2f', height=max(600, len(df_sorted_rating)*20))
                        fig_bar_rating.update_layout(yaxis={'categoryorder':'total ascending'})
                        st.plotly_chart(fig_bar_rating, use_container_width=True)
                    else: st.caption("æœªèƒ½ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼šç­›é€‰/æ’åºåæ•°æ®ä¸ºç©ºã€‚")
                 elif not df['avg_rating'].notna().any():
                     st.caption("æ— æ³•ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼š'avg_rating' åˆ—æ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
                 else: # avg_rating ä¸æ˜¯æ•°å€¼ç±»å‹
                     st.caption("æ— æ³•ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼š'avg_rating' åˆ—å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ã€‚")
            else:
                st.caption(f"æ— æ³•ç”Ÿæˆä¸»é¢˜è´¨é‡å›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_cols_quality}")

            # å›¾è¡¨ 3: å†³ç­–æ„æˆ (æŒ‰æ¥æ”¶ç‡)
            decision_cols = ['Oral', 'Spotlight', 'Poster', 'Reject', 'N/A']
            present_decision_cols = [col for col in decision_cols if col in df.columns]
            required_cols_decision_base = ['Topic_Name', 'acceptance_rate']
            if present_decision_cols and all(col in df.columns for col in required_cols_decision_base):
                 if pd.api.types.is_numeric_dtype(df['acceptance_rate']):
                    st.markdown(f"##### 3. ä¸»é¢˜æ¥æ”¶æ„æˆ (Top {ANALYSIS_TOP_N} æŒ‰æ¥æ”¶ç‡æ’åº)")
                    df_sorted_accept = df.dropna(subset=['acceptance_rate']).sort_values(by='acceptance_rate', ascending=False).head(ANALYSIS_TOP_N)
                    if not df_sorted_accept.empty:
                        df_plot = df_sorted_accept.set_index('Topic_Name')[present_decision_cols]
                        df_plot = df_plot.loc[df_plot.sum(axis=1) > 0]
                        if not df_plot.empty:
                            # ç¡®ä¿æ‰€æœ‰å†³ç­–åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
                            valid_plot = True
                            for col in present_decision_cols:
                                if not pd.api.types.is_numeric_dtype(df_plot[col]):
                                    st.caption(f"æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šå†³ç­–åˆ— '{col}' å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ã€‚")
                                    valid_plot = False; break
                            if valid_plot:
                                df_plot_normalized = df_plot.apply(lambda x: x / x.sum(), axis=1)
                                fig_stack = px.bar(df_plot_normalized, y=df_plot_normalized.index, x=df_plot_normalized.columns,
                                                   orientation='h', title=f'{selected_conf} {selected_year} - Top {ANALYSIS_TOP_N} ä¸»é¢˜å†³ç­–æ„æˆ (æŒ‰æ¥æ”¶ç‡æ’åº)',
                                                   labels={'value': 'è®ºæ–‡æ¯”ä¾‹', 'variable': 'å†³ç­–ç±»å‹', 'y': 'ä¸»é¢˜'},
                                                   height=max(600, len(df_plot_normalized)*25), text_auto='.1%')
                                fig_stack.update_layout(yaxis={'categoryorder':'total ascending'}, xaxis_tickformat=".0%", legend_title_text='å†³ç­–ç±»å‹')
                                fig_stack.update_traces(hovertemplate='<b>%{y}</b><br>%{variable}: %{x:.1%}<extra></extra>')
                                st.plotly_chart(fig_stack, use_container_width=True)
                        else: st.caption("æœªèƒ½ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šç­›é€‰æ‰å…¨é›¶æ•°æ®åä¸ºç©ºã€‚")
                    else: st.caption("æœªèƒ½ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šæŒ‰æ¥å—ç‡ç­›é€‰/æ’åºåæ•°æ®ä¸ºç©ºã€‚")
                 else: # acceptance_rate ä¸æ˜¯æ•°å€¼
                      st.caption("æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼š'acceptance_rate' åˆ—å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ã€‚")
            elif not present_decision_cols: st.caption(f"æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘å†³ç­–åˆ— (å¦‚ Oral, Poster ç­‰)ã€‚")
            else: st.caption(f"æ— æ³•ç”Ÿæˆå†³ç­–æ„æˆå›¾ï¼šCSV æ–‡ä»¶ `{csv_path.name}` ç¼ºå°‘ 'Topic_Name' æˆ– 'acceptance_rate' åˆ—ã€‚")

        # <<< --- B. å¦‚æœè¯†åˆ«ä¸ºè·¨å¹´è¶‹åŠ¿æ–‡ä»¶ (trends) --- >>>
        elif csv_type == "trends":
            st.subheader("ğŸ“ˆ è·¨å¹´ä»½è¶‹åŠ¿å›¾")
            # ... (è¶‹åŠ¿å›¾ç”Ÿæˆé€»è¾‘ä¸ v1.6 ç›¸åŒ) ...
            try:
                if 'year' in df.columns: df_indexed = df.set_index('year')
                elif df.index.name != 'year':
                     potential_year_col = df.columns[0]
                     if pd.api.types.is_numeric_dtype(df[potential_year_col]):
                          df_indexed = df.set_index(potential_year_col)
                          df_indexed.index.name = 'year'
                     else: raise ValueError("æ— æ³•è‡ªåŠ¨è¯†åˆ«å¹´ä»½åˆ—/ç´¢å¼•")
                else: df_indexed = df # å·²ç»æ˜¯å¹´ä»½ç´¢å¼•

                if df_indexed.index.name == 'year' and len(df_indexed.columns) > 0:
                    numeric_topic_cols = get_numeric_columns(df_indexed)
                    if numeric_topic_cols:
                        top_topics = df_indexed[numeric_topic_cols].sum().nlargest(15).index
                        df_top = df_indexed[top_topics]
                        fig_line = px.line(df_top, x=df_top.index, y=df_top.columns, markers=True,
                                           title=f'{selected_conf} - Top 15 ä¸»é¢˜å†å¹´è®ºæ–‡æ•°è¶‹åŠ¿',
                                           labels={'year': 'å¹´ä»½', 'value': 'è®ºæ–‡æ•°é‡', 'variable': 'ä¸»é¢˜'})
                        fig_line.update_layout(xaxis_type='category')
                        st.plotly_chart(fig_line, use_container_width=True)
                    else:
                         st.warning("è¶‹åŠ¿æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°å€¼ç±»å‹çš„ä¸»é¢˜åˆ—ç”¨äºç»˜å›¾ã€‚")
                else:
                    st.warning(f"æ— æ³•ä¸º `{csv_path.name}` ç”Ÿæˆè¶‹åŠ¿æŠ˜çº¿å›¾ã€‚è¯·ç¡®ä¿ CSV æ ¼å¼æ­£ç¡®ã€‚")
            except Exception as trend_e:
                st.error(f"ç”Ÿæˆè·¨å¹´è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {trend_e}")
                st.error(traceback.format_exc())


        # <<< --- C. å¦‚æœè¯†åˆ«ä¸ºåŸå§‹æ•°æ®æ–‡ä»¶ (raw_data) æˆ–å…¶ä»– --- >>>
        else: # åŒ…æ‹¬ 'raw_data', 'analysis_other', 'unknown'
            st.subheader("ğŸ“„ æ•°æ®é¢„è§ˆ")
            st.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹ä¸º **{csv_type}**ã€‚æ­¤ç±»æ–‡ä»¶é€šå¸¸ä¸åŒ…å«ç”¨äºç”Ÿæˆæ ‡å‡†åˆ†æå›¾è¡¨çš„èšåˆæ•°æ®ï¼Œå› æ­¤ä»…æä¾›æ•°æ®è¡¨æ ¼é¢„è§ˆã€‚")
            st.dataframe(df, height=500, use_container_width=True)


        # --- æ˜¾ç¤º/ä¸‹è½½åŸå§‹æ•°æ®è¡¨æ ¼ (é€‚ç”¨äºæ‰€æœ‰ç±»å‹) ---
        st.markdown("---")
        with st.expander("æŸ¥çœ‹/ä¸‹è½½å½“å‰ CSV æ•°æ®è¡¨æ ¼"):
            st.dataframe(df, height=300, use_container_width=True)
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½æ•°æ® {csv_path.name}",
                data=df.to_csv(index=False).encode('utf-8-sig'),
                file_name=csv_path.name,
                mime='text/csv',
                key=f"download_{csv_path.stem}" # æ·»åŠ å”¯ä¸€ key
            )

    except pd.errors.EmptyDataError:
        st.warning(f"æ–‡ä»¶ {csv_path.name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
    except Exception as e:
        st.error(f"å¤„ç† CSV æ–‡ä»¶ {csv_path.name} æ—¶å¤±è´¥: {e}")
        st.error(traceback.format_exc()) # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯

# -----------------------------------------------------------------
# 7. ä¸»åº”ç”¨é€»è¾‘ï¼šä¾§è¾¹æ å¯¼èˆª (ä¿æŒä¸å˜)
# -----------------------------------------------------------------
st.sidebar.title("PubCrawler Pro ğŸš€"); st.sidebar.caption("v1.7 - Robust CSV Handling")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½é¡µé¢", ["ğŸ¤– AI åŠ©æ‰‹ & æœç´¢", "ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜"], key="page_selection")
st.sidebar.divider()
analysis_data_sidebar, conf_list_sidebar, _ = find_analysis_files()
if conf_list_sidebar:
    with st.sidebar.expander("ç›®å‰å·²ç´¢å¼•çš„ä¼šè®®", expanded=True):
        st.dataframe(conf_list_sidebar, use_container_width=True, hide_index=True, column_config={"value": "ä¼šè®®åç§°"})
if page == "ğŸ¤– AI åŠ©æ‰‹ & æœç´¢": render_search_and_chat_page()
elif page == "ğŸ“Š è¶‹åŠ¿åˆ†æä»ªè¡¨ç›˜": render_analysis_dashboard_page()

