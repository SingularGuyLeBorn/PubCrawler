# FILE: src/search/search_local.py

import pandas as pd
from pathlib import Path
import time
import textwrap
from datetime import datetime
import re
import os

# --- é…ç½® ---
PROJECT_ROOT = Path(__file__).parent.parent.parent
METADATA_DIR = PROJECT_ROOT / "output" / "metadata"
SEARCH_RESULTS_DIR = PROJECT_ROOT / "search_results"
# å†…å­˜ä¼˜åŒ–ï¼šåªåŠ è½½æˆ‘ä»¬æœç´¢å’Œæ˜¾ç¤ºæ‰€å¿…éœ€çš„åˆ—
REQUIRED_COLUMNS = ['title', 'authors', 'abstract', 'pdf_url', 'conference', 'year', 'source_file']
# æ–‡ä»¶åˆ†å‰²ï¼šæ¯ä¸ª Markdown æ–‡ä»¶æœ€å¤šå­˜æ”¾çš„è®ºæ–‡æ•°é‡
PAPERS_PER_FILE = 100


def highlight_query(text, query):
    """åœ¨æ–‡æœ¬ä¸­é«˜äº®æ˜¾ç¤ºæŸ¥è¯¢å…³é”®è¯ (Markdownæ ¼å¼)ã€‚"""
    if not isinstance(text, str) or not query:
        return text
    # ä½¿ç”¨ re.IGNORECASE è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æ›¿æ¢
    # ä½¿ç”¨ `**{match.group(0)}**` æ¥åŠ ç²—åŒ¹é…åˆ°çš„æ–‡æœ¬
    try:
        highlighted_text = re.sub(f'({re.escape(query)})', r'**\1**', text, flags=re.IGNORECASE)
        return highlighted_text
    except re.error:
        return text  # å¦‚æœæŸ¥è¯¢æ˜¯æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œåˆ™è¿”å›åŸæ–‡


def load_all_papers_from_csv(directory: Path) -> pd.DataFrame:
    """
    å†…å­˜ä¼˜åŒ–ç‰ˆçš„åŠ è½½å‡½æ•°ï¼šåªè¯»å–å¿…éœ€çš„åˆ—ã€‚
    """
    print(f"[*] æ­£åœ¨ä» {directory} åŠ è½½æ‰€æœ‰è®ºæ–‡æ•°æ®...")
    if not directory.exists():
        print(f"[!] é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {directory}")
        return None

    csv_files = list(directory.rglob("*_data_*.csv"))
    if not csv_files:
        print(f"[!] è­¦å‘Š: åœ¨ç›®å½• {directory} ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• '_data_*.csv' æ–‡ä»¶ã€‚")
        return None

    print(f"[*] å‘ç° {len(csv_files)} ä¸ª CSV æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½ (å†…å­˜ä¼˜åŒ–æ¨¡å¼)...")
    df_list = []
    total_papers = 0

    for f in csv_files:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤´ä»¥ç¡®å®šå¯ç”¨åˆ—
            header = pd.read_csv(f, nrows=0).columns.tolist()
            cols_to_use = [col for col in REQUIRED_COLUMNS if col in header]

            df = pd.read_csv(f, usecols=cols_to_use, dtype=str).fillna('')  # è¯»å–ä¸ºå­—ç¬¦ä¸²å¹¶å¡«å……NA

            if not df.empty:
                # è¡¥å…¨éƒ½éœ€è¦çš„åˆ—ï¼Œå³ä½¿CSVä¸­æ²¡æœ‰
                for col in REQUIRED_COLUMNS:
                    if col not in df.columns:
                        df[col] = ''
                if 'source_file' not in cols_to_use:
                    df['source_file'] = f.name
                df_list.append(df)
                total_papers += len(df)

        except Exception as e:
            print(f"[!] è­¦å‘Š: åŠ è½½æˆ–å¤„ç†æ–‡ä»¶ {f.name} æ—¶å‡ºé”™ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯: {e}")

    if not df_list:
        print("\n[âœ–] æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è®ºæ–‡æ•°æ®ã€‚")
        return None

    print(f"[âœ”] åŠ è½½å®Œæˆï¼å…±æ‰¾åˆ° {total_papers} ç¯‡è®ºæ–‡è®°å½•ã€‚")
    return pd.concat(df_list, ignore_index=True)


def save_results_to_files(results_df: pd.DataFrame, query: str, session_dir: Path):
    """
    å°†æœç´¢ç»“æœä¿å­˜åˆ°ä¸€ä¸ªæˆ–å¤šä¸ª Markdown æ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªæ–‡ä»¶æœ€å¤š PAPERS_PER_FILE ç¯‡ã€‚
    """
    safe_query = re.sub(r'[\\/*?:"<>|]', "", query).replace(" ", "_")
    num_results = len(results_df)

    # è®¡ç®—éœ€è¦åˆ†å‰²æˆå¤šå°‘ä¸ªæ–‡ä»¶
    num_files = (num_results + PAPERS_PER_FILE - 1) // PAPERS_PER_FILE

    saved_files = []

    for i in range(num_files):
        start_index = i * PAPERS_PER_FILE
        end_index = start_index + PAPERS_PER_FILE
        chunk_df = results_df.iloc[start_index:end_index]

        part_num_str = f"_part_{i + 1}" if num_files > 1 else ""
        filename = session_dir / f"search_{safe_query[:30]}{part_num_str}.md"
        saved_files.append(filename)

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# æœç´¢ç»“æœ: \"{query}\"\n\n")
            f.write(f"**æ€»è®¡æ‰¾åˆ° {num_results} ä¸ªåŒ¹é…é¡¹ã€‚** (æ­¤æ–‡ä»¶ä¸ºç¬¬ {i + 1}/{num_files} éƒ¨åˆ†)\n\n")
            f.write("---\n\n")

            for index, row in chunk_df.iterrows():
                # ä½¿ç”¨ highlight_query é«˜äº®æ ‡é¢˜å’Œæ‘˜è¦
                title = highlight_query(row.get('title', 'N/A'), query)
                abstract = highlight_query(row.get('abstract', 'æ— æ‘˜è¦'), query)

                f.write(f"### {start_index + index + 1}. {title}\n\n")
                f.write(f"- **ä½œè€…**: {row.get('authors', 'N/A')}\n")
                f.write(f"- **ä¼šè®®/æœŸåˆŠ**: {row.get('conference', 'N/A')} {row.get('year', '')}\n")

                pdf_url = row.get('pdf_url')
                if pdf_url:
                    f.write(f"- **PDFé“¾æ¥**: [{pdf_url}]({pdf_url})\n")

                f.write("\n**æ‘˜è¦:**\n")
                f.write(f"> {abstract}\n\n")
                f.write(f"*æ¥æºæ–‡ä»¶: `{row.get('source_file', 'N/A')}`*\n\n")
                f.write("---\n\n")

    return saved_files


def search_papers(df: pd.DataFrame, query: str, session_dir: Path):
    """
    æ‰§è¡Œæœç´¢ã€é«˜äº®ã€åˆ†é¡µä¿å­˜å’Œé¢„è§ˆã€‚
    """
    print(f"\n[*] æ­£åœ¨æœç´¢å…³é”®è¯: '{query}'...")
    start_time = time.time()

    # åœ¨æœç´¢å‰ç¡®ä¿åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å….strå±æ€§é”™è¯¯
    df['title'] = df['title'].astype(str)
    df['abstract'] = df['abstract'].astype(str)

    results_mask = df['title'].str.contains(query, case=False, na=False) | \
                   df['abstract'].str.contains(query, case=False, na=False)
    results_df = df[results_mask].reset_index(drop=True)

    end_time = time.time()
    num_results = len(results_df)

    print(f"[âœ”] æœç´¢å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.4f} ç§’ã€‚å…±æ‰¾åˆ° {num_results} ä¸ªåŒ¹é…é¡¹ã€‚")

    if num_results == 0:
        return

    try:
        saved_files = save_results_to_files(results_df, query, session_dir)
        if len(saved_files) == 1:
            print(f"[âœ”] è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³æ–‡ä»¶: {saved_files[0]}")
        else:
            print(f"[âœ”] è¯¦ç»†ç»“æœå·²åˆ†å‰²å¹¶ä¿å­˜è‡³ {len(saved_files)} ä¸ªæ–‡ä»¶ä¸­ (ä½äº {session_dir})")
    except Exception as e:
        print(f"[!] é”™è¯¯: ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")

    preview_count = min(3, num_results)
    print(f"\n--- ç»“æœé¢„è§ˆ (å‰ {preview_count} æ¡ï¼Œå…³é”®è¯å·²ç”¨'**'é«˜äº®) ---")

    for index, row in results_df.head(preview_count).iterrows():
        print(f"\n[{index + 1}] {highlight_query(row['title'], query)}")
        print(f"  - ä¼šè®®/æœŸåˆŠ: {row.get('conference', 'N/A')} {row.get('year', '')}")
        abstract_preview = textwrap.shorten(row.get('abstract', ''), width=200, placeholder="...")
        print(f"  - æ‘˜è¦é¢„è§ˆ: {highlight_query(abstract_preview, query)}")
        print("-" * 20)

    if num_results > preview_count:
        print(f"\n[...] æ›´å¤šç»“æœè¯·æŸ¥çœ‹å·²ä¿å­˜çš„æ–‡ä»¶ã€‚")


def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«ä¼šè¯ç®¡ç†ã€‚"""
    paper_database = load_all_papers_from_csv(METADATA_DIR)
    if paper_database is None:
        return

    # --- ä¼šè¯ç®¡ç† ---
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_dir = SEARCH_RESULTS_DIR / f"session_{session_timestamp}"
    session_dir.mkdir(exist_ok=True)

    print("\n" + "=" * 50)
    print("      æ¬¢è¿ä½¿ç”¨ PubCrawler æœ¬åœ°è®ºæ–‡æœç´¢å¼•æ“ v2.0")
    print("=" * 50)
    print(f"[*] æœ¬æ¬¡ä¼šè¯çš„æ‰€æœ‰æœç´¢ç»“æœå°†ä¿å­˜åœ¨: \n    {session_dir}")

    while True:
        try:
            user_query = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ (æˆ–è¾“å…¥ 'exit' é€€å‡º): ").strip()
            if user_query.lower() == 'exit':
                break
            if not user_query:
                continue

            search_papers(paper_database, user_query, session_dir)

        except KeyboardInterrupt:
            print("\n[ğŸ‘‹] æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œå†è§ï¼")
            break


if __name__ == "__main__":
    main()